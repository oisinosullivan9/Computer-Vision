{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eac703e0",
      "metadata": {
        "id": "eac703e0"
      },
      "source": [
        "# Diffusion-based Image Colorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adbc8683",
      "metadata": {
        "id": "adbc8683"
      },
      "outputs": [],
      "source": [
        "CFG = {\n",
        "\"dataset\": \"CIFAR10\",\n",
        "\"img_size\": 32,\n",
        "\"batch_size\": 128, # reduce to 64 or 32 if running on CPU\n",
        "\"epochs\": 60, # small number for quick iteration\n",
        "\"lr\": 2e-4,\n",
        "\"device\": \"cuda\" if __import__('torch').cuda.is_available() else \"cpu\",\n",
        "\"timesteps\": 200, # number of diffusion steps (DDPM)\n",
        "\"sample_steps\": 25, # steps used for sampling (DDIM/accelerated)\n",
        "\"channels\": 3,\n",
        "# Experiment toggles (for ablation study)\n",
        "\"use_learnable_t_emb\": True,\n",
        "\"use_attention\": False,\n",
        "\"use_ddim\": True,\n",
        "\"max_train_batches\": None # set to an int to limit batches per epoch for faster runs\n",
        "}\n",
        "\n",
        "\n",
        "print(\"Device:\", CFG['device'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd0c9ac",
      "metadata": {
        "id": "2cd0c9ac"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdaffdcc",
      "metadata": {
        "id": "cdaffdcc"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "def show_tensor_images(images, nrow=8, figsize=(8,8), title=None):\n",
        "    images = images.clone().detach().cpu()\n",
        "    grid = make_grid(images, nrow=nrow, normalize=True, value_range=(0,1))\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis('off')\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.imshow(grid.permute(1,2,0))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7face17",
      "metadata": {
        "id": "e7face17"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # [0,1]\n",
        "])\n",
        "\n",
        "train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "val_ds = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# helper: make 1-channel grayscale from 3-channel image\n",
        "def rgb_to_gray(x):\n",
        "    # x: Tensor [C,H,W] with C=3\n",
        "    r, g, b = x[0], x[1], x[2]\n",
        "    gray = 0.2989*r + 0.5870*g + 0.1140*b\n",
        "    return gray.unsqueeze(0)\n",
        "\n",
        "\n",
        "# Wrap dataset to return (grayscale, color)\n",
        "class ColorizationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_ds):\n",
        "        self.base = base_ds\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.base[idx]\n",
        "        # img in [0,1]\n",
        "        gray = rgb_to_gray(img)\n",
        "        return gray, img\n",
        "\n",
        "\n",
        "train_ds_col = ColorizationDataset(train_ds)\n",
        "val_ds_col = ColorizationDataset(val_ds)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_ds_col, batch_size=CFG['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds_col, batch_size=CFG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "# Peek\n",
        "gr, col = next(iter(train_loader))\n",
        "show_tensor_images(torch.cat([gr.repeat(1,3,1,1)[:16], col[:16]], dim=0), nrow=8, title='Grayscale (replicated) | Ground Truth Color')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4fdac6e",
      "metadata": {
        "id": "d4fdac6e"
      },
      "outputs": [],
      "source": [
        "def linear_beta_schedule(timesteps, beta_start=1e-4, beta_end=0.02):\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "T = CFG['timesteps']\n",
        "betas = linear_beta_schedule(T).to(CFG['device'])\n",
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "# helpers that will be used in training/sampling\n",
        "betas_t = betas\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)\n",
        "\n",
        "# utility: extract a tensor of shape [batch_size, 1,1,1] for a timestep t\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    # a: tensor [T], t: tensor of shape [B] with ints\n",
        "    out = a.gather(-1, t).to(t.device) # Fix: Removed .cpu() from t (change if running cpu)\n",
        "    return out.view(-1, *((1,)*(len(x_shape)-1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c977f8",
      "metadata": {
        "id": "52c977f8"
      },
      "outputs": [],
      "source": [
        "class SmallConvBlock(nn.Module):\n",
        "    def __init__(self, c_in, c_out, kernel=3, padding=1):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(c_in, c_out, kernel_size=kernel, padding=padding),\n",
        "            nn.GroupNorm(8, c_out),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "    def forward(self, t):\n",
        "        device = t.device\n",
        "        half = self.dim // 2\n",
        "        emb = torch.log(torch.tensor(10000.0)) / (half - 1)\n",
        "        emb = torch.exp(torch.arange(half, device=device) * -emb)\n",
        "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class LearnableTimestepEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_steps=1000):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(max_steps, dim)\n",
        "    def forward(self, t):\n",
        "        # t tensor of ints shape [B]\n",
        "        return self.emb(t)\n",
        "\n",
        "class TinyUNet(nn.Module):\n",
        "    def __init__(self, in_ch=4, base_ch=64, t_emb_dim=128, use_learnable_t_emb=False, use_attention=False):\n",
        "        super().__init__()\n",
        "        self.use_attention = use_attention\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "        if use_learnable_t_emb:\n",
        "            self.t_emb = LearnableTimestepEmbedding(t_emb_dim, max_steps=T)\n",
        "        else:\n",
        "            self.t_emb = SinusoidalPosEmb(t_emb_dim)\n",
        "        # map timestep emb to channels\n",
        "        self.time_mlp = nn.Sequential(nn.Linear(t_emb_dim, t_emb_dim*2), nn.SiLU(), nn.Linear(t_emb_dim*2, base_ch*2))\n",
        "\n",
        "\n",
        "        # Downsampling path\n",
        "        self.conv1 = SmallConvBlock(in_ch, base_ch) # Output: (B, base_ch, 32, 32)\n",
        "        self.conv2 = SmallConvBlock(base_ch, base_ch*2) # Output: (B, base_ch*2, 16, 16) after pooling\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "        # Bottleneck\n",
        "        self.conv3 = SmallConvBlock(base_ch*2, base_ch*2) # Output: (B, base_ch*2, 8, 8) after pooling\n",
        "        # Upsampling path\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        # Changed conv4 to conv_up1 and adjusted its output channels\n",
        "        self.conv_up1 = SmallConvBlock(base_ch*4, base_ch*2) # Output: (B, base_ch*2, 16, 16)\n",
        "        # Added a new convolutional block for the final upsampling stage\n",
        "        self.conv_up2 = SmallConvBlock(base_ch*3, base_ch) # Output: (B, base_ch, 32, 32)\n",
        "\n",
        "        self.final = nn.Conv2d(base_ch, 3, kernel_size=1) # Predict noise for 3 color channels (Output: B, 3, 32, 32)\n",
        "\n",
        "\n",
        "        if use_attention:\n",
        "            # simple self-attention at bottleneck\n",
        "            self.attn = nn.MultiheadAttention(embed_dim=base_ch*2, num_heads=4, batch_first=True)\n",
        "        else:\n",
        "            self.attn = None\n",
        "\n",
        "    def forward(self, x, t, cond):\n",
        "        # x: noisy color image [B,3,H,W]\n",
        "        # cond: grayscale [B,1,H,W]\n",
        "        # t: tensor of timesteps [B]\n",
        "        B = x.shape[0]\n",
        "        # concat conditioning\n",
        "        h = torch.cat([x, cond], dim=1) # (B, 4, 32, 32)\n",
        "\n",
        "        # Downsampling\n",
        "        h1 = self.conv1(h) # (B, base_ch, 32, 32)\n",
        "        h2 = self.conv2(self.pool(h1)) # (B, base_ch*2, 16, 16)\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.conv3(self.pool(h2)) # (B, base_ch*2, 8, 8)\n",
        "\n",
        "        # Optional attention\n",
        "        if self.attn is not None:\n",
        "            # flatten spatial dims\n",
        "            B, C, H, W = b.shape\n",
        "            flat = b.view(B, C, H*W).permute(0,2,1) # [B, HW, C]\n",
        "            attn_out, _ = self.attn(flat, flat, flat)\n",
        "            attn_out = attn_out.permute(0,2,1).view(B, C, H, W)\n",
        "            b = b + attn_out\n",
        "\n",
        "        # Time embedding\n",
        "        t_emb = self.t_emb(t)\n",
        "        t_m = self.time_mlp(t_emb).view(B, -1, 1, 1)\n",
        "        # Broadcast-add to bottleneck\n",
        "        b = b + t_m\n",
        "\n",
        "        # Upsampling path\n",
        "        up1 = self.up(b) # (B, base_ch*2, 16, 16)\n",
        "        cat1 = torch.cat([up1, h2], dim=1) # (B, base_ch*4, 16, 16)\n",
        "        out1 = self.conv_up1(cat1) # (B, base_ch*2, 16, 16)\n",
        "\n",
        "        up2 = self.up(out1) # (B, base_ch*2, 32, 32)\n",
        "        cat2 = torch.cat([up2, h1], dim=1) # (B, base_ch*3, 32, 32)\n",
        "        out2 = self.conv_up2(cat2) # (B, base_ch, 32, 32)\n",
        "\n",
        "        # Final output\n",
        "        out = self.final(out2) # (B, 3, 32, 32)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f95821db",
      "metadata": {
        "id": "f95821db"
      },
      "outputs": [],
      "source": [
        "model = TinyUNet(in_ch=4, base_ch=64, t_emb_dim=128, use_learnable_t_emb=CFG['use_learnable_t_emb'], use_attention=CFG['use_attention']).to(CFG['device'])\n",
        "print(model)\n",
        "\n",
        "\n",
        "# count params\n",
        "def count_params(m):\n",
        "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print('Trainable params:', count_params(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48bd370c",
      "metadata": {
        "id": "48bd370c"
      },
      "outputs": [],
      "source": [
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "    sqrt_acp = extract(sqrt_alphas_cumprod, t, x_start.shape).to(x_start.device)\n",
        "    sqrt_om_acp = extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape).to(x_start.device)\n",
        "    return sqrt_acp * x_start + sqrt_om_acp * noise\n",
        "\n",
        "\n",
        "# loss: MSE between true noise and predicted noise\n",
        "mse = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf3eea3",
      "metadata": {
        "id": "caf3eea3"
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.AdamW(model.parameters(), lr=CFG['lr'], weight_decay=1e-6)\n",
        "\n",
        "\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(enumerate(train_loader), total=(len(train_loader) if CFG['max_train_batches'] is None else CFG['max_train_batches']))\n",
        "    running_loss = 0.0\n",
        "    for i, (gray, color) in pbar:\n",
        "        if CFG['max_train_batches'] and i >= CFG['max_train_batches']:\n",
        "            break\n",
        "        gray = gray.to(CFG['device'])\n",
        "        color = color.to(CFG['device'])\n",
        "        batch_size = color.shape[0]\n",
        "        t = torch.randint(0, T, (batch_size,), device=CFG['device']).long()\n",
        "        noise = torch.randn_like(color)\n",
        "        x_t = q_sample(color, t, noise=noise)\n",
        "        # model predicts noise given x_t and cond\n",
        "        pred_noise = model(x_t, t, gray)\n",
        "        loss = mse(noise, pred_noise)\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 50 == 0:\n",
        "            pbar.set_description(f\"Epoch {epoch} loss {running_loss/(i+1):.4f}\")\n",
        "    return running_loss / (i+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b138b0ee",
      "metadata": {
        "id": "b138b0ee"
      },
      "outputs": [],
      "source": [
        "def p_sample(model, x_t, t, cond):\n",
        "    # one reverse step (DDPM) predicting noise\n",
        "    bet = betas_t[t].to(x_t.device)\n",
        "    sqrt_alpha = torch.sqrt(alphas[t]).to(x_t.device)\n",
        "    alpha_cum = alphas_cumprod[t].to(x_t.device)\n",
        "    sqrt_one_minus_alpha_cum = torch.sqrt(1 - alpha_cum).to(x_t.device)\n",
        "    # predicted noise\n",
        "    pred_noise = model(x_t, t.repeat(x_t.shape[0]), cond)\n",
        "    # estimate x0\n",
        "    x0_pred = (x_t - sqrt_one_minus_alpha_cum.view(-1,1,1,1)*pred_noise) / torch.sqrt(alpha_cum).view(-1,1,1,1)\n",
        "    # compute mean of p(x_{t-1} | x_t)\n",
        "    coef1 = (bet * torch.sqrt(alphas_cumprod[:-1] if t>0 else torch.tensor([1.0])).to(x_t.device)) / (1. - alpha_cum)\n",
        "    # simplified (vectorized) implementation below uses known formula for posterior mean\n",
        "    posterior_mean_coef1 = bet / torch.sqrt(1. - alpha_cum)\n",
        "    # for simplicity, use standard DDPM update with added noise\n",
        "    # compute the posterior mean directly using common formula\n",
        "    # NOTE: for stability we use vectorized math per batch using tensors extracted via extract()\n",
        "    beta_t = extract(betas_t, t, x_t.shape).to(x_t.device)\n",
        "    alpha_t = extract(alphas, t, x_t.shape).to(x_t.device)\n",
        "    alpha_cum_t = extract(alphas_cumprod, t, x_t.shape).to(x_t.device)\n",
        "    sqrt_recip_alpha_t = torch.sqrt(1.0/alpha_t)\n",
        "    # predicted x0\n",
        "    x0_pred = (x_t - torch.sqrt(1-alpha_cum_t)*pred_noise) / torch.sqrt(alpha_cum_t)\n",
        "    # coef for mean\n",
        "    mean = (torch.sqrt(alpha_cum_t)* (1 - alpha_t) / (1 - alpha_cum_t)) * x0_pred + (alpha_t * (1 - alpha_cum_t) / (1 - alpha_cum_t)) * x_t\n",
        "    # add noise for non-zero timesteps\n",
        "    if (t > 0):\n",
        "        noise = torch.randn_like(x_t)\n",
        "        sigma = torch.sqrt(beta_t)\n",
        "        return mean + sigma * noise\n",
        "    else:\n",
        "        return mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4375c445",
      "metadata": {
        "id": "4375c445"
      },
      "outputs": [],
      "source": [
        "def ddim_sample(model, cond, shape, steps=50, eta=0.0):\n",
        "    # cond: grayscale [B,1,H,W]\n",
        "    device = cond.device\n",
        "    B = shape[0]\n",
        "    x = torch.randn(shape, device=device)\n",
        "    # create time schedule\n",
        "    seq = torch.linspace(T-1, 0, steps, dtype=torch.long)\n",
        "    alphas_cum = alphas_cumprod.to(device)\n",
        "    for i in range(steps):\n",
        "        t = torch.full((B,), int(seq[i].item()), device=device, dtype=torch.long)\n",
        "        with torch.no_grad():\n",
        "            pred_noise = model(x, t, cond)\n",
        "        alpha_t = extract(alphas_cum, t, x.shape).to(device)\n",
        "        sqrt_alpha_t = torch.sqrt(alpha_t)\n",
        "        sqrt_one_minus_alpha_t = torch.sqrt(1 - alpha_t)\n",
        "        x0_pred = (x - sqrt_one_minus_alpha_t * pred_noise) / sqrt_alpha_t\n",
        "        # compute next x\n",
        "        if i == steps-1:\n",
        "            x = x0_pred\n",
        "        else:\n",
        "            t_next = torch.full((B,), int(seq[i+1].item()), device=device, dtype=torch.long)\n",
        "            alpha_next = extract(alphas_cum, t_next, x.shape).to(device)\n",
        "            sigma = eta * torch.sqrt((1 - alpha_next) / (1 - alpha_t) * (1 - alpha_t/alpha_next))\n",
        "            dir_part = torch.sqrt(1.0 - alpha_next - sigma**2) * pred_noise\n",
        "            noise = sigma * torch.randn_like(x)\n",
        "            x = torch.sqrt(alpha_next) * x0_pred + dir_part + noise\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1d5720c",
      "metadata": {
        "id": "d1d5720c"
      },
      "outputs": [],
      "source": [
        "best_val = 1e9\n",
        "for epoch in range(1, CFG['epochs']+1):\n",
        "    t0 = time.time()\n",
        "    loss = train_one_epoch(epoch)\n",
        "    t1 = time.time()\n",
        "    print(f\"Epoch {epoch} completed in {t1-t0:.1f}s, loss {loss:.4f}\")\n",
        "    # quick validation visualization: take a batch from val set and sample\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        gray, color = next(iter(val_loader))\n",
        "        gray = gray.to(CFG['device'])\n",
        "        color = color.to(CFG['device'])\n",
        "        B = min(8, color.shape[0])\n",
        "        cond = gray[:B]\n",
        "        if CFG['use_ddim']:\n",
        "            samples = ddim_sample(model, cond, shape=(B,3,CFG['img_size'],CFG['img_size']), steps=CFG['sample_steps'], eta=0.0)\n",
        "        else:\n",
        "            # naive full ancestral sampling (slow)\n",
        "            x = torch.randn((B,3,CFG['img_size'],CFG['img_size']), device=CFG['device'])\n",
        "            for t_ in reversed(range(T)):\n",
        "                t = torch.full((B,), t_, device=CFG['device'], dtype=torch.long)\n",
        "                x = p_sample(model, x, t, cond)\n",
        "            samples = x\n",
        "        # clamp to [0,1]\n",
        "        samples = samples.clamp(0,1)\n",
        "        # show grayscale, ground truth, and sample\n",
        "        cat = torch.cat([cond.repeat(1,3,1,1)[:B], color[:B], samples[:B]], dim=0)\n",
        "        show_tensor_images(cat, nrow=B, title=f'Epoch {epoch}: Input Gray | GT Color | Sampled Color')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "203bea20",
      "metadata": {
        "id": "203bea20"
      },
      "outputs": [],
      "source": [
        "torch.save({'model_state_dict': model.state_dict(), 'cfg': CFG}, 'colorization_diffusion_small.pth')\n",
        "print('Saved model to colorization_diffusion_small.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce5b9383",
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = torch.load('colorization_diffusion_small.pth', map_location=CFG['device'])\n",
        "\n",
        "loaded_cfg = checkpoint['cfg']\n",
        "loaded_model = TinyUNet(\n",
        "    in_ch=4,\n",
        "    base_ch=64,\n",
        "    t_emb_dim=128,\n",
        "    use_learnable_t_emb=loaded_cfg['use_learnable_t_emb'],\n",
        "    use_attention=loaded_cfg['use_attention']\n",
        ").to(loaded_cfg['device'])\n",
        "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "loaded_model.eval()\n",
        "print('Loaded model from checkpoint.')\n",
        "\n",
        "with torch.no_grad():\n",
        "    gray_test, color_test = next(iter(val_loader))\n",
        "    gray_test = gray_test.to(loaded_cfg['device'])\n",
        "    color_test = color_test.to(loaded_cfg['device'])\n",
        "\n",
        "    B_test = min(8, color_test.shape[0])\n",
        "    cond_test = gray_test[:B_test]\n",
        "    gt_color_test = color_test[:B_test]\n",
        "\n",
        "    if loaded_cfg['use_ddim']:\n",
        "        generated_samples = ddim_sample(\n",
        "            loaded_model,\n",
        "            cond_test,\n",
        "            shape=(B_test, 3, loaded_cfg['img_size'], loaded_cfg['img_size']),\n",
        "            steps=loaded_cfg['sample_steps'],\n",
        "            eta=0.0\n",
        "        )\n",
        "    else:\n",
        "        generated_samples = torch.randn((B_test,3,loaded_cfg['img_size'],loaded_cfg['img_size']), device=loaded_cfg['device'])\n",
        "        for t_step in reversed(range(T)):\n",
        "            t = torch.full((B_test,), t_step, device=loaded_cfg['device'], dtype=torch.long)\n",
        "            generated_samples = p_sample(loaded_model, generated_samples, t, cond_test)\n",
        "\n",
        "    generated_samples = generated_samples.clamp(0,1)\n",
        "\n",
        "    display_images = torch.cat([\n",
        "        cond_test.repeat(1,3,1,1)[:B_test],\n",
        "        gt_color_test,\n",
        "        generated_samples\n",
        "    ], dim=0)\n",
        "    show_tensor_images(display_images, nrow=B_test, title='Loaded Model: Input Gray | GT Color | Generated Color')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f48f25f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import numpy as np\n",
        "\n",
        "gt_color_np = gt_color_test.permute(0,2,3,1).cpu().numpy()  # Convert to (B,H,W,C) and numpy\n",
        "generated_samples_np = generated_samples.permute(0,2,3,1).cpu().numpy()\n",
        "\n",
        "psnr_values = []\n",
        "ssim_values = []\n",
        "\n",
        "for i in range (B_test):\n",
        "    current_psnr = psnr(gt_color_np[i], generated_samples_np[i], data_range=1.0)\n",
        "    psnr_values.append(current_psnr)\n",
        "\n",
        "    current_ssim = ssim(gt_color_np[i], generated_samples_np[i], data_range=1.0, channel_axis=2)\n",
        "    ssim_values.append(current_ssim)\n",
        "\n",
        "print(f'Average PSNR: {np.mean(psnr_values):.2f} dB')\n",
        "print(f'Average SSIM: {np.mean(ssim_values):.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
