{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    train_img_dir = os.path.abspath(os.path.join('..', 'data', 'train'))\n",
    "    val_img_dir = os.path.abspath(os.path.join('..', 'data', 'val'))\n",
    "    test_img_dir = os.path.abspath(os.path.join('..', 'data', 'test'))\n",
    "\n",
    "    train_fixations_json = os.path.abspath(os.path.join('..', '..', 'salgan', 'data', 'fixations_train2014.json'))\n",
    "    val_fixations_json = os.path.abspath(os.path.join('..', '..', 'salgan', 'data', 'fixations_val2014.json'))\n",
    "\n",
    "    image_size = (256, 192)\n",
    "    saliency_size = (256, 192)\n",
    "\n",
    "    batch_size = 4\n",
    "    num_workers = 4\n",
    "    num_epochs = 5\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    timesteps = 1000\n",
    "    beta_start = 1e-4\n",
    "    beta_end = 0.02\n",
    "\n",
    "    output_dir = os.path.abspath('./saliency_diffusion_outputs')\n",
    "    checkpoint_path = os.path.join(output_dir, 'saliency_diffusion_unet.pt')\n",
    "    sample_dir = os.path.join(output_dir, 'samples')\n",
    "\n",
    "    device = 'cuda' if os.environ.get('CUDA_VISIBLE_DEVICES') is not None else 'cuda' if __import__('torch').cuda.is_available() else 'cpu'\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "os.makedirs(cfg.sample_dir, exist_ok=True)\n",
    "cfg.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Using device:', cfg.device)\n",
    "print('Train dir exists:', os.path.isdir(cfg.train_img_dir))\n",
    "print('Val dir exists:', os.path.isdir(cfg.val_img_dir))\n",
    "print('Test dir exists:', os.path.isdir(cfg.test_img_dir))\n",
    "print('Train fixations JSON exists:', os.path.isfile(cfg.train_fixations_json))\n",
    "print('Val fixations JSON exists:', os.path.isfile(cfg.val_fixations_json))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbbc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: str) -> Dict:\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def parse_fixations_json(fixations: Dict) -> Dict[str, np.ndarray]:\n",
    "    img_id_to_name = {}\n",
    "    if 'images' in fixations:\n",
    "        for img in fixations['images']:\n",
    "            img_id_to_name[img['id']] = img['file_name']\n",
    "\n",
    "    mapping: Dict[str. List[Tuple[int, int]]] = {}\n",
    "    if 'annotations' in fixations:\n",
    "        for ann in fixations['annotations']:\n",
    "            img_id = ann.get('image_id')\n",
    "            pts = ann.get('fixations') or ann.get('points') or []\n",
    "            if img_id is None:\n",
    "                continue\n",
    "            fname = img_id_to_name.get(img_id, None)\n",
    "            if fname is None:\n",
    "                continue\n",
    "            mapping.setdefault(fname, []).extend(pts)\n",
    "\n",
    "    saliency_maps: Dict[str, np.ndarray] = {}\n",
    "    W, H = cfg.saliency_size\n",
    "    for fname, pts in mapping.items():\n",
    "        sal_map = np.zeros((H, W), dtype=np.float32)\n",
    "        for p in pts:\n",
    "            if len(p) < 2:\n",
    "                continue\n",
    "            x, y = p[0], p[1]\n",
    "            ix = int(np.clip(x / max(1.0, x) * (W - 1), 0, W - 1)) if False else int(np.clip(x, 0, W - 1))\n",
    "            iy = int(np.clip(y, 0, H - 1))\n",
    "            sal_map[iy, ix] += 1.0\n",
    "\n",
    "            if sal_map.max() > 0:\n",
    "                sal_map /= sal_map.max()\n",
    "\n",
    "            saliency_maps[fname] = sal_map\n",
    "\n",
    "        return saliency_maps\n",
    "    \n",
    "try:\n",
    "    train_fix_raw = load_json(cfg.train_fixations_json)\n",
    "    val_fix_raw = load_json(cfg.val_fixations_json)\n",
    "    print('Loaded fixation JSONs.')\n",
    "except Exception as e:\n",
    "    print('Error loading or parsing fixation JSONs. Please adapt parse_fixations_json to your format.')\n",
    "    print(e)\n",
    "    train_fix_raw, val_fix_raw = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12431126",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_saliency_maps = parse_fixations_json(train_fix_raw) if train_fix_raw is not None else {}\n",
    "val_saliency_maps = parse_fixations_json(val_fix_raw) if val_fix_raw is not None else {}\n",
    "\n",
    "print('Train saliency entries:', len(train_saliency_maps))\n",
    "print('Val saliency entries:', len(val_saliency_maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliconSaliencyDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, saliency_maps: Dict[str, np.ndarray], image_size=(256, 192)):\n",
    "        self.img_dir = img_dir\n",
    "        self.saliency_maps = saliency_maps\n",
    "        self.image_size = image_size\n",
    "        # Collect only images that have saliency info\n",
    "        self.image_files = [f for f in os.listdir(img_dir) if f in saliency_maps]\n",
    "        self.image_files.sort()\n",
    "\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size[1], image_size[0])),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        # Saliency in [0,1], single-channel\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, fname)\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.img_transform(img)\n",
    "\n",
    "        sal_map = self.saliency_maps[fname]\n",
    "        H, W = self.image_size[1], self.image_size[0]\n",
    "        sal_img = Image.fromarray((sal_map * 255).astype(np.uint8))\n",
    "        sal_img = sal_img.resize((W, H), resample=Image.BILINEAR)\n",
    "        sal_tensor = torch.from_numpy(np.array(sal_img)).float() / 255.0\n",
    "        sal_tensor = sal_tensor.unsqueeze(0)  # (1, H, W)\n",
    "\n",
    "        return img, sal_tensor, fname\n",
    "    \n",
    "train_dataset = SaliconSaliencyDataset(cfg.train_img_dir, train_saliency_maps, image_size=cfg.image_size)\n",
    "val_dataset = SaliconSaliencyDataset(cfg.val_img_dir, val_saliency_maps, image_size=cfg.image_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "print('Train dataset size:', len(train_dataset))\n",
    "print('Val dataset size:', len(val_dataset))\n",
    "\n",
    "batch = next(iter(train_loader)) if len(train_dataset) > 0 else None\n",
    "if batch is not None:\n",
    "    imgs, sal_maps, fnames = batch\n",
    "    print('Batch shapes:', imgs.shape, sal_maps.shape)\n",
    "    grid = vutils.make_grid(imgs, nrow=min(4, imgs.size(0)))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.title('Sample input images')\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    grid_sal = vutils.make_grid(sal_maps, nrow=min(4, sal_maps.size(0)))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.title('Sample ground truth saliency maps')\n",
    "    plt.imshow(grid_sal[0].cpu(), cmap='hot')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Warning: Train dataset is empty. Check your paths and JSON mapping.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.time_mlp = None\n",
    "        if time_emb_dim is not None:\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                nn.Linear(time_emb_dim, out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, t_emb=None):\n",
    "        h = self.conv1(x)\n",
    "        if self.time_mlp is not None and t_emb is not None:\n",
    "            # Add time embedding (broadcast over spatial dims)\n",
    "            temb = self.time_mlp(t_emb)[:, :, None, None]\n",
    "            h = h + temb\n",
    "        h = self.act(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.act(h)\n",
    "        return h\n",
    "    \n",
    "class UNetSaliency(nn.Module):\n",
    "    def __init__(self, img_channels=3, saliency_channels=1, base_ch=64, time_emb_dim=128):\n",
    "        super().__init__()\n",
    "        in_ch = img_channels + saliency_channels\n",
    "\n",
    "        # Time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, time_emb_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch, time_emb_dim)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch * 2, time_emb_dim)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(base_ch * 2, base_ch * 4, time_emb_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch * 4, base_ch * 2, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(base_ch * 4, base_ch * 2, time_emb_dim)\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch * 2, base_ch, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(base_ch * 2, base_ch, time_emb_dim)\n",
    "        \n",
    "        self.out_conv = nn.Conv2d(base_ch, saliency_channels, 1)\n",
    "\n",
    "    def forward(self, x_img, x_sal_noisy, t):\n",
    "        # x_img: (B, 3, H, W), x_sal_noisy: (B, 1, H, W), t: (B,) timestep index\n",
    "        t = t.float().unsqueeze(-1) / cfg.timesteps\n",
    "        t_emb = self.time_mlp(t)\n",
    "\n",
    "        x = torch.cat([x_img, x_sal_noisy], dim=1)\n",
    "        \n",
    "        e1 = self.enc1(x, t_emb)\n",
    "        p1 = self.pool1(e1)\n",
    "        e2 = self.enc2(p1, t_emb)\n",
    "        p2 = self.pool2(e2)\n",
    "\n",
    "        b = self.bottleneck(p2, t_emb)\n",
    "\n",
    "        u2 = self.up2(b)\n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1), t_emb)\n",
    "        u1 = self.up1(d2)\n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1), t_emb)\n",
    "\n",
    "        out = self.out_conv(d1)\n",
    "        return out\n",
    "\n",
    "model = UNetSaliency().to(cfg.device)\n",
    "print('Model params:', sum(p.numel() for p in model.parameters()) / 1e6, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce253e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyDDPM(nn.Module):\n",
    "    def __init__(self, model: nn.Module, timesteps: int = 1000, beta_start: float = 1e-4, beta_end: float = 0.02):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "        betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "       \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - alphas_cumprod))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, x_img, x_sal, t):\n",
    "        noise = torch.randn_like(x_sal)\n",
    "        x_noisy = self.q_sample(x_sal, t, noise)\n",
    "        noise_pred = self.model(x_img, x_noisy, t)\n",
    "        return nn.functional.mse_loss(noise_pred, noise)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x_img, t, x):\n",
    "        betas_t = self.betas[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_recip_alphas_t = torch.sqrt(1.0 / self.alphas[t]).view(-1, 1, 1, 1)\n",
    "       \n",
    "        # Predict noise\n",
    "        noise_pred = self.model(x_img, x, t)\n",
    "        model_mean = sqrt_recip_alphas_t * (x - betas_t / sqrt_one_minus_alphas_cumprod_t * noise_pred)\n",
    "       \n",
    "        if t[0] == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma_t = torch.sqrt(betas_t)\n",
    "            return model_mean + sigma_t * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, x_img, shape):\n",
    "        # x_img: condition, shape: (B, 1, H, W) saliency shape\n",
    "        x = torch.randn(shape, device=x_img.device)\n",
    "        B = x.shape[0]\n",
    "        for i in reversed(range(self.timesteps)):\n",
    "            t = torch.full((B,), i, device=x_img.device, dtype=torch.long)\n",
    "            x = self.p_sample(x_img, t, x)\n",
    "        return x\n",
    "\n",
    "ddpm = SaliencyDDPM(model, timesteps=cfg.timesteps, beta_start=cfg.beta_start, beta_end=cfg.beta_end).to(cfg.device)\n",
    "ddpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(ddpm.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "\n",
    "def train_epoch(epoch_idx: int):\n",
    "    ddpm.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for imgs, sal_maps, _ in train_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        \n",
    "        b = imgs.size(0)\n",
    "        t = torch.randint(0, cfg.timesteps, (b,), device=cfg.device).long()\n",
    "\n",
    "        loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    return total_loss / max(1, num_batches)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(epoch_idx: int):\n",
    "    ddpm.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for imgs, sal_maps, _ in val_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        b = imgs.size(0)\n",
    "        t = torch.randint(0, cfg.timesteps, (b,), device=cfg.device).long()\n",
    "        loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    return total_loss / max(1, num_batches)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    train_loss = train_epoch(epoch)\n",
    "    val_loss = validate_epoch(epoch)\n",
    "    print(f\"Epoch {epoch+1}/{cfg.num_epochs} | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(ddpm.state_dict(), cfg.checkpoint_path)\n",
    "        print('  Saved new best model to', cfg.checkpoint_path)\n",
    "print('Training complete. Best val loss:', best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f86db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(cfg.checkpoint_path):\n",
    "    ddpm.load_state_dict(torch.load(cfg.checkpoint_path, map_location=cfg.device))\n",
    "    print('Loaded best checkpoint.')\n",
    "else:\n",
    "    print('Checkpoint not found; using current model weights.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d6f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_samples_from_loader(loader, num_batches: int = 1, tag: str = 'val'):\n",
    "    ddpm.eval()\n",
    "    batch_count = 0\n",
    "    for imgs, sal_maps, fnames in loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        B = imgs.size(0)\n",
    "\n",
    "        # Sample saliency maps via reverse diffusion\n",
    "        samples = ddpm.sample(imgs, shape=sal_maps.shape)\n",
    "        samples = samples.clamp(0.0, 1.0)\n",
    "        \n",
    "        for i in range(B):\n",
    "            img = imgs[i].cpu()\n",
    "            gt = sal_maps[i, 0].cpu()\n",
    "            pred = samples[i, 0].cpu()\n",
    "\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "            axs[0].imshow(img.permute(1, 2, 0))\n",
    "            axs[0].set_title('Image')\n",
    "            axs[0].axis('off')\n",
    "\n",
    "            axs[1].imshow(gt, cmap='hot')\n",
    "            axs[1].set_title('GT saliency')\n",
    "            axs[1].axis('off')\n",
    "\n",
    "            axs[2].imshow(pred, cmap='hot')\n",
    "            axs[2].set_title('Pred saliency (diffusion)')\n",
    "            axs[2].axis('off')\n",
    "            plt.tight_layout()\n",
    "            save_path = os.path.join(cfg.sample_dir, f'{tag}_{fnames[i]}')\n",
    "            fig.savefig(save_path, dpi=150)\n",
    "            plt.close(fig)\n",
    "\n",
    "        batch_count += 1\n",
    "        if batch_count >= num_batches:\n",
    "            break\n",
    "\n",
    "    print(f'Saved sample visualizations to {cfg.sample_dir} (tag={tag}).')\n",
    "# Generate a few validation samples\n",
    "if len(val_dataset) > 0:\n",
    "    generate_samples_from_loader(val_loader, num_batches=1, tag='val')\n",
    "else:\n",
    "    print('No validation data; skipping sample generation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bf8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, image_size=(256, 192)):\n",
    "        self.img_dir = img_dir\n",
    "        self.files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.files.sort()\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((image_size[1], image_size[0])),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        img_path = os.path.join(self.img_dir, fname)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img, fname\n",
    "\n",
    "test_dataset = TestImageDataset(cfg.test_img_dir, image_size=cfg.image_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "print('Test dataset size:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_test_inference():\n",
    "    ddpm.eval()\n",
    "    for imgs, fnames in test_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        B, _, H, W = imgs.shape\n",
    "        sal_shape = (B, 1, H, W)\n",
    "        preds = ddpm.sample(imgs, shape=sal_shape)\n",
    "        preds = preds.clamp(0.0, 1.0).cpu().numpy()\n",
    "\n",
    "        for i in range(B):\n",
    "            pred_map = (preds[i, 0] * 255).astype(np.uint8)\n",
    "            im = Image.fromarray(pred_map)\n",
    "            save_name = os.path.splitext(fnames[i])[0] + '_saliency.png'\n",
    "            save_path = os.path.join(cfg.output_dir, save_name)\n",
    "            im.save(save_path)\n",
    "    print('Saved predicted saliency maps for test images to', cfg.output_dir)\n",
    "\n",
    "if len(test_dataset) > 0:\n",
    "    run_test_inference()\n",
    "else:\n",
    "    print('No test images found; skipping test inference.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
