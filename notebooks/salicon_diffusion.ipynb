{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa0b321",
   "metadata": {},
   "source": [
    "# Setup Config Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    train_img_dir = os.path.abspath(os.path.join('..', 'data', 'train'))\n",
    "    val_img_dir = os.path.abspath(os.path.join('..', 'data', 'val'))\n",
    "    test_img_dir = os.path.abspath(os.path.join('..', 'data', 'test'))\n",
    "\n",
    "    train_fixations_json = os.path.abspath(os.path.join('..', 'data', 'fixations_train2014.json'))\n",
    "    val_fixations_json = os.path.abspath(os.path.join('..', 'data', 'fixations_val2014.json'))\n",
    "\n",
    "    image_size = (256, 192)\n",
    "    saliency_size = (256, 192)\n",
    "    saliency_gaussian_sigma = 8.0\n",
    "\n",
    "    batch_size = 8\n",
    "    num_workers = 4\n",
    "    num_epochs = 60\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    timesteps = 500\n",
    "    beta_start = 1e-4\n",
    "    beta_end = 0.02\n",
    "\n",
    "    val_subset_size = 500  # set to None to use full val set during validation\n",
    "\n",
    "    cfg_use_guidance = True\n",
    "    cfg_guidance_prob = 0.1  # probability of dropping conditioning image during training\n",
    "    cfg_guidance_scale = 1.5  # guidance strength during sampling (>1 increases saliency sharpness)\n",
    "\n",
    "    use_ddim = False        # toggle DDIM sampling for ablation\n",
    "    ddim_steps = 50          # number of DDIM inference steps\n",
    "    ddim_eta = 0.0           # eta=0 -> deterministic DDIM\n",
    "\n",
    "    use_self_attention = True  # toggle self-attention layers in UNet\n",
    "    use_cross_attention = True  # toggle cross-attention layers in UNet\n",
    "\n",
    "    output_dir = os.path.abspath('./saliency_diffusion_outputs')\n",
    "    checkpoint_path = os.path.join(output_dir, 'saliency_diffusion_unet.pt')\n",
    "    sample_dir = os.path.join(output_dir, 'samples')\n",
    "    gt_saliency_dir = os.path.abspath('./saliency_ground_truth')\n",
    "    train_saliency_dir = os.path.join(gt_saliency_dir, 'train')\n",
    "    val_saliency_dir = os.path.join(gt_saliency_dir, 'val')\n",
    "\n",
    "    device = 'cuda' if os.environ.get('CUDA_VISIBLE_DEVICES') is not None else 'cuda' if __import__('torch').cuda.is_available() else 'cpu'\n",
    "\n",
    "cfg = Config()\n",
    "for path in [cfg.output_dir, cfg.sample_dir, cfg.gt_saliency_dir, cfg.train_saliency_dir, cfg.val_saliency_dir]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "cfg.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73d6a0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import lru_cache\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import wandb\n",
    "\n",
    "print('Using device:', cfg.device)\n",
    "print('Train dir exists:', os.path.isdir(cfg.train_img_dir))\n",
    "print('Val dir exists:', os.path.isdir(cfg.val_img_dir))\n",
    "print('Test dir exists:', os.path.isdir(cfg.test_img_dir))\n",
    "print('Train fixations JSON exists:', os.path.isfile(cfg.train_fixations_json))\n",
    "print('Val fixations JSON exists:', os.path.isfile(cfg.val_fixations_json))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a97ef",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing: JSON to dense saliency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbbc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: str) -> Dict:\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "@lru_cache(maxsize=32)\n",
    "def _gaussian_kernel1d(radius: int, sigma: float) -> np.ndarray:\n",
    "    ax = np.arange(-radius, radius + 1, dtype=np.float32)\n",
    "    kernel = np.exp(-(ax ** 2) / (2.0 * sigma ** 2))\n",
    "    kernel_sum = kernel.sum()\n",
    "    if kernel_sum > 0:\n",
    "        kernel /= kernel_sum\n",
    "    return kernel.astype(np.float32)\n",
    "\n",
    "def apply_gaussian_blur(array: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    if sigma <= 0:\n",
    "        return array\n",
    "    radius = max(1, int(round(3.0 * sigma)))\n",
    "    kernel = _gaussian_kernel1d(radius, float(sigma))\n",
    "    array = array.astype(np.float32, copy=False)\n",
    "    blurred = np.apply_along_axis(lambda m: np.convolve(m, kernel, mode='same'), axis=0, arr=array)\n",
    "    blurred = np.apply_along_axis(lambda m: np.convolve(m, kernel, mode='same'), axis=1, arr=blurred)\n",
    "    return blurred.astype(np.float32)\n",
    "    \n",
    "def parse_fixations_json(fixations: Dict, split: str, cache_dir: Optional[str] = None) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Parse SALICON-style fixation annotations into dense saliency maps.\"\"\"\n",
    "    if not fixations:\n",
    "        return {}\n",
    "\n",
    "    if cache_dir:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    # Map image id to file metadata\n",
    "    img_meta: Dict[int, str] = {}\n",
    "    dims: Dict[str, Tuple[int, int]] = {}\n",
    "    for img in fixations.get('images', []):\n",
    "        img_id = img.get('id')\n",
    "        fname = img.get('file_name')\n",
    "        if img_id is None or not fname:\n",
    "            continue\n",
    "        img_meta[img_id] = fname\n",
    "        width = img.get('width')\n",
    "        height = img.get('height')\n",
    "        if width and height:\n",
    "            dims[fname] = (int(width), int(height))\n",
    "\n",
    "    # Aggregate fixation points per image filename\n",
    "    mapping: Dict[str, List[Tuple[float, float]]] = {}\n",
    "    for ann in fixations.get('annotations', []):\n",
    "        img_id = ann.get('image_id')\n",
    "        fname = img_meta.get(img_id)\n",
    "        if not fname:\n",
    "            continue\n",
    "        pts = ann.get('fixations') or ann.get('points') or []\n",
    "        if not pts:\n",
    "            continue\n",
    "        bucket = mapping.setdefault(fname, [])\n",
    "        for p in pts:\n",
    "            if not isinstance(p, (list, tuple)) or len(p) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                row = float(p[0])\n",
    "                col = float(p[1])\n",
    "            except (TypeError, ValueError):\n",
    "                continue\n",
    "            if np.isnan(row) or np.isnan(col):\n",
    "                continue\n",
    "            # SALICON fixations are 1-indexed (row, col) == (y, x)\n",
    "            y = row - 1.0\n",
    "            x = col - 1.0\n",
    "            bucket.append((y, x))\n",
    "\n",
    "    saliency_maps: Dict[str, np.ndarray] = {}\n",
    "    W, H = cfg.saliency_size\n",
    "\n",
    "    processed = 0\n",
    "    filenames = sorted(set(img_meta.values()))\n",
    "    for fname in filenames:\n",
    "        sal_map = np.zeros((H, W), dtype=np.float32)\n",
    "        pts = mapping.get(fname, [])\n",
    "        orig_w, orig_h = dims.get(fname, (None, None))\n",
    "\n",
    "        if pts:\n",
    "            for y, x in pts:\n",
    "                if orig_w and orig_h and orig_w > 1 and orig_h > 1:\n",
    "                    sy = (y / max(1.0, orig_h - 1)) * (H - 1)\n",
    "                    sx = (x / max(1.0, orig_w - 1)) * (W - 1)\n",
    "                else:\n",
    "                    sy, sx = y, x\n",
    "\n",
    "                iy = int(round(np.clip(sy, 0, H - 1)))\n",
    "                ix = int(round(np.clip(sx, 0, W - 1)))\n",
    "                sal_map[iy, ix] += 1.0\n",
    "\n",
    "            if cfg.saliency_gaussian_sigma and cfg.saliency_gaussian_sigma > 0:\n",
    "                sal_map = apply_gaussian_blur(sal_map, cfg.saliency_gaussian_sigma)\n",
    "\n",
    "            if sal_map.max() > 0:\n",
    "                sal_map /= sal_map.max()\n",
    "\n",
    "        saliency_maps[fname] = sal_map.astype(np.float32)\n",
    "        processed += 1\n",
    "\n",
    "        if cache_dir:\n",
    "            stem = os.path.splitext(fname)[0]\n",
    "            cache_path = os.path.join(cache_dir, f'{stem}.npy')\n",
    "            np.save(cache_path, sal_map)\n",
    "\n",
    "    msg_prefix = f\"{split.capitalize()} saliency maps\"\n",
    "    if cache_dir:\n",
    "        print(f\"{msg_prefix}: saved {processed} arrays to {cache_dir}\")\n",
    "    else:\n",
    "        print(f\"{msg_prefix}: generated {processed} arrays in memory\")\n",
    "\n",
    "    return saliency_maps\n",
    "    \n",
    "try:\n",
    "    train_fix_raw = load_json(cfg.train_fixations_json)\n",
    "    val_fix_raw = load_json(cfg.val_fixations_json)\n",
    "    print('Loaded fixation JSONs.')\n",
    "except Exception as e:\n",
    "    print('Error loading or parsing fixation JSONs. Please adapt parse_fixations_json to your format.')\n",
    "    print(e)\n",
    "    train_fix_raw, val_fix_raw = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26a8b24",
   "metadata": {},
   "source": [
    "# Parse JSON data (check if cached first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12431126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cached saliency maps already exist on disk\n",
    "def check_saliency_cache_exists(cache_dir: str, img_dir: str, min_coverage: float = 0.9) -> bool:\n",
    "    \"\"\"Check if cached saliency maps exist for most images in the directory.\"\"\"\n",
    "    if not os.path.isdir(cache_dir):\n",
    "        return False\n",
    "    \n",
    "    # Count images in img_dir\n",
    "    img_files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    if len(img_files) == 0:\n",
    "        return False\n",
    "    \n",
    "    # Count cached .npy files\n",
    "    cached_files = [f for f in os.listdir(cache_dir) if f.endswith('.npy')]\n",
    "    \n",
    "    coverage = len(cached_files) / len(img_files)\n",
    "    print(f'  Cache: {len(cached_files)}/{len(img_files)} files ({coverage*100:.1f}% coverage)')\n",
    "    return coverage >= min_coverage\n",
    "\n",
    "# Try to use cached saliency maps if they exist\n",
    "print('Checking for cached saliency maps...')\n",
    "train_cache_exists = check_saliency_cache_exists(cfg.train_saliency_dir, cfg.train_img_dir)\n",
    "val_cache_exists = check_saliency_cache_exists(cfg.val_saliency_dir, cfg.val_img_dir)\n",
    "\n",
    "if train_cache_exists and val_cache_exists:\n",
    "    print('Using cached saliency maps from disk')\n",
    "    train_saliency_maps = {}  # Empty dict - dataset will load from cache_dir\n",
    "    val_saliency_maps = {}\n",
    "else:\n",
    "    print('Cache not found or incomplete - parsing fixations JSON...')\n",
    "    train_saliency_maps = parse_fixations_json(train_fix_raw, split='train', cache_dir=cfg.train_saliency_dir) if train_fix_raw is not None else {}\n",
    "    val_saliency_maps = parse_fixations_json(val_fix_raw, split='val', cache_dir=cfg.val_saliency_dir) if val_fix_raw is not None else {}\n",
    "\n",
    "print('Train saliency entries in memory:', len(train_saliency_maps))\n",
    "print('Val saliency entries in memory:', len(val_saliency_maps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8e508",
   "metadata": {},
   "source": [
    "# Pytorch Dataset & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliconSaliencyDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, saliency_maps: Dict[str, np.ndarray], image_size=cfg.image_size, saliency_cache_dir: Optional[str] = None):\n",
    "        self.img_dir = img_dir\n",
    "        self.saliency_maps = saliency_maps or {}\n",
    "        self.saliency_cache_dir = saliency_cache_dir\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Collect only images that have saliency info (either in-memory or cached on disk)\n",
    "        self.image_files = [f for f in os.listdir(img_dir) if self._has_saliency(f)]\n",
    "        self.image_files.sort()\n",
    "\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size[1], image_size[0])),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        # Saliency maps are kept in [0,1] range, single-channel\n",
    "\n",
    "    def _has_saliency(self, fname: str) -> bool:\n",
    "        if fname in self.saliency_maps:\n",
    "            return True\n",
    "        if not self.saliency_cache_dir:\n",
    "            return False\n",
    "        cache_path = os.path.join(self.saliency_cache_dir, f\"{os.path.splitext(fname)[0]}.npy\")\n",
    "        return os.path.isfile(cache_path)\n",
    "\n",
    "    def _load_saliency(self, fname: str) -> np.ndarray:\n",
    "        sal_map = self.saliency_maps.get(fname)\n",
    "        if sal_map is None and self.saliency_cache_dir:\n",
    "            cache_path = os.path.join(self.saliency_cache_dir, f\"{os.path.splitext(fname)[0]}.npy\")\n",
    "            if os.path.isfile(cache_path):\n",
    "                sal_map = np.load(cache_path, allow_pickle=False)\n",
    "        if sal_map is None:\n",
    "            raise KeyError(f'No saliency map found for {fname}.')\n",
    "        return sal_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, fname)\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.img_transform(img)\n",
    "\n",
    "        sal_map = self._load_saliency(fname)\n",
    "        H, W = self.image_size[1], self.image_size[0]\n",
    "        if sal_map.shape != (H, W):\n",
    "            sal_img = Image.fromarray((sal_map * 255).astype(np.uint8))\n",
    "            sal_img = sal_img.resize((W, H), resample=Image.BILINEAR)\n",
    "            sal_tensor = torch.from_numpy(np.array(sal_img)).float() / 255.0\n",
    "        else:\n",
    "            sal_tensor = torch.from_numpy(sal_map).float()\n",
    "        sal_tensor = sal_tensor.unsqueeze(0)  # (1, H, W)\n",
    "\n",
    "        return img, sal_tensor, fname\n",
    "    \n",
    "train_dataset = SaliconSaliencyDataset(\n",
    "    cfg.train_img_dir,\n",
    "    train_saliency_maps,\n",
    "    image_size=cfg.image_size,\n",
    "    saliency_cache_dir=cfg.train_saliency_dir\n",
    ")\n",
    "val_dataset = SaliconSaliencyDataset(\n",
    "    cfg.val_img_dir,\n",
    "    val_saliency_maps,\n",
    "    image_size=cfg.image_size,\n",
    "    saliency_cache_dir=cfg.val_saliency_dir\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "print('Train dataset size:', len(train_dataset))\n",
    "print('Val dataset size:', len(val_dataset))\n",
    "\n",
    "batch = next(iter(train_loader)) if len(train_dataset) > 0 else None\n",
    "if batch is not None:\n",
    "    imgs, sal_maps, fnames = batch\n",
    "    print('Batch shapes:', imgs.shape, sal_maps.shape)\n",
    "    grid = vutils.make_grid(imgs, nrow=min(4, imgs.size(0)))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.title('Sample input images')\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    grid_sal = vutils.make_grid(sal_maps, nrow=min(4, sal_maps.size(0)))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.title('Sample ground truth saliency maps')\n",
    "    plt.imshow(grid_sal[0].cpu(), cmap='gray', vmin=0.0, vmax=1.0) #cmap = hot\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Warning: Train dataset is empty. Check your paths and JSON mapping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fefa6cc",
   "metadata": {},
   "source": [
    "# Unet Architecture\n",
    "- ResidualBlock: Conv blocks with GroupNorm, SiLU activation, and optional time embedding injection\n",
    "- SpatialSelfAttention: Multi-head self-attention over spatial dimensions (bottleneck)\n",
    "- sinusoidal_time_embedding: Positional encoding for diffusion timesteps\n",
    "- UNetSaliency: 3-level encoder-decoder with skip connections and attention bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _groupnorm_groups(num_channels: int) -> int:\n",
    "    for group in [32, 16, 8, 4, 2]:\n",
    "        if num_channels % group == 0:\n",
    "            return group\n",
    "    return 1\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, time_emb_dim: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(_groupnorm_groups(in_ch), in_ch, eps=1e-6)\n",
    "        self.norm2 = nn.GroupNorm(_groupnorm_groups(out_ch), out_ch, eps=1e-6)\n",
    "        self.act = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.residual = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "        if time_emb_dim is not None:\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(time_emb_dim, out_ch)\n",
    "            )\n",
    "        else:\n",
    "            self.time_mlp = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_emb: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        h = self.norm1(x)\n",
    "        h = self.act(h)\n",
    "        h = self.conv1(h)\n",
    "        if self.time_mlp is not None and t_emb is not None:\n",
    "            h = h + self.time_mlp(t_emb)[:, :, None, None]\n",
    "        h = self.norm2(h)\n",
    "        h = self.act(h)\n",
    "        h = self.conv2(h)\n",
    "        return h + self.residual(x)\n",
    "\n",
    "\n",
    "class SpatialSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention over spatial dimensions with pre-norm and residual.\"\"\"\n",
    "    def __init__(self, channels: int, num_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = channels // num_heads\n",
    "        assert channels % num_heads == 0, \"channels must be divisible by num_heads\"\n",
    "        \n",
    "        self.norm = nn.GroupNorm(_groupnorm_groups(channels), channels, eps=1e-6)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n",
    "        self.proj = nn.Conv2d(channels, channels, 1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        residual = x\n",
    "        \n",
    "        # Pre-norm\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        qkv = self.qkv(x)  # (B, 3*C, H, W)\n",
    "        qkv = qkv.view(B, 3, self.num_heads, self.head_dim, H * W)\n",
    "        qkv = qkv.permute(1, 0, 2, 4, 3)  # (3, B, heads, HW, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each (B, heads, HW, head_dim)\n",
    "        \n",
    "        # Attention\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # (B, heads, HW, HW)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attn, v)  # (B, heads, HW, head_dim)\n",
    "        out = out.permute(0, 1, 3, 2).reshape(B, C, H, W)\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out + residual\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention: query from one feature map, key/value from another, with pre-norm and residual.\"\"\"\n",
    "    def __init__(self, channels: int, context_channels: int, num_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = channels // num_heads\n",
    "        assert channels % num_heads == 0, \"channels must be divisible by num_heads\"\n",
    "        \n",
    "        self.norm_q = nn.GroupNorm(_groupnorm_groups(channels), channels, eps=1e-6)\n",
    "        self.norm_kv = nn.GroupNorm(_groupnorm_groups(context_channels), context_channels, eps=1e-6)\n",
    "        \n",
    "        self.q_proj = nn.Conv2d(channels, channels, 1)\n",
    "        self.k_proj = nn.Conv2d(context_channels, channels, 1)\n",
    "        self.v_proj = nn.Conv2d(context_channels, channels, 1)\n",
    "        self.out_proj = nn.Conv2d(channels, channels, 1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x: torch.Tensor, context: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        _, Cc, Hc, Wc = context.shape\n",
    "        residual = x\n",
    "        \n",
    "        # Pre-norm\n",
    "        x = self.norm_q(x)\n",
    "        context = self.norm_kv(context)\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.q_proj(x).view(B, self.num_heads, self.head_dim, H * W).permute(0, 1, 3, 2)\n",
    "        k = self.k_proj(context).view(B, self.num_heads, self.head_dim, Hc * Wc).permute(0, 1, 3, 2)\n",
    "        v = self.v_proj(context).view(B, self.num_heads, self.head_dim, Hc * Wc).permute(0, 1, 3, 2)\n",
    "        \n",
    "        # Attention\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.permute(0, 1, 3, 2).reshape(B, C, H, W)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        return out + residual\n",
    "\n",
    "\n",
    "# Sinusoidal positional time embeddings\n",
    "def sinusoidal_time_embedding(t: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    # t: (B,) in [0, timesteps)\n",
    "    device = t.device\n",
    "    half_dim = dim // 2\n",
    "    # log space frequencies\n",
    "    emb_factor = math.log(10000) / max(1, half_dim - 1)\n",
    "    # shape (half_dim,)\n",
    "    freqs = torch.exp(torch.arange(0, half_dim, device=device) * (-emb_factor))\n",
    "    # normalize t to [0,1]\n",
    "    t_norm = t.float() / max(1, cfg.timesteps - 1)\n",
    "    # outer product (B, half_dim)\n",
    "    angles = t_norm[:, None] * freqs[None, :]\n",
    "    emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "    if dim % 2 == 1:\n",
    "        # pad one channel if odd dim\n",
    "        emb = torch.nn.functional.pad(emb, (0, 1), mode='constant', value=0.0)\n",
    "    return emb\n",
    "\n",
    "class UNetSaliency(nn.Module):\n",
    "    def __init__(self, img_channels=3, saliency_channels=1, base_ch=64, time_emb_dim=128, pos_dim=64, attn_heads=4):\n",
    "        super().__init__()\n",
    "        in_ch = img_channels + saliency_channels\n",
    "\n",
    "        # Time embedding: sinusoidal -> linear projection -> MLP\n",
    "        self.pos_dim = pos_dim\n",
    "        self.time_proj = nn.Linear(pos_dim, time_emb_dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Encoder (deeper: 3 downsamples + bottleneck)\n",
    "        self.enc1 = ResidualBlock(in_ch, base_ch, time_emb_dim)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ResidualBlock(base_ch, base_ch * 2, time_emb_dim)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ResidualBlock(base_ch * 2, base_ch * 4, time_emb_dim)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck with attention\n",
    "        self.bottleneck = ResidualBlock(base_ch * 4, base_ch * 8, time_emb_dim)\n",
    "        self.bottleneck_self_attn = SpatialSelfAttention(base_ch * 8, num_heads=attn_heads)\n",
    "        \n",
    "        # Image encoder for cross-attention context (downsampled to bottleneck resolution)\n",
    "        self.img_encoder = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, base_ch * 2, 3, stride=2, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(base_ch * 2, base_ch * 4, 3, stride=2, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(base_ch * 4, base_ch * 8, 3, stride=2, padding=1),\n",
    "        )\n",
    "        self.bottleneck_cross_attn = CrossAttention(base_ch * 8, base_ch * 8, num_heads=attn_heads)\n",
    "\n",
    "        # Decoder (mirror)\n",
    "        self.up3 = nn.ConvTranspose2d(base_ch * 8, base_ch * 4, 2, stride=2)\n",
    "        self.dec3 = ResidualBlock(base_ch * 8, base_ch * 4, time_emb_dim)\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch * 4, base_ch * 2, 2, stride=2)\n",
    "        self.dec2 = ResidualBlock(base_ch * 4, base_ch * 2, time_emb_dim)\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch * 2, base_ch, 2, stride=2)\n",
    "        self.dec1 = ResidualBlock(base_ch * 2, base_ch, time_emb_dim)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(base_ch, saliency_channels, 1)\n",
    "\n",
    "    def forward(self, x_img, x_sal_noisy, t):\n",
    "        # x_img: (B, 3, H, W), x_sal_noisy: (B, 1, H, W), t: (B,) timestep index\n",
    "        # Build sinusoidal positional embedding\n",
    "        pos = sinusoidal_time_embedding(t, self.pos_dim)\n",
    "        t_emb = self.time_proj(pos)\n",
    "        t_emb = self.time_mlp(t_emb)\n",
    "\n",
    "        x = torch.cat([x_img, x_sal_noisy], dim=1)\n",
    "        \n",
    "        e1 = self.enc1(x, t_emb)\n",
    "        p1 = self.pool1(e1)\n",
    "        e2 = self.enc2(p1, t_emb)\n",
    "        p2 = self.pool2(e2)\n",
    "        e3 = self.enc3(p2, t_emb)\n",
    "        p3 = self.pool3(e3)\n",
    "\n",
    "        # Bottleneck with self-attention and cross-attention\n",
    "        b = self.bottleneck(p3, t_emb)\n",
    "        if cfg.use_self_attention:\n",
    "            b = self.bottleneck_self_attn(b)\n",
    "        \n",
    "        if cfg.use_cross_attention:\n",
    "            img_ctx = self.img_encoder(x_img)\n",
    "            b = self.bottleneck_cross_attn(b, img_ctx)\n",
    "\n",
    "        u3 = self.up3(b)\n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1), t_emb)\n",
    "        u2 = self.up2(d3)\n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1), t_emb)\n",
    "        u1 = self.up1(d2)\n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1), t_emb)\n",
    "\n",
    "        out = self.out_conv(d1)\n",
    "        return out\n",
    "\n",
    "model = UNetSaliency().to(cfg.device)\n",
    "print('Model params:', sum(p.numel() for p in model.parameters()) / 1e6, 'M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04824a3f",
   "metadata": {},
   "source": [
    "# Diffusion Model\n",
    "- SaliencyDDPM: Wrapper for denoising diffusion probabilistic model\n",
    "- Forward diffusion: q_sample adds noise according to variance schedule\n",
    "- Loss: Noise MSE + L1/MSE reconstruction + SSIM on predicted \n",
    "- Classifier-free guidance: _predict_noise with unconditional/conditional blending\n",
    "- Reverse sampling: p_sample (stochastic DDPM) and ddim_sample (deterministic DDIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce253e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyDDPM(nn.Module):\n",
    "    def __init__(self, model: nn.Module, timesteps=cfg.timesteps, beta_start=cfg.beta_start, beta_end=cfg.beta_end):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "        betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = torch.cat([torch.ones(1), alphas_cumprod[:-1]])\n",
    "        \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt((1.0 - alphas_cumprod) / alphas_cumprod))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        sqrt_recip_alphas_t = self.sqrt_recip_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_recipm1_alphas_t = self.sqrt_recipm1_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_recip_alphas_t * x_t - sqrt_recipm1_alphas_t * noise\n",
    "\n",
    "    def _predict_noise(self, x_img, x, t, guidance_scale=cfg.cfg_guidance_scale):\n",
    "        if guidance_scale is None or guidance_scale <= 1.0:\n",
    "            return self.model(x_img, x, t)\n",
    "        zeros = torch.zeros_like(x_img)\n",
    "        model_in = torch.cat([zeros, x_img], dim=0)\n",
    "        x_rep = torch.cat([x, x], dim=0)\n",
    "        t_rep = torch.cat([t, t], dim=0)\n",
    "        noise_pred = self.model(model_in, x_rep, t_rep)\n",
    "        noise_uncond, noise_cond = noise_pred.chunk(2, dim=0)\n",
    "        return noise_uncond + guidance_scale * (noise_cond - noise_uncond)\n",
    "\n",
    "    def p_losses(self, x_img, x_sal, t):\n",
    "        noise = torch.randn_like(x_sal)\n",
    "        x_noisy = self.q_sample(x_sal, t, noise)\n",
    "        noise_pred = self._predict_noise(x_img, x_noisy, t)\n",
    "        # Standard noise MSE\n",
    "        loss_noise = nn.functional.mse_loss(noise_pred, noise)\n",
    "        # Auxiliary reconstruction loss on x0 to better match GT\n",
    "        x0_pred = self.predict_start_from_noise(x_noisy, t, noise_pred)\n",
    "        x0_pred = x0_pred.clamp(0.0, 1.0)\n",
    "        # Mix L1 and MSE for sharper alignment\n",
    "        loss_l1 = nn.functional.l1_loss(x0_pred, x_sal)\n",
    "        loss_mse = nn.functional.mse_loss(x0_pred, x_sal)\n",
    "        # Lightweight SSIM\n",
    "        def ssim_simple(a, b):\n",
    "            mu_a = a.mean(dim=(-2, -1), keepdim=True)\n",
    "            mu_b = b.mean(dim=(-2, -1), keepdim=True)\n",
    "            var_a = a.var(dim=(-2, -1), keepdim=True)\n",
    "            var_b = b.var(dim=(-2, -1), keepdim=True)\n",
    "            cov_ab = ((a - mu_a) * (b - mu_b)).mean(dim=(-2, -1), keepdim=True)\n",
    "            C1, C2 = 0.01**2, 0.03**2\n",
    "            ssim_map = ((2*mu_a*mu_b + C1) * (2*cov_ab + C2)) / ((mu_a**2 + mu_b**2 + C1) * (var_a + var_b + C2))\n",
    "            return 1.0 - ssim_map.mean()\n",
    "        loss_ssim = ssim_simple(x0_pred, x_sal)\n",
    "        # Weights\n",
    "        w_noise, w_l1, w_mse, w_ssim = 1.0, 0.5, 0.5, 0.5\n",
    "        return w_noise*loss_noise + w_l1*loss_l1 + w_mse*loss_mse + w_ssim*loss_ssim\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x_img, t, x, guidance_scale=cfg.cfg_guidance_scale):\n",
    "        betas_t = self.betas[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_recip_alphas_t = torch.sqrt(1.0 / self.alphas[t]).view(-1, 1, 1, 1)\n",
    "        \n",
    "        noise_pred = self._predict_noise(x_img, x, t, guidance_scale)\n",
    "        model_mean = sqrt_recip_alphas_t * (x - betas_t / sqrt_one_minus_alphas_cumprod_t * noise_pred)\n",
    "        if t[0] == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma_t = torch.sqrt(betas_t)\n",
    "            return model_mean + sigma_t * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, x_img, shape, guidance_scale=cfg.cfg_guidance_scale, use_ddim=cfg.use_ddim, ddim_steps=cfg.ddim_steps, ddim_eta=cfg.ddim_eta):\n",
    "        if use_ddim:\n",
    "            steps = ddim_steps if ddim_steps is not None else self.timesteps\n",
    "            return self.ddim_sample(x_img, shape, steps, guidance_scale, ddim_eta)\n",
    "        x = torch.randn(shape, device=x_img.device)\n",
    "        B = x.shape[0]\n",
    "        for i in reversed(range(self.timesteps)):\n",
    "            t = torch.full((B,), i, device=x_img.device, dtype=torch.long)\n",
    "            x = self.p_sample(x_img, t, x, guidance_scale)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(self, x_img, shape, steps: int, guidance_scale=cfg.cfg_guidance_scale, eta=cfg.ddim_eta):\n",
    "        device = x_img.device\n",
    "        x = torch.randn(shape, device=device)\n",
    "        B = x.shape[0]\n",
    "        steps = max(1, min(int(steps), self.timesteps))\n",
    "        seq = torch.linspace(self.timesteps - 1, 0, steps, device=device, dtype=torch.long)\n",
    "        eps = 1e-6\n",
    "        for idx in range(steps):\n",
    "            t_val = int(seq[idx].item())\n",
    "            t = torch.full((B,), t_val, device=device, dtype=torch.long)\n",
    "            noise_pred = self._predict_noise(x_img, x, t, guidance_scale)\n",
    "\n",
    "            alpha_cum = self.alphas_cumprod[t].view(B, 1, 1, 1).clamp(min=eps, max=1.0)\n",
    "            sqrt_alpha_cum = torch.sqrt(alpha_cum)\n",
    "            sqrt_one_minus_alpha = torch.sqrt((1.0 - alpha_cum).clamp(min=0.0))\n",
    "            x0_pred = (x - sqrt_one_minus_alpha * noise_pred) / sqrt_alpha_cum\n",
    "            x0_pred = x0_pred.clamp(0.0, 1.0)\n",
    "\n",
    "            if idx == steps - 1:\n",
    "                x = x0_pred\n",
    "                break\n",
    "\n",
    "            t_next_val = int(seq[idx + 1].item())\n",
    "            t_next = torch.full((B,), t_next_val, device=device, dtype=torch.long)\n",
    "            alpha_cum_next = self.alphas_cumprod[t_next].view(B, 1, 1, 1).clamp(min=eps, max=1.0)\n",
    "            sqrt_alpha_cum_next = torch.sqrt(alpha_cum_next)\n",
    "\n",
    "            sigma = eta * torch.sqrt(((1 - alpha_cum_next) / (1 - alpha_cum)).clamp(min=0.0))\n",
    "            sigma = sigma * torch.sqrt((1 - alpha_cum / alpha_cum_next).clamp(min=0.0))\n",
    "            dir_xt = torch.sqrt((1 - alpha_cum_next - sigma**2).clamp(min=0.0)) * noise_pred\n",
    "            noise = sigma * torch.randn_like(x) if eta > 0 else torch.zeros_like(x)\n",
    "\n",
    "            x = sqrt_alpha_cum_next * x0_pred + dir_xt + noise\n",
    "        return x\n",
    "\n",
    "\n",
    "ddpm = SaliencyDDPM(model, timesteps=cfg.timesteps, beta_start=cfg.beta_start, beta_end=cfg.beta_end).to(cfg.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9d51b",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "- W&B initialization with full config logging\n",
    "- AdamW optimizer + cosine annealing LR scheduler\n",
    "- Mixed precision (AMP) with GradScaler for faster training\n",
    "- Classifier-free guidance: 10% conditioning dropout via _maybe_drop_condition\n",
    "- Per-epoch + best checkpoint saving with full state (model, optimizer, scheduler)\n",
    "- Validation subset for faster epoch validation (val_subset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases just before training (after data preprocessing)\n",
    "wandb.init(\n",
    "    project=\"salicon-diffusion\",\n",
    "    config={\n",
    "        \"image_size\": cfg.image_size,\n",
    "        \"saliency_size\": cfg.saliency_size,\n",
    "        \"saliency_gaussian_sigma\": cfg.saliency_gaussian_sigma,\n",
    "        \"batch_size\": cfg.batch_size,\n",
    "        \"num_epochs\": cfg.num_epochs,\n",
    "        \"learning_rate\": cfg.learning_rate,\n",
    "        \"weight_decay\": cfg.weight_decay,\n",
    "        \"timesteps\": cfg.timesteps,\n",
    "        \"beta_start\": cfg.beta_start,\n",
    "        \"beta_end\": cfg.beta_end,\n",
    "        \"cfg_use_guidance\": cfg.cfg_use_guidance,\n",
    "        \"cfg_guidance_prob\": cfg.cfg_guidance_prob,\n",
    "        \"cfg_guidance_scale\": cfg.cfg_guidance_scale,\n",
    "        \"use_ddim\": cfg.use_ddim,\n",
    "        \"ddim_steps\": cfg.ddim_steps,\n",
    "        \"ddim_eta\": cfg.ddim_eta,\n",
    "    }\n",
    "\n",
    ")\n",
    "print(f\"W&B run initialized: {wandb.run.name}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(ddpm.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, cfg.num_epochs))\n",
    "\n",
    "# Mixed precision training for faster forward/backward passes\n",
    "scaler = torch.amp.GradScaler('cuda') if cfg.device == 'cuda' else None\n",
    "use_amp = cfg.device == 'cuda'\n",
    "\n",
    "# Directory for per-epoch checkpoints\n",
    "per_epoch_dir = os.path.join(cfg.output_dir, 'epochs')\n",
    "os.makedirs(per_epoch_dir, exist_ok=True)\n",
    "\n",
    "def _maybe_drop_condition(imgs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Randomly zero-out conditioning images for classifier-free guidance training.\"\"\"\n",
    "    if cfg.cfg_guidance_prob <= 0:\n",
    "        return imgs\n",
    "    b = imgs.size(0)\n",
    "    drop_mask = (torch.rand(b, device=imgs.device) < cfg.cfg_guidance_prob)\n",
    "    if drop_mask.any():\n",
    "        imgs = imgs.clone()\n",
    "        imgs[drop_mask] = 0.0\n",
    "    return imgs\n",
    "\n",
    "def train_epoch(epoch_idx: int):\n",
    "    ddpm.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for imgs, sal_maps, _ in train_loader:\n",
    "        imgs = imgs.to(cfg.device, non_blocking=True)\n",
    "        sal_maps = sal_maps.to(cfg.device, non_blocking=True)\n",
    "        imgs = _maybe_drop_condition(imgs)\n",
    "        # Normalize saliency per image for consistent targets\n",
    "        sal_maps = sal_maps / sal_maps.amax(dim=(-2, -1), keepdim=True).clamp(min=1e-6)\n",
    "        b = imgs.size(0)\n",
    "        t = torch.randint(0, cfg.timesteps, (b,), device=cfg.device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(cfg.device, enabled=use_amp):\n",
    "            loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "        if use_amp and scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    scheduler.step()\n",
    "    return total_loss / max(1, num_batches)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(epoch_idx: int, max_batches:int = None):\n",
    "    ddpm.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    if max_batches is None and cfg.val_subset_size is not None:\n",
    "        max_batches = cfg.val_subset_size // cfg.batch_size\n",
    "        \n",
    "    for imgs, sal_maps, _ in val_loader:\n",
    "        if max_batches is not None and num_batches >= max_batches:\n",
    "            break\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        sal_maps = sal_maps / sal_maps.amax(dim=(-2, -1), keepdim=True).clamp(min=1e-6)\n",
    "        b = imgs.size(0)\n",
    "        t = torch.randint(0, cfg.timesteps, (b,), device=cfg.device).long()\n",
    "        with torch.amp.autocast(cfg.device, enabled=use_amp):\n",
    "            loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    return total_loss / max(1, num_batches)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    train_loss = train_epoch(epoch)\n",
    "    val_loss = validate_epoch(epoch)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{cfg.num_epochs} | LR: {current_lr:.2e} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train/loss\": train_loss,\n",
    "        \"val/loss\": val_loss,\n",
    "        \"train/learning_rate\": current_lr,\n",
    "    })\n",
    "\n",
    "    # Build checkpoint payload\n",
    "    ckpt = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state': ddpm.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'scheduler_state': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "    # Save per-epoch\n",
    "    epoch_filename = os.path.join(per_epoch_dir, f'epoch_{epoch+1:03d}.pt')\n",
    "    torch.save(ckpt, epoch_filename)\n",
    "    # Save last\n",
    "    torch.save(ckpt, os.path.join(cfg.output_dir, 'last.pt'))\n",
    "    print(f'  Saved epoch checkpoint to {epoch_filename}')\n",
    "\n",
    "    # Update and save best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        ckpt['best_val_loss'] = best_val_loss\n",
    "        torch.save(ckpt, cfg.checkpoint_path)\n",
    "        print('  Updated best model ->', cfg.checkpoint_path)\n",
    "        \n",
    "        wandb.run.summary[\"best_val_loss\"] = best_val_loss\n",
    "        wandb.run.summary[\"best_epoch\"] = epoch + 1\n",
    "\n",
    "print('Training complete. Best val loss:', best_val_loss)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a51741",
   "metadata": {},
   "source": [
    "# Optional: Resume optimizer/scheduler from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = cfg.checkpoint_path\n",
    "if os.path.isfile(ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=cfg.device)\n",
    "    if isinstance(ckpt, dict):\n",
    "        if 'optimizer_state' in ckpt:\n",
    "            try:\n",
    "                optimizer.load_state_dict(ckpt['optimizer_state'])\n",
    "                print('Restored optimizer state from checkpoint.')\n",
    "            except Exception as e:\n",
    "                print('Failed to restore optimizer state:', e)\n",
    "        if 'scheduler_state' in ckpt:\n",
    "            try:\n",
    "                scheduler.load_state_dict(ckpt['scheduler_state'])\n",
    "                print('Restored scheduler state from checkpoint.')\n",
    "            except Exception as e:\n",
    "                print('Failed to restore scheduler state:', e)\n",
    "        best_val_loss = ckpt.get('best_val_loss', best_val_loss)\n",
    "        resume_epoch = ckpt.get('epoch', 0)\n",
    "        print(f'Resume info -> epoch: {resume_epoch}, best_val_loss: {best_val_loss:.4f}')\n",
    "    else:\n",
    "        print('Checkpoint does not contain optimizer/scheduler state (raw state_dict).')\n",
    "else:\n",
    "    print('No checkpoint found to resume training state.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f86db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = cfg.checkpoint_path\n",
    "# ckpt_path = \"../weights/saliency_diffusion_unet_best.pt\"\n",
    "if os.path.isfile(ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=cfg.device)\n",
    "    # Handle both raw state_dict and packaged checkpoint dict\n",
    "    state_dict = ckpt.get('model_state', ckpt)\n",
    "    ddpm.load_state_dict(state_dict)\n",
    "    print('Loaded best checkpoint from', ckpt_path)\n",
    "else:\n",
    "    print('Checkpoint not found; using current model weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be844e5a",
   "metadata": {},
   "source": [
    "# Sample Visualization (Image/GT/Pred Saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d6f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_samples_from_loader(loader, num_batches: int = 1, tag: str = 'val'):\n",
    "    ddpm.eval()\n",
    "    batch_count = 0\n",
    "    for imgs, sal_maps, fnames in loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        sal_maps = sal_maps / sal_maps.amax(dim=(-2, -1), keepdim=True).clamp(min=1e-6)\n",
    "        B = imgs.size(0)\n",
    "\n",
    "        samples = ddpm.sample(\n",
    "            imgs,\n",
    "            shape=sal_maps.shape,\n",
    "            guidance_scale=cfg.cfg_guidance_scale if cfg.cfg_guidance_scale > 0 else None,\n",
    "            use_ddim=cfg.use_ddim,\n",
    "            ddim_steps=cfg.ddim_steps,\n",
    "            ddim_eta=cfg.ddim_eta,\n",
    "        ).clamp(0.0, 1.0)\n",
    "        \n",
    "        # Correlation between predicted and GT saliency maps\n",
    "        def pearson(a, b):\n",
    "            a = a.view(a.size(0), -1)\n",
    "            b = b.view(b.size(0), -1)\n",
    "            a = a - a.mean(dim=1, keepdim=True)\n",
    "            b = b - b.mean(dim=1, keepdim=True)\n",
    "            num = (a*b).sum(dim=1)\n",
    "            den = torch.sqrt((a*a).sum(dim=1) * (b*b).sum(dim=1)).clamp(min=1e-6)\n",
    "            return (num/den).mean().item()\n",
    "        p_corr = pearson(samples, sal_maps)\n",
    "        print(f'Batch Pearson corr: {p_corr:.3f}')\n",
    "        \n",
    "        for i in range(B):\n",
    "            img = imgs[i].cpu()\n",
    "            gt = sal_maps[i, 0].cpu()\n",
    "            pred = samples[i, 0].cpu()\n",
    "\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "            axs[0].imshow(img.permute(1, 2, 0))\n",
    "            axs[0].set_title('Image')\n",
    "            axs[0].axis('off')\n",
    "\n",
    "            axs[1].imshow(gt, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            axs[1].set_title('GT saliency')\n",
    "            axs[1].axis('off')\n",
    "\n",
    "            axs[2].imshow(pred, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            axs[2].set_title('Pred saliency')\n",
    "            axs[2].axis('off')\n",
    "            plt.tight_layout()\n",
    "            save_path = os.path.join(cfg.sample_dir, f'{tag}_{fnames[i]}')\n",
    "            fig.savefig(save_path, dpi=150)\n",
    "            plt.close(fig)\n",
    "\n",
    "        batch_count += 1\n",
    "        if batch_count >= num_batches:\n",
    "            break\n",
    "\n",
    "    print(f'Saved sample visualizations to {cfg.sample_dir} (tag={tag}).')\n",
    "# Generate a few validation samples\n",
    "if len(val_dataset) > 0:\n",
    "    generate_samples_from_loader(val_loader, num_batches=1, tag='val')\n",
    "else:\n",
    "    print('No validation data; skipping sample generation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8f9db",
   "metadata": {},
   "source": [
    "# Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7202544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "class SaliencyMetrics:\n",
    "    \"\"\"Standard saliency evaluation metrics.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_map(s_map):\n",
    "        \"\"\"Normalize to [0, 1] range.\"\"\"\n",
    "        s_map = s_map.astype(np.float32)\n",
    "        if s_map.max() > s_map.min():\n",
    "            s_map = (s_map - s_map.min()) / (s_map.max() - s_map.min())\n",
    "        return s_map\n",
    "    \n",
    "    @staticmethod\n",
    "    def auc_judd(pred_map, fixation_map):\n",
    "        \"\"\"\n",
    "        AUC-Judd: Area under ROC curve treating fixated pixels as positives.\n",
    "        \n",
    "        Args:\n",
    "            pred_map: Predicted saliency map (H, W) in [0, 1]\n",
    "            fixation_map: Binary fixation map (H, W) where 1=fixation, 0=background\n",
    "        \"\"\"\n",
    "        pred_map = SaliencyMetrics.normalize_map(pred_map)\n",
    "        fixation_map = fixation_map.astype(bool)\n",
    "        \n",
    "        # Get saliency values at fixated and non-fixated locations\n",
    "        fixated_values = pred_map[fixation_map]\n",
    "        non_fixated_values = pred_map[~fixation_map]\n",
    "        \n",
    "        if len(fixated_values) == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # Compute AUC using all background pixels\n",
    "        all_values = np.concatenate([fixated_values, non_fixated_values])\n",
    "        labels = np.concatenate([np.ones(len(fixated_values)), np.zeros(len(non_fixated_values))])\n",
    "        \n",
    "        # Sort by saliency values\n",
    "        sorted_indices = np.argsort(all_values)[::-1]\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        \n",
    "        # Compute AUC via trapezoidal rule\n",
    "        tp = np.cumsum(sorted_labels)\n",
    "        fp = np.cumsum(1 - sorted_labels)\n",
    "        \n",
    "        tp_rate = tp / max(1, tp[-1])\n",
    "        fp_rate = fp / max(1, fp[-1])\n",
    "        \n",
    "        auc = np.trapz(tp_rate, fp_rate)\n",
    "        return auc\n",
    "    \n",
    "    @staticmethod\n",
    "    def auc_borji(pred_map, fixation_map, n_splits=100, step_size=0.1):\n",
    "        \"\"\"\n",
    "        AUC-Borji: AUC with random non-fixated samples (shuffled).\n",
    "        \n",
    "        Args:\n",
    "            pred_map: Predicted saliency map (H, W)\n",
    "            fixation_map: Binary fixation map (H, W)\n",
    "            n_splits: Number of random splits\n",
    "            step_size: Threshold step size\n",
    "        \"\"\"\n",
    "        pred_map = SaliencyMetrics.normalize_map(pred_map)\n",
    "        fixation_map = fixation_map.astype(bool)\n",
    "        \n",
    "        fixated_values = pred_map[fixation_map]\n",
    "        non_fixated_values = pred_map[~fixation_map]\n",
    "        \n",
    "        if len(fixated_values) == 0 or len(non_fixated_values) == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        n_fixations = len(fixated_values)\n",
    "        \n",
    "        aucs = []\n",
    "        for _ in range(n_splits):\n",
    "            # Randomly sample same number of non-fixated points\n",
    "            neg_samples = np.random.choice(non_fixated_values, size=min(n_fixations, len(non_fixated_values)), replace=False)\n",
    "            \n",
    "            # Compute AUC for this split\n",
    "            all_values = np.concatenate([fixated_values, neg_samples])\n",
    "            labels = np.concatenate([np.ones(len(fixated_values)), np.zeros(len(neg_samples))])\n",
    "            \n",
    "            thresholds = np.arange(0, 1 + step_size, step_size)\n",
    "            tp_rates = []\n",
    "            fp_rates = []\n",
    "            \n",
    "            for thresh in thresholds:\n",
    "                tp = np.sum((all_values[labels == 1] >= thresh))\n",
    "                fp = np.sum((all_values[labels == 0] >= thresh))\n",
    "                fn = np.sum((all_values[labels == 1] < thresh))\n",
    "                tn = np.sum((all_values[labels == 0] < thresh))\n",
    "                \n",
    "                tpr = tp / max(1, (tp + fn))\n",
    "                fpr = fp / max(1, (fp + tn))\n",
    "                tp_rates.append(tpr)\n",
    "                fp_rates.append(fpr)\n",
    "            \n",
    "            auc = np.trapz(tp_rates[::-1], fp_rates[::-1])\n",
    "            aucs.append(auc)\n",
    "        \n",
    "        return np.mean(aucs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cc(pred_map, gt_map):\n",
    "        \"\"\"\n",
    "        CC (Correlation Coefficient): Pearson correlation with ground truth.\n",
    "        \n",
    "        Args:\n",
    "            pred_map: Predicted saliency map (H, W)\n",
    "            gt_map: Ground truth saliency map (H, W)\n",
    "        \"\"\"\n",
    "        pred_map = SaliencyMetrics.normalize_map(pred_map)\n",
    "        gt_map = SaliencyMetrics.normalize_map(gt_map)\n",
    "        \n",
    "        pred_flat = pred_map.flatten()\n",
    "        gt_flat = gt_map.flatten()\n",
    "        \n",
    "        # Remove mean\n",
    "        pred_centered = pred_flat - pred_flat.mean()\n",
    "        gt_centered = gt_flat - gt_flat.mean()\n",
    "        \n",
    "        # Pearson correlation\n",
    "        numerator = np.sum(pred_centered * gt_centered)\n",
    "        denominator = np.sqrt(np.sum(pred_centered ** 2) * np.sum(gt_centered ** 2))\n",
    "        \n",
    "        if denominator < 1e-10:\n",
    "            return 0.0\n",
    "        \n",
    "        return numerator / denominator\n",
    "    \n",
    "    @staticmethod\n",
    "    def nss(pred_map, fixation_map):\n",
    "        \"\"\"\n",
    "        NSS (Normalized Scanpath Saliency): Mean normalized saliency at fixation points.\n",
    "        \n",
    "        Args:\n",
    "            pred_map: Predicted saliency map (H, W)\n",
    "            fixation_map: Binary fixation map (H, W)\n",
    "        \"\"\"\n",
    "        pred_map = pred_map.astype(np.float32)\n",
    "        fixation_map = fixation_map.astype(bool)\n",
    "        \n",
    "        if not fixation_map.any():\n",
    "            return np.nan\n",
    "        \n",
    "        # Normalize saliency map: zero mean, unit std\n",
    "        pred_mean = pred_map.mean()\n",
    "        pred_std = pred_map.std()\n",
    "        \n",
    "        if pred_std < 1e-10:\n",
    "            return 0.0\n",
    "        \n",
    "        pred_normalized = (pred_map - pred_mean) / pred_std\n",
    "        \n",
    "        # Mean value at fixation points\n",
    "        nss_value = pred_normalized[fixation_map].mean()\n",
    "        \n",
    "        return nss_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_all_metrics(pred_map, gt_map, fixation_map=None):\n",
    "        \"\"\"\n",
    "        Compute all metrics.\n",
    "        \n",
    "        Args:\n",
    "            pred_map: Predicted saliency (H, W) numpy array\n",
    "            gt_map: Ground truth continuous saliency (H, W)\n",
    "            fixation_map: Binary fixation map (H, W), optional\n",
    "                         If None, will threshold gt_map > 0.5\n",
    "        \n",
    "        Returns:\n",
    "            dict with all metric scores\n",
    "        \"\"\"\n",
    "        pred_map = np.asarray(pred_map, dtype=np.float32)\n",
    "        gt_map = np.asarray(gt_map, dtype=np.float32)\n",
    "        \n",
    "        if fixation_map is None:\n",
    "            # Create fixation map from GT by thresholding\n",
    "            fixation_map = (gt_map > 0.5).astype(np.uint8)\n",
    "        else:\n",
    "            fixation_map = np.asarray(fixation_map, dtype=np.uint8)\n",
    "        \n",
    "        metrics = {\n",
    "            'CC': SaliencyMetrics.cc(pred_map, gt_map),\n",
    "            'NSS': SaliencyMetrics.nss(pred_map, fixation_map),\n",
    "            'AUC-Judd': SaliencyMetrics.auc_judd(pred_map, fixation_map),\n",
    "            'AUC-Borji': SaliencyMetrics.auc_borji(pred_map, fixation_map, n_splits=100)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "print('Saliency metrics module loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d5435",
   "metadata": {},
   "source": [
    "# Evaluate Metrics on Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec66ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_metrics_on_dataset(loader, num_samples=None, use_guidance=cfg.cfg_use_guidance):\n",
    "    \"\"\"\n",
    "    Evaluate saliency metrics on validation set.\n",
    "    \n",
    "    Args:\n",
    "        loader: DataLoader for the dataset\n",
    "        num_samples: Max number of samples to evaluate (None = all)\n",
    "        use_guidance: Whether to use classifier-free guidance\n",
    "    \n",
    "    Returns:\n",
    "        dict with averaged metrics\n",
    "    \"\"\"\n",
    "    ddpm.eval()\n",
    "    \n",
    "    all_metrics = {\n",
    "        'CC': [],\n",
    "        'NSS': [],\n",
    "        'AUC-Judd': [],\n",
    "        'AUC-Borji': []\n",
    "    }\n",
    "    \n",
    "    guidance_scale = cfg.cfg_guidance_scale if use_guidance else None\n",
    "    count = 0\n",
    "    \n",
    "    for imgs, sal_maps, fnames in loader:\n",
    "        if num_samples is not None and count >= num_samples:\n",
    "            break\n",
    "            \n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        \n",
    "        # Normalize GT maps\n",
    "        sal_maps_norm = sal_maps / sal_maps.amax(dim=(-2, -1), keepdim=True).clamp(min=1e-6)\n",
    "        \n",
    "        # Generate predictions\n",
    "        B = imgs.size(0)\n",
    "        preds = ddpm.sample(\n",
    "            imgs,\n",
    "            shape=sal_maps.shape,\n",
    "            guidance_scale=guidance_scale,\n",
    "            use_ddim=cfg.use_ddim, \n",
    "            ddim_steps=cfg.ddim_steps, \n",
    "            ddim_eta=cfg.ddim_eta, \n",
    "        ).clamp(0.0, 1.0)\n",
    "        \n",
    "        # Compute metrics for each image in batch\n",
    "        for i in range(B):\n",
    "            if num_samples is not None and count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            pred_np = preds[i, 0].cpu().numpy()\n",
    "            gt_np = sal_maps_norm[i, 0].cpu().numpy()\n",
    "            \n",
    "            # Create fixation map by thresholding GT\n",
    "            fixation_map = (gt_np > 0.5).astype(np.uint8)\n",
    "            \n",
    "            # Compute all metrics\n",
    "            metrics = SaliencyMetrics.compute_all_metrics(pred_np, gt_np, fixation_map)\n",
    "            \n",
    "            for key, value in metrics.items():\n",
    "                if not np.isnan(value):\n",
    "                    all_metrics[key].append(value)\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            if count % 10 == 0:\n",
    "                print(f'Processed {count} samples...')\n",
    "    \n",
    "    # Average metrics\n",
    "    avg_metrics = {}\n",
    "    for key, values in all_metrics.items():\n",
    "        if len(values) > 0:\n",
    "            avg_metrics[key] = np.mean(values)\n",
    "            avg_metrics[f'{key}_std'] = np.std(values)\n",
    "        else:\n",
    "            avg_metrics[key] = np.nan\n",
    "            avg_metrics[f'{key}_std'] = np.nan\n",
    "    \n",
    "    print('\\n' + '='*60)\n",
    "    print('SALIENCY EVALUATION METRICS')\n",
    "    print('='*60)\n",
    "    print(f'Evaluated on {count} samples')\n",
    "    print('-'*60)\n",
    "    for key in ['CC', 'NSS', 'AUC-Judd', 'AUC-Borji']:\n",
    "        mean_val = avg_metrics.get(key, np.nan)\n",
    "        std_val = avg_metrics.get(f'{key}_std', np.nan)\n",
    "        print(f'{key:12s}: {mean_val:.4f}  {std_val:.4f}')\n",
    "    print('='*60)\n",
    "    \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19875b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics = evaluate_metrics_on_dataset(val_loader, num_samples=50, use_guidance=cfg.cfg_use_guidance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4eaa29",
   "metadata": {},
   "source": [
    "# Inference on test split (random subset, visualize image + saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bf8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, image_size=cfg.image_size):\n",
    "        self.img_dir = img_dir\n",
    "        self.files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.files.sort()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size[1], image_size[0])),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        img_path = os.path.join(self.img_dir, fname)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img, fname\n",
    "\n",
    "test_dataset = TestImageDataset(cfg.test_img_dir, image_size=cfg.image_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "print('Test dataset size:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c62f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer_test_split(loader, num_samples=10, save_numpy=True, save_png=True, guidance_scale=None):\n",
    "    ddpm.eval()\n",
    "    pred_dir = os.path.join(cfg.output_dir, 'test_predictions')\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "    dataset = loader.dataset\n",
    "    if len(dataset) == 0:\n",
    "        print('Test dataset empty; nothing to infer.')\n",
    "        return\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    chosen = rng.choice(len(dataset), size=min(num_samples, len(dataset)), replace=False)\n",
    "    subset = torch.utils.data.Subset(dataset, chosen.tolist())\n",
    "    subset_loader = DataLoader(\n",
    "        subset,\n",
    "        batch_size=loader.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=loader.num_workers,\n",
    "        pin_memory=getattr(loader, 'pin_memory', False),\n",
    "    )\n",
    "\n",
    "    processed = 0\n",
    "    for imgs, fnames in subset_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        preds = ddpm.sample(\n",
    "            imgs,\n",
    "            shape=(imgs.size(0), 1, cfg.image_size[1], cfg.image_size[0]),\n",
    "            guidance_scale=guidance_scale if guidance_scale is not None else cfg.cfg_guidance_scale\n",
    "        ).clamp(0.0, 1.0)\n",
    "        for i in range(imgs.size(0)):\n",
    "            fname = fnames[i]\n",
    "            stem = os.path.splitext(fname)[0]\n",
    "            pred = preds[i, 0].cpu()\n",
    "\n",
    "            if save_numpy:\n",
    "                np.save(os.path.join(pred_dir, f'{stem}.npy'), pred.numpy())\n",
    "\n",
    "            if save_png:\n",
    "                img_rgb = imgs[i].cpu().permute(1, 2, 0).clamp(0.0, 1.0)\n",
    "\n",
    "                fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
    "                axs[0].imshow(img_rgb)\n",
    "                axs[0].set_title('Image')\n",
    "                axs[0].axis('off')\n",
    "\n",
    "                axs[1].imshow(pred, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "                axs[1].set_title('Predicted Saliency')\n",
    "                axs[1].axis('off')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(pred_dir, f'{stem}.png'), dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "            processed += 1\n",
    "\n",
    "    print(f'Saved predictions to {pred_dir} (samples={processed}).')\n",
    "\n",
    "# Run inference over 10 random test images\n",
    "if len(test_dataset) > 0:\n",
    "    infer_test_split(test_loader, num_samples=10)\n",
    "else:\n",
    "    print('Test dataset empty; nothing to infer.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
