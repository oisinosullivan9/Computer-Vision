{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "class Config:\n",
    "    train_img_dir = os.path.abspath(os.path.join('..', 'data', 'train'))\n",
    "    val_img_dir = os.path.abspath(os.path.join('..', 'data', 'val'))\n",
    "    test_img_dir = os.path.abspath(os.path.join('..', 'data', 'test'))\n",
    "\n",
    "    train_fixations_json = os.path.abspath(os.path.join('..', 'data', 'fixations_train2014.json'))\n",
    "    val_fixations_json = os.path.abspath(os.path.join('..', 'data', 'fixations_val2014.json'))\n",
    "\n",
    "    image_size = (256, 192)\n",
    "    saliency_size = (256, 192)\n",
    "    saliency_gaussian_sigma = 8.0\n",
    "\n",
    "    batch_size = 16\n",
    "    num_workers = 8\n",
    "    num_epochs = 100\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    timesteps = 250\n",
    "    beta_start = 1e-4\n",
    "    beta_end = 0.02\n",
    "\n",
    "    cfg_guidance_prob = 0.1  # probability of dropping conditioning image during training\n",
    "    cfg_guidance_scale = 1.5  # guidance strength during sampling (>1 increases saliency sharpness)\n",
    "\n",
    "    output_dir = os.path.abspath('./saliency_diffusion_outputs')\n",
    "    checkpoint_path = os.path.join(output_dir, 'saliency_diffusion_unet.pt')\n",
    "    sample_dir = os.path.join(output_dir, 'samples')\n",
    "    gt_saliency_dir = os.path.abspath('./saliency_ground_truth')\n",
    "    train_saliency_dir = os.path.join(gt_saliency_dir, 'train')\n",
    "    val_saliency_dir = os.path.join(gt_saliency_dir, 'val')\n",
    "\n",
    "    device = 'cuda' if os.environ.get('CUDA_VISIBLE_DEVICES') is not None else 'cuda' if __import__('torch').cuda.is_available() else 'cpu'\n",
    "\n",
    "cfg = Config()\n",
    "for path in [cfg.output_dir, cfg.sample_dir, cfg.gt_saliency_dir, cfg.train_saliency_dir, cfg.val_saliency_dir]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "cfg.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c03390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Mixed Precision Training (Automatic Mixed Precision)\n",
    "# Provides ~2x speedup with negligible accuracy impact\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "USE_AMP = torch.cuda.is_available()  # Enable if GPU available\n",
    "USE_MULTI_GPU = torch.cuda.device_count() > 1  # Enable if multiple GPUs\n",
    "\n",
    "print(f'Mixed Precision (AMP): {\"ENABLED\" if USE_AMP else \"DISABLED\"}')\n",
    "print(f'Multi-GPU: {\"ENABLED\" if USE_MULTI_GPU else \"DISABLED\"}')\n",
    "\n",
    "# Wrap model for multi-GPU if available\n",
    "if USE_MULTI_GPU:\n",
    "    print(f'Using {torch.cuda.device_count()} GPUs with DataParallel')\n",
    "    ddpm = nn.DataParallel(ddpm)\n",
    "    # Note: Access underlying model with ddpm.module for saving/loading\n",
    "\n",
    "# Initialize gradient scaler for mixed precision\n",
    "scaler = GradScaler() if USE_AMP else None\n",
    "\n",
    "def train_epoch_optimized(epoch_idx: int):\n",
    "    \"\"\"Optimized training with mixed precision and optional multi-GPU.\"\"\"\n",
    "    if USE_MULTI_GPU:\n",
    "        ddpm.module.train()\n",
    "    else:\n",
    "        ddpm.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for imgs, sal_maps, _ in train_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        \n",
    "        # Maybe drop conditioning for classifier-free guidance\n",
    "        if USE_MULTI_GPU:\n",
    "            imgs = ddpm.module._maybe_drop_condition(imgs) if hasattr(ddpm.module, '_maybe_drop_condition') else imgs\n",
    "        else:\n",
    "            imgs = _maybe_drop_condition(imgs)\n",
    "        \n",
    "        # Normalize saliency\n",
    "        sal_maps = sal_maps / sal_maps.amax(dim=(-2, -1), keepdim=True).clamp(min=1e-6)\n",
    "        \n",
    "        b = imgs.size(0)\n",
    "        t = torch.randint(0, cfg.timesteps, (b,), device=cfg.device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        if USE_AMP:\n",
    "            with autocast():\n",
    "                if USE_MULTI_GPU:\n",
    "                    loss = ddpm.module.p_losses(imgs, sal_maps, t)\n",
    "                else:\n",
    "                    loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "            \n",
    "            # Scaled backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # Regular training\n",
    "            if USE_MULTI_GPU:\n",
    "                loss = ddpm.module.p_losses(imgs, sal_maps, t)\n",
    "            else:\n",
    "                loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    scheduler.step()\n",
    "    return total_loss / max(1, num_batches)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch_optimized(epoch_idx: int):\n",
    "    \"\"\"Optimized validation with mixed precision.\"\"\"\n",
    "    if USE_MULTI_GPU:\n",
    "        ddpm.module.eval()\n",
    "    else:\n",
    "        ddpm.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for imgs, sal_maps, _ in val_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        sal_maps = sal_maps / sal_maps.amax(dim=(-2, -1), keepdim=True).clamp(min=1e-6)\n",
    "        \n",
    "        b = imgs.size(0)\n",
    "        t = torch.randint(0, cfg.timesteps, (b,), device=cfg.device).long()\n",
    "        \n",
    "        if USE_AMP:\n",
    "            with autocast():\n",
    "                if USE_MULTI_GPU:\n",
    "                    loss = ddpm.module.p_losses(imgs, sal_maps, t)\n",
    "                else:\n",
    "                    loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "        else:\n",
    "            if USE_MULTI_GPU:\n",
    "                loss = ddpm.module.p_losses(imgs, sal_maps, t)\n",
    "            else:\n",
    "                loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / max(1, num_batches)\n",
    "\n",
    "print('\\n✓ Optimized training functions ready')\n",
    "print('  - Mixed precision (AMP) for 2x speedup')\n",
    "print('  - Multi-GPU support if available')\n",
    "print('\\nUse train_epoch_optimized() and validate_epoch_optimized() for faster training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03627d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available resources for parallelization\n",
    "\n",
    "print('=== System Resources ===')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU count: {torch.cuda.device_count()}')\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "        print(f'    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB')\n",
    "    print(f'Current GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.3f} GB')\n",
    "    print(f'Current GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.3f} GB')\n",
    "\n",
    "print(f'\\nCPU cores: {os.cpu_count()}')\n",
    "print(f'Current num_workers: {cfg.num_workers}')\n",
    "print(f'Current batch_size: {cfg.batch_size}')\n",
    "\n",
    "# Recommendations\n",
    "print('\\n=== Recommendations ===')\n",
    "if torch.cuda.is_available():\n",
    "    gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if gpu_mem_gb >= 24:\n",
    "        print(f'✓ Large GPU ({gpu_mem_gb:.0f}GB): Try batch_size=16-32')\n",
    "    elif gpu_mem_gb >= 12:\n",
    "        print(f'✓ Medium GPU ({gpu_mem_gb:.0f}GB): Try batch_size=12-16')\n",
    "    else:\n",
    "        print(f'⚠ Small GPU ({gpu_mem_gb:.0f}GB): Keep batch_size=8 or lower')\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f'✓ Multiple GPUs detected: Consider DataParallel or DDP')\n",
    "    \n",
    "    print(f'✓ Mixed precision (AMP) recommended for 2x speedup')\n",
    "\n",
    "cpu_cores = os.cpu_count() or 4\n",
    "recommended_workers = min(cpu_cores // 2, 8)\n",
    "print(f'✓ Recommended num_workers: {recommended_workers} (you have {cfg.num_workers})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558d6e3",
   "metadata": {},
   "source": [
    "## Training Optimizations\n",
    "\n",
    "**Parallelization options:**\n",
    "1. **Multi-GPU**: Use DataParallel or DistributedDataParallel\n",
    "2. **Mixed Precision (AMP)**: 2x faster training with minimal accuracy loss\n",
    "3. **Larger batch size**: More parallel processing (if GPU memory allows)\n",
    "4. **More workers**: Increase `num_workers` if CPU/RAM allows\n",
    "5. **Gradient accumulation**: Simulate larger batches without more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import lru_cache\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "print('Using device:', cfg.device)\n",
    "print('Train dir exists:', os.path.isdir(cfg.train_img_dir))\n",
    "print('Val dir exists:', os.path.isdir(cfg.val_img_dir))\n",
    "print('Test dir exists:', os.path.isdir(cfg.test_img_dir))\n",
    "print('Train fixations JSON exists:', os.path.isfile(cfg.train_fixations_json))\n",
    "print('Val fixations JSON exists:', os.path.isfile(cfg.val_fixations_json))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbbc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: str) -> Dict:\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "@lru_cache(maxsize=32)\n",
    "def _gaussian_kernel1d(radius: int, sigma: float) -> np.ndarray:\n",
    "    ax = np.arange(-radius, radius + 1, dtype=np.float32)\n",
    "    kernel = np.exp(-(ax ** 2) / (2.0 * sigma ** 2))\n",
    "    kernel_sum = kernel.sum()\n",
    "    if kernel_sum > 0:\n",
    "        kernel /= kernel_sum\n",
    "    return kernel.astype(np.float32)\n",
    "\n",
    "def apply_gaussian_blur(array: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    if sigma <= 0:\n",
    "        return array\n",
    "    radius = max(1, int(round(3.0 * sigma)))\n",
    "    kernel = _gaussian_kernel1d(radius, float(sigma))\n",
    "    array = array.astype(np.float32, copy=False)\n",
    "    blurred = np.apply_along_axis(lambda m: np.convolve(m, kernel, mode='same'), axis=0, arr=array)\n",
    "    blurred = np.apply_along_axis(lambda m: np.convolve(m, kernel, mode='same'), axis=1, arr=blurred)\n",
    "    return blurred.astype(np.float32)\n",
    "    \n",
    "def parse_fixations_json(fixations: Dict, split: str, cache_dir: Optional[str] = None) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Parse SALICON-style fixation annotations into dense saliency maps.\"\"\"\n",
    "    if not fixations:\n",
    "        return {}\n",
    "\n",
    "    if cache_dir:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    # Map image id to file metadata\n",
    "    img_meta: Dict[int, str] = {}\n",
    "    dims: Dict[str, Tuple[int, int]] = {}\n",
    "    for img in fixations.get('images', []):\n",
    "        img_id = img.get('id')\n",
    "        fname = img.get('file_name')\n",
    "        if img_id is None or not fname:\n",
    "            continue\n",
    "        img_meta[img_id] = fname\n",
    "        width = img.get('width')\n",
    "        height = img.get('height')\n",
    "        if width and height:\n",
    "            dims[fname] = (int(width), int(height))\n",
    "\n",
    "    # Aggregate fixation points per image filename\n",
    "    mapping: Dict[str, List[Tuple[float, float]]] = {}\n",
    "    for ann in fixations.get('annotations', []):\n",
    "        img_id = ann.get('image_id')\n",
    "        fname = img_meta.get(img_id)\n",
    "        if not fname:\n",
    "            continue\n",
    "        pts = ann.get('fixations') or ann.get('points') or []\n",
    "        if not pts:\n",
    "            continue\n",
    "        bucket = mapping.setdefault(fname, [])\n",
    "        for p in pts:\n",
    "            if not isinstance(p, (list, tuple)) or len(p) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                row = float(p[0])\n",
    "                col = float(p[1])\n",
    "            except (TypeError, ValueError):\n",
    "                continue\n",
    "            if np.isnan(row) or np.isnan(col):\n",
    "                continue\n",
    "            # SALICON fixations are 1-indexed (row, col) == (y, x)\n",
    "            y = row - 1.0\n",
    "            x = col - 1.0\n",
    "            bucket.append((y, x))\n",
    "\n",
    "    saliency_maps: Dict[str, np.ndarray] = {}\n",
    "    W, H = cfg.saliency_size\n",
    "\n",
    "    processed = 0\n",
    "    filenames = sorted(set(img_meta.values()))\n",
    "    for fname in filenames:\n",
    "        sal_map = np.zeros((H, W), dtype=np.float32)\n",
    "        pts = mapping.get(fname, [])\n",
    "        orig_w, orig_h = dims.get(fname, (None, None))\n",
    "\n",
    "        if pts:\n",
    "            for y, x in pts:\n",
    "                if orig_w and orig_h and orig_w > 1 and orig_h > 1:\n",
    "                    sy = (y / max(1.0, orig_h - 1)) * (H - 1)\n",
    "                    sx = (x / max(1.0, orig_w - 1)) * (W - 1)\n",
    "                else:\n",
    "                    sy, sx = y, x\n",
    "\n",
    "                iy = int(round(np.clip(sy, 0, H - 1)))\n",
    "                ix = int(round(np.clip(sx, 0, W - 1)))\n",
    "                sal_map[iy, ix] += 1.0\n",
    "\n",
    "            if cfg.saliency_gaussian_sigma and cfg.saliency_gaussian_sigma > 0:\n",
    "                sal_map = apply_gaussian_blur(sal_map, cfg.saliency_gaussian_sigma)\n",
    "\n",
    "            if sal_map.max() > 0:\n",
    "                sal_map /= sal_map.max()\n",
    "\n",
    "        saliency_maps[fname] = sal_map.astype(np.float32)\n",
    "        processed += 1\n",
    "\n",
    "        if cache_dir:\n",
    "            stem = os.path.splitext(fname)[0]\n",
    "            cache_path = os.path.join(cache_dir, f'{stem}.npy')\n",
    "            np.save(cache_path, sal_map)\n",
    "\n",
    "    msg_prefix = f\"{split.capitalize()} saliency maps\"\n",
    "    if cache_dir:\n",
    "        print(f\"{msg_prefix}: saved {processed} arrays to {cache_dir}\")\n",
    "    else:\n",
    "        print(f\"{msg_prefix}: generated {processed} arrays in memory\")\n",
    "\n",
    "    return saliency_maps\n",
    "    \n",
    "try:\n",
    "    train_fix_raw = load_json(cfg.train_fixations_json)\n",
    "    val_fix_raw = load_json(cfg.val_fixations_json)\n",
    "    print('Loaded fixation JSONs.')\n",
    "except Exception as e:\n",
    "    print('Error loading or parsing fixation JSONs. Please adapt parse_fixations_json to your format.')\n",
    "    print(e)\n",
    "    train_fix_raw, val_fix_raw = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12431126",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_saliency_maps = parse_fixations_json(train_fix_raw, split='train', cache_dir=cfg.train_saliency_dir) if train_fix_raw is not None else {}\n",
    "val_saliency_maps = parse_fixations_json(val_fix_raw, split='val', cache_dir=cfg.val_saliency_dir) if val_fix_raw is not None else {}\n",
    "\n",
    "print('Train saliency entries in memory:', len(train_saliency_maps))\n",
    "print('Val saliency entries in memory:', len(val_saliency_maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliconSaliencyDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, saliency_maps: Dict[str, np.ndarray], image_size=(256, 192), saliency_cache_dir: Optional[str] = None):\n",
    "        self.img_dir = img_dir\n",
    "        self.saliency_maps = saliency_maps or {}\n",
    "        self.saliency_cache_dir = saliency_cache_dir\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Collect only images that have saliency info (either in-memory or cached on disk)\n",
    "        self.image_files = [f for f in os.listdir(img_dir) if self._has_saliency(f)]\n",
    "        self.image_files.sort()\n",
    "\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size[1], image_size[0])),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        # Saliency maps are kept in [0,1] range, single-channel\n",
    "\n",
    "    def _has_saliency(self, fname: str) -> bool:\n",
    "        if fname in self.saliency_maps:\n",
    "            return True\n",
    "        if not self.saliency_cache_dir:\n",
    "            return False\n",
    "        cache_path = os.path.join(self.saliency_cache_dir, f\"{os.path.splitext(fname)[0]}.npy\")\n",
    "        return os.path.isfile(cache_path)\n",
    "\n",
    "    def _load_saliency(self, fname: str) -> np.ndarray:\n",
    "        sal_map = self.saliency_maps.get(fname)\n",
    "        if sal_map is None and self.saliency_cache_dir:\n",
    "            cache_path = os.path.join(self.saliency_cache_dir, f\"{os.path.splitext(fname)[0]}.npy\")\n",
    "            if os.path.isfile(cache_path):\n",
    "                sal_map = np.load(cache_path, allow_pickle=False)\n",
    "        if sal_map is None:\n",
    "            raise KeyError(f'No saliency map found for {fname}.')\n",
    "        return sal_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, fname)\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.img_transform(img)\n",
    "\n",
    "        sal_map = self._load_saliency(fname)\n",
    "        H, W = self.image_size[1], self.image_size[0]\n",
    "        if sal_map.shape != (H, W):\n",
    "            sal_img = Image.fromarray((sal_map * 255).astype(np.uint8))\n",
    "            sal_img = sal_img.resize((W, H), resample=Image.BILINEAR)\n",
    "            sal_tensor = torch.from_numpy(np.array(sal_img)).float() / 255.0\n",
    "        else:\n",
    "            sal_tensor = torch.from_numpy(sal_map).float()\n",
    "        sal_tensor = sal_tensor.unsqueeze(0)  # (1, H, W)\n",
    "\n",
    "        return img, sal_tensor, fname\n",
    "    \n",
    "train_dataset = SaliconSaliencyDataset(\n",
    "    cfg.train_img_dir,\n",
    "    train_saliency_maps,\n",
    "    image_size=cfg.image_size,\n",
    "    saliency_cache_dir=cfg.train_saliency_dir\n",
    ")\n",
    "val_dataset = SaliconSaliencyDataset(\n",
    "    cfg.val_img_dir,\n",
    "    val_saliency_maps,\n",
    "    image_size=cfg.image_size,\n",
    "    saliency_cache_dir=cfg.val_saliency_dir\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "print('Train dataset size:', len(train_dataset))\n",
    "print('Val dataset size:', len(val_dataset))\n",
    "\n",
    "batch = next(iter(train_loader)) if len(train_dataset) > 0 else None\n",
    "if batch is not None:\n",
    "    imgs, sal_maps, fnames = batch\n",
    "    print('Batch shapes:', imgs.shape, sal_maps.shape)\n",
    "    grid = vutils.make_grid(imgs, nrow=min(4, imgs.size(0)))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.title('Sample input images')\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    grid_sal = vutils.make_grid(sal_maps, nrow=min(4, sal_maps.size(0)))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.title('Sample ground truth saliency maps')\n",
    "    plt.imshow(grid_sal[0].cpu(), cmap='gray', vmin=0.0, vmax=1.0) #cmap = hot\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Warning: Train dataset is empty. Check your paths and JSON mapping.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc957419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def visualize_cached_saliency(split: str = 'train', num_samples: int = 3):\n",
    "    cache_dir = cfg.train_saliency_dir if split == 'train' else cfg.val_saliency_dir\n",
    "    img_dir = cfg.train_img_dir if split == 'train' else cfg.val_img_dir\n",
    "    if not os.path.isdir(cache_dir):\n",
    "        print(f'Cache directory not found: {cache_dir}')\n",
    "        return\n",
    "\n",
    "    saliency_paths = sorted(glob.glob(os.path.join(cache_dir, '*.npy')))\n",
    "    if not saliency_paths:\n",
    "        print(f'No cached saliency maps found in {cache_dir}')\n",
    "        return\n",
    "\n",
    "    for npy_path in saliency_paths[:max(0, num_samples)]:\n",
    "        sal_map = np.load(npy_path)\n",
    "        base = os.path.splitext(os.path.basename(npy_path))[0]\n",
    "        candidates = sorted(glob.glob(os.path.join(img_dir, base + '.*')))\n",
    "        img = None\n",
    "        if candidates:\n",
    "            try:\n",
    "                img = Image.open(candidates[0]).convert('RGB')\n",
    "                img = img.resize(cfg.image_size)\n",
    "            except Exception as exc:\n",
    "                print(f'Failed to load image for {base}: {exc}')\n",
    "                img = None\n",
    "\n",
    "        if img is not None:\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
    "            axs[0].imshow(img)\n",
    "            axs[0].set_title('Image')\n",
    "            axs[0].axis('off')\n",
    "            axs[1].imshow(sal_map, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            axs[1].set_title(f'Saliency {base}')\n",
    "            axs[1].axis('off')\n",
    "        else:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "            ax.imshow(sal_map, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            ax.set_title(f'Saliency {base}')\n",
    "            ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualize_cached_saliency('train', num_samples=3)\n",
    "visualize_cached_saliency('val', num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _groupnorm_groups(num_channels: int) -> int:\n",
    "    for group in [32, 16, 8, 4, 2]:\n",
    "        if num_channels % group == 0:\n",
    "            return group\n",
    "    return 1\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, time_emb_dim: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(_groupnorm_groups(in_ch), in_ch, eps=1e-6)\n",
    "        self.norm2 = nn.GroupNorm(_groupnorm_groups(out_ch), out_ch, eps=1e-6)\n",
    "        self.act = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.residual = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "        if time_emb_dim is not None:\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(time_emb_dim, out_ch)\n",
    "            )\n",
    "        else:\n",
    "            self.time_mlp = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_emb: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        h = self.norm1(x)\n",
    "        h = self.act(h)\n",
    "        h = self.conv1(h)\n",
    "        if self.time_mlp is not None and t_emb is not None:\n",
    "            h = h + self.time_mlp(t_emb)[:, :, None, None]\n",
    "        h = self.norm2(h)\n",
    "        h = self.act(h)\n",
    "        h = self.conv2(h)\n",
    "        return h + self.residual(x)\n",
    "\n",
    "\n",
    "class SpatialSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention over spatial dimensions with pre-norm and residual.\"\"\"\n",
    "    def __init__(self, channels: int, num_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = channels // num_heads\n",
    "        assert channels % num_heads == 0, \"channels must be divisible by num_heads\"\n",
    "        \n",
    "        self.norm = nn.GroupNorm(_groupnorm_groups(channels), channels, eps=1e-6)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n",
    "        self.proj = nn.Conv2d(channels, channels, 1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        residual = x\n",
    "        \n",
    "        # Pre-norm\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        qkv = self.qkv(x)  # (B, 3*C, H, W)\n",
    "        qkv = qkv.view(B, 3, self.num_heads, self.head_dim, H * W)\n",
    "        qkv = qkv.permute(1, 0, 2, 4, 3)  # (3, B, heads, HW, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each (B, heads, HW, head_dim)\n",
    "        \n",
    "        # Attention\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # (B, heads, HW, HW)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attn, v)  # (B, heads, HW, head_dim)\n",
    "        out = out.permute(0, 1, 3, 2).reshape(B, C, H, W)\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out + residual\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention: query from one feature map, key/value from another, with pre-norm and residual.\"\"\"\n",
    "    def __init__(self, channels: int, context_channels: int, num_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = channels // num_heads\n",
    "        assert channels % num_heads == 0, \"channels must be divisible by num_heads\"\n",
    "        \n",
    "        self.norm_q = nn.GroupNorm(_groupnorm_groups(channels), channels, eps=1e-6)\n",
    "        self.norm_kv = nn.GroupNorm(_groupnorm_groups(context_channels), context_channels, eps=1e-6)\n",
    "        \n",
    "        self.q_proj = nn.Conv2d(channels, channels, 1)\n",
    "        self.k_proj = nn.Conv2d(context_channels, channels, 1)\n",
    "        self.v_proj = nn.Conv2d(context_channels, channels, 1)\n",
    "        self.out_proj = nn.Conv2d(channels, channels, 1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x: torch.Tensor, context: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        _, Cc, Hc, Wc = context.shape\n",
    "        residual = x\n",
    "        \n",
    "        # Pre-norm\n",
    "        x = self.norm_q(x)\n",
    "        context = self.norm_kv(context)\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.q_proj(x).view(B, self.num_heads, self.head_dim, H * W).permute(0, 1, 3, 2)\n",
    "        k = self.k_proj(context).view(B, self.num_heads, self.head_dim, Hc * Wc).permute(0, 1, 3, 2)\n",
    "        v = self.v_proj(context).view(B, self.num_heads, self.head_dim, Hc * Wc).permute(0, 1, 3, 2)\n",
    "        \n",
    "        # Attention\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.permute(0, 1, 3, 2).reshape(B, C, H, W)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        return out + residual\n",
    "\n",
    "\n",
    "# Sinusoidal positional time embeddings\n",
    "def sinusoidal_time_embedding(t: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    # t: (B,) in [0, timesteps)\n",
    "    device = t.device\n",
    "    half_dim = dim // 2\n",
    "    # log space frequencies\n",
    "    emb_factor = math.log(10000) / max(1, half_dim - 1)\n",
    "    # shape (half_dim,)\n",
    "    freqs = torch.exp(torch.arange(0, half_dim, device=device) * (-emb_factor))\n",
    "    # normalize t to [0,1]\n",
    "    t_norm = t.float() / max(1, cfg.timesteps - 1)\n",
    "    # outer product (B, half_dim)\n",
    "    angles = t_norm[:, None] * freqs[None, :]\n",
    "    emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "    if dim % 2 == 1:\n",
    "        # pad one channel if odd dim\n",
    "        emb = torch.nn.functional.pad(emb, (0, 1), mode='constant', value=0.0)\n",
    "    return emb\n",
    "\n",
    "class UNetSaliency(nn.Module):\n",
    "    def __init__(self, img_channels=3, saliency_channels=1, base_ch=64, time_emb_dim=128, pos_dim=64, attn_heads=4):\n",
    "        super().__init__()\n",
    "        in_ch = img_channels + saliency_channels\n",
    "\n",
    "        # Time embedding: sinusoidal -> linear projection -> MLP\n",
    "        self.pos_dim = pos_dim\n",
    "        self.time_proj = nn.Linear(pos_dim, time_emb_dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Encoder (deeper: 3 downsamples + bottleneck)\n",
    "        self.enc1 = ResidualBlock(in_ch, base_ch, time_emb_dim)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ResidualBlock(base_ch, base_ch * 2, time_emb_dim)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ResidualBlock(base_ch * 2, base_ch * 4, time_emb_dim)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck with attention\n",
    "        self.bottleneck = ResidualBlock(base_ch * 4, base_ch * 8, time_emb_dim)\n",
    "        self.bottleneck_self_attn = SpatialSelfAttention(base_ch * 8, num_heads=attn_heads)\n",
    "        \n",
    "        # Image encoder for cross-attention context (downsampled to bottleneck resolution)\n",
    "        self.img_encoder = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, base_ch * 2, 3, stride=2, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(base_ch * 2, base_ch * 4, 3, stride=2, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(base_ch * 4, base_ch * 8, 3, stride=2, padding=1),\n",
    "        )\n",
    "        self.bottleneck_cross_attn = CrossAttention(base_ch * 8, base_ch * 8, num_heads=attn_heads)\n",
    "\n",
    "        # Decoder (mirror)\n",
    "        self.up3 = nn.ConvTranspose2d(base_ch * 8, base_ch * 4, 2, stride=2)\n",
    "        self.dec3 = ResidualBlock(base_ch * 8, base_ch * 4, time_emb_dim)\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch * 4, base_ch * 2, 2, stride=2)\n",
    "        self.dec2 = ResidualBlock(base_ch * 4, base_ch * 2, time_emb_dim)\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch * 2, base_ch, 2, stride=2)\n",
    "        self.dec1 = ResidualBlock(base_ch * 2, base_ch, time_emb_dim)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(base_ch, saliency_channels, 1)\n",
    "\n",
    "    def forward(self, x_img, x_sal_noisy, t):\n",
    "        # x_img: (B, 3, H, W), x_sal_noisy: (B, 1, H, W), t: (B,) timestep index\n",
    "        # Build sinusoidal positional embedding\n",
    "        pos = sinusoidal_time_embedding(t, self.pos_dim)\n",
    "        t_emb = self.time_proj(pos)\n",
    "        t_emb = self.time_mlp(t_emb)\n",
    "\n",
    "        x = torch.cat([x_img, x_sal_noisy], dim=1)\n",
    "        \n",
    "        e1 = self.enc1(x, t_emb)\n",
    "        p1 = self.pool1(e1)\n",
    "        e2 = self.enc2(p1, t_emb)\n",
    "        p2 = self.pool2(e2)\n",
    "        e3 = self.enc3(p2, t_emb)\n",
    "        p3 = self.pool3(e3)\n",
    "\n",
    "        # Bottleneck with self-attention and cross-attention\n",
    "        b = self.bottleneck(p3, t_emb)\n",
    "        b = self.bottleneck_self_attn(b)\n",
    "        \n",
    "        # Cross-attend to image features\n",
    "        img_ctx = self.img_encoder(x_img)\n",
    "        b = self.bottleneck_cross_attn(b, img_ctx)\n",
    "\n",
    "        u3 = self.up3(b)\n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1), t_emb)\n",
    "        u2 = self.up2(d3)\n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1), t_emb)\n",
    "        u1 = self.up1(d2)\n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1), t_emb)\n",
    "\n",
    "        out = self.out_conv(d1)\n",
    "        return out\n",
    "\n",
    "model = UNetSaliency().to(cfg.device)\n",
    "print('Model params:', sum(p.numel() for p in model.parameters()) / 1e6, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce253e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyDDPM(nn.Module):\n",
    "    def __init__(self, model: nn.Module, timesteps: int = 1000, beta_start: float = 1e-4, beta_end: float = 0.02):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "        betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = torch.cat([torch.ones(1), alphas_cumprod[:-1]])\n",
    "        \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt((1.0 - alphas_cumprod) / alphas_cumprod))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        sqrt_recip_alphas_t = self.sqrt_recip_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_recipm1_alphas_t = self.sqrt_recipm1_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_recip_alphas_t * x_t - sqrt_recipm1_alphas_t * noise\n",
    "\n",
    "    def _predict_noise(self, x_img, x, t, guidance_scale: Optional[float] = None):\n",
    "        if guidance_scale is None or guidance_scale <= 1.0:\n",
    "            return self.model(x_img, x, t)\n",
    "        zeros = torch.zeros_like(x_img)\n",
    "        model_in = torch.cat([zeros, x_img], dim=0)\n",
    "        x_rep = torch.cat([x, x], dim=0)\n",
    "        t_rep = torch.cat([t, t], dim=0)\n",
    "        noise_pred = self.model(model_in, x_rep, t_rep)\n",
    "        noise_uncond, noise_cond = noise_pred.chunk(2, dim=0)\n",
    "        return noise_uncond + guidance_scale * (noise_cond - noise_uncond)\n",
    "\n",
    "    def p_losses(self, x_img, x_sal, t):\n",
    "        noise = torch.randn_like(x_sal)\n",
    "        x_noisy = self.q_sample(x_sal, t, noise)\n",
    "        noise_pred = self._predict_noise(x_img, x_noisy, t)\n",
    "        # Standard noise MSE\n",
    "        loss_noise = nn.functional.mse_loss(noise_pred, noise)\n",
    "        # Auxiliary reconstruction loss on x0 to better match GT\n",
    "        x0_pred = self.predict_start_from_noise(x_noisy, t, noise_pred)\n",
    "        x0_pred = x0_pred.clamp(0.0, 1.0)\n",
    "        # Mix L1 and MSE for sharper alignment\n",
    "        loss_l1 = nn.functional.l1_loss(x0_pred, x_sal)\n",
    "        loss_mse = nn.functional.mse_loss(x0_pred, x_sal)\n",
    "        # Lightweight SSIM\n",
    "        def ssim_simple(a, b):\n",
    "            mu_a = a.mean(dim=(-2, -1), keepdim=True)\n",
    "            mu_b = b.mean(dim=(-2, -1), keepdim=True)\n",
    "            var_a = a.var(dim=(-2, -1), keepdim=True)\n",
    "            var_b = b.var(dim=(-2, -1), keepdim=True)\n",
    "            cov_ab = ((a - mu_a) * (b - mu_b)).mean(dim=(-2, -1), keepdim=True)\n",
    "            C1, C2 = 0.01**2, 0.03**2\n",
    "            ssim_map = ((2*mu_a*mu_b + C1) * (2*cov_ab + C2)) / ((mu_a**2 + mu_b**2 + C1) * (var_a + var_b + C2))\n",
    "            return 1.0 - ssim_map.mean()\n",
    "        loss_ssim = ssim_simple(x0_pred, x_sal)\n",
    "        # Weights\n",
    "        w_noise, w_l1, w_mse, w_ssim = 1.0, 0.5, 0.5, 0.5\n",
    "        return w_noise*loss_noise + w_l1*loss_l1 + w_mse*loss_mse + w_ssim*loss_ssim\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x_img, t, x, guidance_scale: Optional[float] = None):\n",
    "        betas_t = self.betas[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_recip_alphas_t = torch.sqrt(1.0 / self.alphas[t]).view(-1, 1, 1, 1)\n",
    "        \n",
    "        noise_pred = self._predict_noise(x_img, x, t, guidance_scale)\n",
    "        model_mean = sqrt_recip_alphas_t * (x - betas_t / sqrt_one_minus_alphas_cumprod_t * noise_pred)\n",
    "        if t[0] == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma_t = torch.sqrt(betas_t)\n",
    "            return model_mean + sigma_t * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, x_img, shape, guidance_scale: Optional[float] = None):\n",
    "        # x_img: condition, shape: (B, 1, H, W) saliency shape\n",
    "        x = torch.randn(shape, device=x_img.device)\n",
    "        B = x.shape[0]\n",
    "        for i in reversed(range(self.timesteps)):\n",
    "            t = torch.full((B,), i, device=x_img.device, dtype=torch.long)\n",
    "            x = self.p_sample(x_img, t, x, guidance_scale)\n",
    "        return x\n",
    "\n",
    "ddpm = SaliencyDDPM(model, timesteps=cfg.timesteps, beta_start=cfg.beta_start, beta_end=cfg.beta_end).to(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(ddpm.parameters(), lr=5e-5, weight_decay=cfg.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, cfg.num_epochs))\n",
    "\n",
    "# Directory for per-epoch checkpoints\n",
    "per_epoch_dir = os.path.join(cfg.output_dir, 'epochs')\n",
    "os.makedirs(per_epoch_dir, exist_ok=True)\n",
    "\n",
    "def _maybe_drop_condition(imgs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Randomly zero-out conditioning images for classifier-free guidance training.\"\"\"\n",
    "    if cfg.cfg_guidance_prob <= 0:\n",
    "        return imgs\n",
    "    b = imgs.size(0)\n",
    "    drop_mask = (torch.rand(b, device=imgs.device) < cfg.cfg_guidance_prob)\n",
    "    if drop_mask.any():\n",
    "        imgs = imgs.clone()\n",
    "        imgs[drop_mask] = 0.0\n",
    "    return imgs\n",
    "\n",
    "def train_epoch(epoch_idx: int):\n",
    "    ddpm.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for imgs, sal_maps, _ in train_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        imgs = _maybe_drop_condition(imgs)\n",
    "        # Normalize saliency per image for consistent targets\n",
    "        sal_maps = sal_maps / sal_maps.amax(dim=(-2, -1), keepdim=True).clamp(min=1e-6)\n",
    "        b = imgs.size(0)\n",
    "        t = torch.randint(0, cfg.timesteps, (b,), device=cfg.device).long()\n",
    "        loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    scheduler.step()\n",
    "    return total_loss / max(1, num_batches)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(epoch_idx: int):\n",
    "    ddpm.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for imgs, sal_maps, _ in val_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        imgs = _maybe_drop_condition(imgs)\n",
    "        sal_maps = sal_maps / sal_maps.amax(dim=(-2, -1), keepdim=True).clamp(min=1e-6)\n",
    "        b = imgs.size(0)\n",
    "        t = torch.randint(0, cfg.timesteps, (b,), device=cfg.device).long()\n",
    "        loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    return total_loss / max(1, num_batches)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    train_loss = train_epoch_optimized(epoch)\n",
    "    val_loss = validate_epoch_optimized(epoch)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{cfg.num_epochs} | LR: {current_lr:.2e} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "    # Build checkpoint payload\n",
    "    ckpt = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state': ddpm.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'scheduler_state': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "    # Save per-epoch\n",
    "    epoch_filename = os.path.join(per_epoch_dir, f'epoch_{epoch+1:03d}.pt')\n",
    "    torch.save(ckpt, epoch_filename)\n",
    "    # Save last\n",
    "    torch.save(ckpt, os.path.join(cfg.output_dir, 'last.pt'))\n",
    "    print(f'  Saved epoch checkpoint to {epoch_filename}')\n",
    "\n",
    "    # Update and save best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        ckpt['best_val_loss'] = best_val_loss\n",
    "        torch.save(ckpt, cfg.checkpoint_path)\n",
    "        print('  Updated best model ->', cfg.checkpoint_path)\n",
    "\n",
    "print('Training complete. Best val loss:', best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Resume optimizer/scheduler from checkpoint\n",
    "ckpt_path = cfg.checkpoint_path\n",
    "if os.path.isfile(ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=cfg.device)\n",
    "    if isinstance(ckpt, dict):\n",
    "        if 'optimizer_state' in ckpt:\n",
    "            try:\n",
    "                optimizer.load_state_dict(ckpt['optimizer_state'])\n",
    "                print('Restored optimizer state from checkpoint.')\n",
    "            except Exception as e:\n",
    "                print('Failed to restore optimizer state:', e)\n",
    "        if 'scheduler_state' in ckpt:\n",
    "            try:\n",
    "                scheduler.load_state_dict(ckpt['scheduler_state'])\n",
    "                print('Restored scheduler state from checkpoint.')\n",
    "            except Exception as e:\n",
    "                print('Failed to restore scheduler state:', e)\n",
    "        best_val_loss = ckpt.get('best_val_loss', best_val_loss)\n",
    "        resume_epoch = ckpt.get('epoch', 0)\n",
    "        print(f'Resume info -> epoch: {resume_epoch}, best_val_loss: {best_val_loss:.4f}')\n",
    "    else:\n",
    "        print('Checkpoint does not contain optimizer/scheduler state (raw state_dict).')\n",
    "else:\n",
    "    print('No checkpoint found to resume training state.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f86db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = cfg.checkpoint_path\n",
    "if os.path.isfile(ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=cfg.device)\n",
    "    # Handle both raw state_dict and packaged checkpoint dict\n",
    "    state_dict = ckpt.get('model_state', ckpt)\n",
    "    ddpm.load_state_dict(state_dict)\n",
    "    print('Loaded best checkpoint from', ckpt_path)\n",
    "else:\n",
    "    print('Checkpoint not found; using current model weights.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d6f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_samples_from_loader(loader, num_batches: int = 1, tag: str = 'val'):\n",
    "    ddpm.eval()\n",
    "    batch_count = 0\n",
    "    for imgs, sal_maps, fnames in loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        sal_maps = sal_maps / sal_maps.amax(dim=(-2, -1), keepdim=True).clamp(min=1e-6)\n",
    "        B = imgs.size(0)\n",
    "\n",
    "        samples = ddpm.sample(\n",
    "            imgs,\n",
    "            shape=sal_maps.shape,\n",
    "            guidance_scale=cfg.cfg_guidance_scale if cfg.cfg_guidance_scale and cfg.cfg_guidance_scale > 0 else None\n",
    "        ).clamp(0.0, 1.0)\n",
    "        \n",
    "        # Correlation metrics\n",
    "        def pearson(a, b):\n",
    "            a = a.view(a.size(0), -1)\n",
    "            b = b.view(b.size(0), -1)\n",
    "            a = a - a.mean(dim=1, keepdim=True)\n",
    "            b = b - b.mean(dim=1, keepdim=True)\n",
    "            num = (a*b).sum(dim=1)\n",
    "            den = torch.sqrt((a*a).sum(dim=1) * (b*b).sum(dim=1)).clamp(min=1e-6)\n",
    "            return (num/den).mean().item()\n",
    "        p_corr = pearson(samples, sal_maps)\n",
    "        print(f'Batch Pearson corr: {p_corr:.3f}')\n",
    "        \n",
    "        for i in range(B):\n",
    "            img = imgs[i].cpu()\n",
    "            gt = sal_maps[i, 0].cpu()\n",
    "            pred = samples[i, 0].cpu()\n",
    "\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "            axs[0].imshow(img.permute(1, 2, 0))\n",
    "            axs[0].set_title('Image')\n",
    "            axs[0].axis('off')\n",
    "\n",
    "            axs[1].imshow(gt, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            axs[1].set_title('GT saliency')\n",
    "            axs[1].axis('off')\n",
    "\n",
    "            axs[2].imshow(pred, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            axs[2].set_title('Pred saliency')\n",
    "            axs[2].axis('off')\n",
    "            plt.tight_layout()\n",
    "            save_path = os.path.join(cfg.sample_dir, f'{tag}_{fnames[i]}')\n",
    "            fig.savefig(save_path, dpi=150)\n",
    "            plt.close(fig)\n",
    "\n",
    "        batch_count += 1\n",
    "        if batch_count >= num_batches:\n",
    "            break\n",
    "\n",
    "    print(f'Saved sample visualizations to {cfg.sample_dir} (tag={tag}).')\n",
    "# Generate a few validation samples\n",
    "if len(val_dataset) > 0:\n",
    "    generate_samples_from_loader(val_loader, num_batches=1, tag='val')\n",
    "else:\n",
    "    print('No validation data; skipping sample generation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7202544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "class SaliencyMetrics:\n",
    "    \"\"\"Standard saliency evaluation metrics.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_map(s_map):\n",
    "        \"\"\"Normalize to [0, 1] range.\"\"\"\n",
    "        s_map = s_map.astype(np.float32)\n",
    "        if s_map.max() > s_map.min():\n",
    "            s_map = (s_map - s_map.min()) / (s_map.max() - s_map.min())\n",
    "        return s_map\n",
    "    \n",
    "    @staticmethod\n",
    "    def auc_judd(pred_map, fixation_map):\n",
    "        \"\"\"\n",
    "        AUC-Judd: Area under ROC curve treating fixated pixels as positives.\n",
    "        \n",
    "        Args:\n",
    "            pred_map: Predicted saliency map (H, W) in [0, 1]\n",
    "            fixation_map: Binary fixation map (H, W) where 1=fixation, 0=background\n",
    "        \"\"\"\n",
    "        pred_map = SaliencyMetrics.normalize_map(pred_map)\n",
    "        fixation_map = fixation_map.astype(bool)\n",
    "        \n",
    "        # Get saliency values at fixated and non-fixated locations\n",
    "        fixated_values = pred_map[fixation_map]\n",
    "        non_fixated_values = pred_map[~fixation_map]\n",
    "        \n",
    "        if len(fixated_values) == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # Compute AUC using all background pixels\n",
    "        all_values = np.concatenate([fixated_values, non_fixated_values])\n",
    "        labels = np.concatenate([np.ones(len(fixated_values)), np.zeros(len(non_fixated_values))])\n",
    "        \n",
    "        # Sort by saliency values\n",
    "        sorted_indices = np.argsort(all_values)[::-1]\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        \n",
    "        # Compute AUC via trapezoidal rule\n",
    "        tp = np.cumsum(sorted_labels)\n",
    "        fp = np.cumsum(1 - sorted_labels)\n",
    "        \n",
    "        tp_rate = tp / max(1, tp[-1])\n",
    "        fp_rate = fp / max(1, fp[-1])\n",
    "        \n",
    "        auc = np.trapz(tp_rate, fp_rate)\n",
    "        return auc\n",
    "    \n",
    "    @staticmethod\n",
    "    def auc_borji(pred_map, fixation_map, n_splits=100, step_size=0.1):\n",
    "        \"\"\"\n",
    "        AUC-Borji: AUC with random non-fixated samples (shuffled).\n",
    "        \n",
    "        Args:\n",
    "            pred_map: Predicted saliency map (H, W)\n",
    "            fixation_map: Binary fixation map (H, W)\n",
    "            n_splits: Number of random splits\n",
    "            step_size: Threshold step size\n",
    "        \"\"\"\n",
    "        pred_map = SaliencyMetrics.normalize_map(pred_map)\n",
    "        fixation_map = fixation_map.astype(bool)\n",
    "        \n",
    "        fixated_values = pred_map[fixation_map]\n",
    "        non_fixated_values = pred_map[~fixation_map]\n",
    "        \n",
    "        if len(fixated_values) == 0 or len(non_fixated_values) == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        n_fixations = len(fixated_values)\n",
    "        \n",
    "        aucs = []\n",
    "        for _ in range(n_splits):\n",
    "            # Randomly sample same number of non-fixated points\n",
    "            neg_samples = np.random.choice(non_fixated_values, size=min(n_fixations, len(non_fixated_values)), replace=False)\n",
    "            \n",
    "            # Compute AUC for this split\n",
    "            all_values = np.concatenate([fixated_values, neg_samples])\n",
    "            labels = np.concatenate([np.ones(len(fixated_values)), np.zeros(len(neg_samples))])\n",
    "            \n",
    "            thresholds = np.arange(0, 1 + step_size, step_size)\n",
    "            tp_rates = []\n",
    "            fp_rates = []\n",
    "            \n",
    "            for thresh in thresholds:\n",
    "                tp = np.sum((all_values[labels == 1] >= thresh))\n",
    "                fp = np.sum((all_values[labels == 0] >= thresh))\n",
    "                fn = np.sum((all_values[labels == 1] < thresh))\n",
    "                tn = np.sum((all_values[labels == 0] < thresh))\n",
    "                \n",
    "                tpr = tp / max(1, (tp + fn))\n",
    "                fpr = fp / max(1, (fp + tn))\n",
    "                tp_rates.append(tpr)\n",
    "                fp_rates.append(fpr)\n",
    "            \n",
    "            auc = np.trapz(tp_rates[::-1], fp_rates[::-1])\n",
    "            aucs.append(auc)\n",
    "        \n",
    "        return np.mean(aucs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cc(pred_map, gt_map):\n",
    "        \"\"\"\n",
    "        CC (Correlation Coefficient): Pearson correlation with ground truth.\n",
    "        \n",
    "        Args:\n",
    "            pred_map: Predicted saliency map (H, W)\n",
    "            gt_map: Ground truth saliency map (H, W)\n",
    "        \"\"\"\n",
    "        pred_map = SaliencyMetrics.normalize_map(pred_map)\n",
    "        gt_map = SaliencyMetrics.normalize_map(gt_map)\n",
    "        \n",
    "        pred_flat = pred_map.flatten()\n",
    "        gt_flat = gt_map.flatten()\n",
    "        \n",
    "        # Remove mean\n",
    "        pred_centered = pred_flat - pred_flat.mean()\n",
    "        gt_centered = gt_flat - gt_flat.mean()\n",
    "        \n",
    "        # Pearson correlation\n",
    "        numerator = np.sum(pred_centered * gt_centered)\n",
    "        denominator = np.sqrt(np.sum(pred_centered ** 2) * np.sum(gt_centered ** 2))\n",
    "        \n",
    "        if denominator < 1e-10:\n",
    "            return 0.0\n",
    "        \n",
    "        return numerator / denominator\n",
    "    \n",
    "    @staticmethod\n",
    "    def nss(pred_map, fixation_map):\n",
    "        \"\"\"\n",
    "        NSS (Normalized Scanpath Saliency): Mean normalized saliency at fixation points.\n",
    "        \n",
    "        Args:\n",
    "            pred_map: Predicted saliency map (H, W)\n",
    "            fixation_map: Binary fixation map (H, W)\n",
    "        \"\"\"\n",
    "        pred_map = pred_map.astype(np.float32)\n",
    "        fixation_map = fixation_map.astype(bool)\n",
    "        \n",
    "        if not fixation_map.any():\n",
    "            return np.nan\n",
    "        \n",
    "        # Normalize saliency map: zero mean, unit std\n",
    "        pred_mean = pred_map.mean()\n",
    "        pred_std = pred_map.std()\n",
    "        \n",
    "        if pred_std < 1e-10:\n",
    "            return 0.0\n",
    "        \n",
    "        pred_normalized = (pred_map - pred_mean) / pred_std\n",
    "        \n",
    "        # Mean value at fixation points\n",
    "        nss_value = pred_normalized[fixation_map].mean()\n",
    "        \n",
    "        return nss_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_all_metrics(pred_map, gt_map, fixation_map=None):\n",
    "        \"\"\"\n",
    "        Compute all metrics.\n",
    "        \n",
    "        Args:\n",
    "            pred_map: Predicted saliency (H, W) numpy array\n",
    "            gt_map: Ground truth continuous saliency (H, W)\n",
    "            fixation_map: Binary fixation map (H, W), optional\n",
    "                         If None, will threshold gt_map > 0.5\n",
    "        \n",
    "        Returns:\n",
    "            dict with all metric scores\n",
    "        \"\"\"\n",
    "        pred_map = np.asarray(pred_map, dtype=np.float32)\n",
    "        gt_map = np.asarray(gt_map, dtype=np.float32)\n",
    "        \n",
    "        if fixation_map is None:\n",
    "            # Create fixation map from GT by thresholding\n",
    "            fixation_map = (gt_map > 0.5).astype(np.uint8)\n",
    "        else:\n",
    "            fixation_map = np.asarray(fixation_map, dtype=np.uint8)\n",
    "        \n",
    "        metrics = {\n",
    "            'CC': SaliencyMetrics.cc(pred_map, gt_map),\n",
    "            'NSS': SaliencyMetrics.nss(pred_map, fixation_map),\n",
    "            'AUC-Judd': SaliencyMetrics.auc_judd(pred_map, fixation_map),\n",
    "            'AUC-Borji': SaliencyMetrics.auc_borji(pred_map, fixation_map, n_splits=100)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "print('Saliency metrics module loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec66ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_metrics_on_dataset(loader, num_samples=None, use_guidance=True):\n",
    "    \"\"\"\n",
    "    Evaluate saliency metrics on validation/test set.\n",
    "    \n",
    "    Args:\n",
    "        loader: DataLoader for the dataset\n",
    "        num_samples: Max number of samples to evaluate (None = all)\n",
    "        use_guidance: Whether to use classifier-free guidance\n",
    "    \n",
    "    Returns:\n",
    "        dict with averaged metrics\n",
    "    \"\"\"\n",
    "    ddpm.eval()\n",
    "    \n",
    "    all_metrics = {\n",
    "        'CC': [],\n",
    "        'NSS': [],\n",
    "        'AUC-Judd': [],\n",
    "        'AUC-Borji': []\n",
    "    }\n",
    "    \n",
    "    guidance_scale = cfg.cfg_guidance_scale if use_guidance else None\n",
    "    count = 0\n",
    "    \n",
    "    for imgs, sal_maps, fnames in loader:\n",
    "        if num_samples is not None and count >= num_samples:\n",
    "            break\n",
    "            \n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        \n",
    "        # Normalize GT maps\n",
    "        sal_maps_norm = sal_maps / sal_maps.amax(dim=(-2, -1), keepdim=True).clamp(min=1e-6)\n",
    "        \n",
    "        # Generate predictions\n",
    "        B = imgs.size(0)\n",
    "        preds = ddpm.sample(\n",
    "            imgs,\n",
    "            shape=sal_maps.shape,\n",
    "            guidance_scale=guidance_scale\n",
    "        ).clamp(0.0, 1.0)\n",
    "        \n",
    "        # Compute metrics for each image in batch\n",
    "        for i in range(B):\n",
    "            if num_samples is not None and count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            pred_np = preds[i, 0].cpu().numpy()\n",
    "            gt_np = sal_maps_norm[i, 0].cpu().numpy()\n",
    "            \n",
    "            # Create fixation map by thresholding GT\n",
    "            fixation_map = (gt_np > 0.5).astype(np.uint8)\n",
    "            \n",
    "            # Compute all metrics\n",
    "            metrics = SaliencyMetrics.compute_all_metrics(pred_np, gt_np, fixation_map)\n",
    "            \n",
    "            for key, value in metrics.items():\n",
    "                if not np.isnan(value):\n",
    "                    all_metrics[key].append(value)\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            if count % 10 == 0:\n",
    "                print(f'Processed {count} samples...')\n",
    "    \n",
    "    # Average metrics\n",
    "    avg_metrics = {}\n",
    "    for key, values in all_metrics.items():\n",
    "        if len(values) > 0:\n",
    "            avg_metrics[key] = np.mean(values)\n",
    "            avg_metrics[f'{key}_std'] = np.std(values)\n",
    "        else:\n",
    "            avg_metrics[key] = np.nan\n",
    "            avg_metrics[f'{key}_std'] = np.nan\n",
    "    \n",
    "    print('\\n' + '='*60)\n",
    "    print('SALIENCY EVALUATION METRICS')\n",
    "    print('='*60)\n",
    "    print(f'Evaluated on {count} samples')\n",
    "    print('-'*60)\n",
    "    for key in ['CC', 'NSS', 'AUC-Judd', 'AUC-Borji']:\n",
    "        mean_val = avg_metrics.get(key, np.nan)\n",
    "        std_val = avg_metrics.get(f'{key}_std', np.nan)\n",
    "        print(f'{key:12s}: {mean_val:.4f} ± {std_val:.4f}')\n",
    "    print('='*60)\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "# Evaluate on validation set (use fewer samples for speed)\n",
    "if len(val_dataset) > 0:\n",
    "    val_metrics = evaluate_metrics_on_dataset(val_loader, num_samples=50, use_guidance=True)\n",
    "else:\n",
    "    print('No validation data available for evaluation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bf8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, image_size=(256, 192)):\n",
    "        self.img_dir = img_dir\n",
    "        self.files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.files.sort()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size[1], image_size[0])),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        img_path = os.path.join(self.img_dir, fname)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img, fname\n",
    "\n",
    "test_dataset = TestImageDataset(cfg.test_img_dir, image_size=cfg.image_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "print('Test dataset size:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c62f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on test split (random subset, visualize image + saliency)\n",
    "@torch.no_grad()\n",
    "def infer_test_split(loader, num_samples=10, save_numpy=True, save_png=True, guidance_scale=None):\n",
    "    ddpm.eval()\n",
    "    pred_dir = os.path.join(cfg.output_dir, 'test_predictions')\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "    dataset = loader.dataset\n",
    "    if len(dataset) == 0:\n",
    "        print('Test dataset empty; nothing to infer.')\n",
    "        return\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    chosen = rng.choice(len(dataset), size=min(num_samples, len(dataset)), replace=False)\n",
    "    subset = torch.utils.data.Subset(dataset, chosen.tolist())\n",
    "    subset_loader = DataLoader(\n",
    "        subset,\n",
    "        batch_size=loader.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=loader.num_workers,\n",
    "        pin_memory=getattr(loader, 'pin_memory', False),\n",
    "    )\n",
    "\n",
    "    processed = 0\n",
    "    for imgs, fnames in subset_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        preds = ddpm.sample(\n",
    "            imgs,\n",
    "            shape=(imgs.size(0), 1, cfg.image_size[1], cfg.image_size[0]),\n",
    "            guidance_scale=guidance_scale if guidance_scale is not None else cfg.cfg_guidance_scale\n",
    "        ).clamp(0.0, 1.0)\n",
    "        for i in range(imgs.size(0)):\n",
    "            fname = fnames[i]\n",
    "            stem = os.path.splitext(fname)[0]\n",
    "            pred = preds[i, 0].cpu()\n",
    "\n",
    "            if save_numpy:\n",
    "                np.save(os.path.join(pred_dir, f'{stem}.npy'), pred.numpy())\n",
    "\n",
    "            if save_png:\n",
    "                img_rgb = imgs[i].cpu().permute(1, 2, 0).clamp(0.0, 1.0)\n",
    "\n",
    "                fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
    "                axs[0].imshow(img_rgb)\n",
    "                axs[0].set_title('Image')\n",
    "                axs[0].axis('off')\n",
    "\n",
    "                axs[1].imshow(pred, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "                axs[1].set_title('Predicted Saliency')\n",
    "                axs[1].axis('off')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(pred_dir, f'{stem}.png'), dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "            processed += 1\n",
    "\n",
    "    print(f'Saved predictions to {pred_dir} (samples={processed}).')\n",
    "\n",
    "# Run inference over 10 random test images\n",
    "if len(test_dataset) > 0:\n",
    "    infer_test_split(test_loader, num_samples=10)\n",
    "else:\n",
    "    print('Test dataset empty; nothing to infer.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ZIP file for submission\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "def create_submission_zip(submission_dir: str, output_zip: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Create a ZIP file from the submission directory.\n",
    "    \n",
    "    Args:\n",
    "        submission_dir: Directory containing PNG saliency maps\n",
    "        output_zip: Path to output ZIP file (default: submission_dir.zip)\n",
    "    \"\"\"\n",
    "    if output_zip is None:\n",
    "        output_zip = f'{submission_dir}.zip'\n",
    "    \n",
    "    if not os.path.isdir(submission_dir):\n",
    "        print(f'Error: Directory not found: {submission_dir}')\n",
    "        return None\n",
    "    \n",
    "    png_files = sorted(Path(submission_dir).glob('*.png'))\n",
    "    \n",
    "    if len(png_files) == 0:\n",
    "        print(f'Error: No PNG files found in {submission_dir}')\n",
    "        return None\n",
    "    \n",
    "    print(f'Creating ZIP archive with {len(png_files)} files...')\n",
    "    \n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for png_file in png_files:\n",
    "            # Add file with just the filename (no directory structure)\n",
    "            zipf.write(png_file, arcname=png_file.name)\n",
    "    \n",
    "    zip_size_mb = os.path.getsize(output_zip) / (1024 * 1024)\n",
    "    print(f'\\n✓ Created submission ZIP: {output_zip}')\n",
    "    print(f'✓ Size: {zip_size_mb:.2f} MB')\n",
    "    print(f'✓ Files: {len(png_files)}')\n",
    "    print(f'\\nReady to submit to SALICON challenge!')\n",
    "    \n",
    "    return output_zip\n",
    "\n",
    "# Example: Create ZIP after generating predictions\n",
    "# Uncomment to run:\n",
    "# if os.path.isdir(submission_dir):\n",
    "#     zip_path = create_submission_zip(submission_dir)\n",
    "# else:\n",
    "#     print('Generate predictions first by running the previous cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f85e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_with_official_metrics(\n",
    "    loader,\n",
    "    fixations_json_path: str,\n",
    "    num_samples: Optional[int] = None,\n",
    "    use_guidance: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate using official MIT Saliency Benchmark metrics.\n",
    "    \n",
    "    Args:\n",
    "        loader: DataLoader with (images, saliency_maps, filenames)\n",
    "        fixations_json_path: Path to fixations JSON (for discrete fixation points)\n",
    "        num_samples: Max number of samples (None = all)\n",
    "        use_guidance: Use classifier-free guidance\n",
    "    \n",
    "    Returns:\n",
    "        dict with official metric scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from saliency.saliency_metrics import AUC_Judd, AUC_Borji, NSS, CC, SIM, EMD, KLdiv\n",
    "    except ImportError:\n",
    "        print('Official metrics not available. Please install saliency package:')\n",
    "        print('  pip install git+https://github.com/cvzoya/saliency.git')\n",
    "        return None\n",
    "    \n",
    "    ddpm.eval()\n",
    "    \n",
    "    # Load fixations JSON to get discrete fixation points\n",
    "    with open(fixations_json_path, 'r') as f:\n",
    "        fixations_data = json.load(f)\n",
    "    \n",
    "    # Build image_id -> fixations mapping\n",
    "    img_id_to_name = {img['id']: img['file_name'] for img in fixations_data.get('images', [])}\n",
    "    img_id_to_dims = {img['id']: (img['width'], img['height']) for img in fixations_data.get('images', [])}\n",
    "    \n",
    "    fixations_by_name = {}\n",
    "    for ann in fixations_data.get('annotations', []):\n",
    "        img_id = ann.get('image_id')\n",
    "        fname = img_id_to_name.get(img_id)\n",
    "        if not fname:\n",
    "            continue\n",
    "        pts = ann.get('fixations', [])\n",
    "        fixations_by_name.setdefault(fname, []).extend(pts)\n",
    "    \n",
    "    # Initialize metric accumulators\n",
    "    metrics_accum = {\n",
    "        'AUC-Judd': [],\n",
    "        'AUC-Borji': [],\n",
    "        'shuffled-AUC': [],\n",
    "        'NSS': [],\n",
    "        'CC': [],\n",
    "        'SIM': [],\n",
    "        'KL': [],\n",
    "        'EMD': []\n",
    "    }\n",
    "    \n",
    "    guidance_scale = cfg.cfg_guidance_scale if use_guidance else None\n",
    "    count = 0\n",
    "    \n",
    "    print(f'Evaluating with official SALICON metrics...')\n",
    "    \n",
    "    for imgs, sal_maps, fnames in loader:\n",
    "        if num_samples is not None and count >= num_samples:\n",
    "            break\n",
    "        \n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        \n",
    "        # Normalize GT\n",
    "        sal_maps_norm = sal_maps / sal_maps.amax(dim=(-2, -1), keepdim=True).clamp(min=1e-6)\n",
    "        \n",
    "        # Generate predictions\n",
    "        B = imgs.size(0)\n",
    "        preds = ddpm.sample(\n",
    "            imgs,\n",
    "            shape=sal_maps.shape,\n",
    "            guidance_scale=guidance_scale\n",
    "        ).clamp(0.0, 1.0)\n",
    "        \n",
    "        for i in range(B):\n",
    "            if num_samples is not None and count >= num_samples:\n",
    "                break\n",
    "            \n",
    "            fname = fnames[i]\n",
    "            \n",
    "            # Get prediction and GT as numpy arrays\n",
    "            pred_map = preds[i, 0].cpu().numpy()\n",
    "            gt_map = sal_maps_norm[i, 0].cpu().numpy()\n",
    "            \n",
    "            # Get discrete fixations for this image\n",
    "            fixations = fixations_by_name.get(fname, [])\n",
    "            if not fixations:\n",
    "                print(f'Warning: No fixations found for {fname}, skipping...')\n",
    "                continue\n",
    "            \n",
    "            # Convert fixations to binary map at current resolution\n",
    "            H, W = pred_map.shape\n",
    "            orig_dims = img_id_to_dims.get([k for k, v in img_id_to_name.items() if v == fname][0], (W, H))\n",
    "            orig_w, orig_h = orig_dims\n",
    "            \n",
    "            fixation_map = np.zeros((H, W), dtype=bool)\n",
    "            for fix in fixations:\n",
    "                if len(fix) < 2:\n",
    "                    continue\n",
    "                # SALICON: (row, col) = (y, x), 1-indexed\n",
    "                y, x = fix[0] - 1, fix[1] - 1\n",
    "                # Scale to current resolution\n",
    "                if orig_h > 1 and orig_w > 1:\n",
    "                    sy = int(np.round((y / (orig_h - 1)) * (H - 1)))\n",
    "                    sx = int(np.round((x / (orig_w - 1)) * (W - 1)))\n",
    "                else:\n",
    "                    sy, sx = int(y), int(x)\n",
    "                sy = np.clip(sy, 0, H - 1)\n",
    "                sx = np.clip(sx, 0, W - 1)\n",
    "                fixation_map[sy, sx] = True\n",
    "            \n",
    "            # Compute official metrics\n",
    "            try:\n",
    "                # Metrics that use saliency map + fixations\n",
    "                auc_j = AUC_Judd(pred_map, fixation_map)\n",
    "                nss = NSS(pred_map, fixation_map)\n",
    "                \n",
    "                # Metrics that compare two saliency maps\n",
    "                cc = CC(pred_map, gt_map)\n",
    "                sim = SIM(pred_map, gt_map)\n",
    "                kl = KLdiv(pred_map, gt_map)\n",
    "                \n",
    "                # AUC variants (these may require additional fixation sets)\n",
    "                try:\n",
    "                    auc_b = AUC_Borji(pred_map, fixation_map)\n",
    "                    metrics_accum['AUC-Borji'].append(auc_b)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    # shuffled AUC uses fixations from other images\n",
    "                    other_fixations = []\n",
    "                    for other_fname, other_pts in fixations_by_name.items():\n",
    "                        if other_fname != fname:\n",
    "                            other_fixations.extend(other_pts[:10])  # Sample some\n",
    "                            if len(other_fixations) >= 100:\n",
    "                                break\n",
    "                    \n",
    "                    if other_fixations:\n",
    "                        other_fix_map = np.zeros((H, W), dtype=bool)\n",
    "                        for fix in other_fixations[:100]:\n",
    "                            if len(fix) < 2:\n",
    "                                continue\n",
    "                            y, x = fix[0] - 1, fix[1] - 1\n",
    "                            if orig_h > 1 and orig_w > 1:\n",
    "                                sy = int(np.round((y / (orig_h - 1)) * (H - 1)))\n",
    "                                sx = int(np.round((x / (orig_w - 1)) * (W - 1)))\n",
    "                            else:\n",
    "                                sy, sx = int(y), int(x)\n",
    "                            sy = np.clip(sy, 0, H - 1)\n",
    "                            sx = np.clip(sx, 0, W - 1)\n",
    "                            other_fix_map[sy, sx] = True\n",
    "                        \n",
    "                        # Shuffled AUC\n",
    "                        s_auc = AUC_Judd(pred_map, other_fix_map)\n",
    "                        metrics_accum['shuffled-AUC'].append(s_auc)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # EMD (Earth Mover's Distance) - computationally expensive\n",
    "                try:\n",
    "                    emd = EMD(pred_map, gt_map)\n",
    "                    metrics_accum['EMD'].append(emd)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Store valid metrics\n",
    "                metrics_accum['AUC-Judd'].append(auc_j)\n",
    "                metrics_accum['NSS'].append(nss)\n",
    "                metrics_accum['CC'].append(cc)\n",
    "                metrics_accum['SIM'].append(sim)\n",
    "                metrics_accum['KL'].append(kl)\n",
    "                \n",
    "                count += 1\n",
    "                if count % 10 == 0:\n",
    "                    print(f'Processed {count} samples...')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f'Error computing metrics for {fname}: {e}')\n",
    "                continue\n",
    "    \n",
    "    # Compute averages\n",
    "    results = {}\n",
    "    print('\\n' + '='*70)\n",
    "    print('OFFICIAL SALICON EVALUATION METRICS')\n",
    "    print('='*70)\n",
    "    print(f'Evaluated on {count} samples')\n",
    "    print('-'*70)\n",
    "    \n",
    "    for metric_name, values in metrics_accum.items():\n",
    "        if len(values) > 0:\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values)\n",
    "            results[metric_name] = mean_val\n",
    "            results[f'{metric_name}_std'] = std_val\n",
    "            print(f'{metric_name:15s}: {mean_val:.4f} ± {std_val:.4f}')\n",
    "        else:\n",
    "            print(f'{metric_name:15s}: N/A')\n",
    "    \n",
    "    print('='*70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Evaluate on validation set\n",
    "try:\n",
    "    # Check if official metrics are available\n",
    "    from saliency.saliency_metrics import AUC_Judd\n",
    "    metrics_available = True\n",
    "except ImportError:\n",
    "    metrics_available = False\n",
    "\n",
    "if metrics_available and len(val_dataset) > 0:\n",
    "    print('Run this cell to evaluate with official metrics:')\n",
    "    print('official_results = evaluate_with_official_metrics(')\n",
    "    print('    val_loader,')\n",
    "    print('    cfg.val_fixations_json,')\n",
    "    print('    num_samples=50,')\n",
    "    print('    use_guidance=True')\n",
    "    print(')')\n",
    "else:\n",
    "    if not metrics_available:\n",
    "        print('Install official metrics first: pip install git+https://github.com/cvzoya/saliency.git')\n",
    "    else:\n",
    "        print('Check validation dataset availability.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a40b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from saliency.saliency_metrics import AUC_Judd, AUC_Borji, NSS, CC, SIM, EMD, KLdiv\n",
    "    print('✓ Official SALICON evaluation metrics loaded successfully')\n",
    "    OFFICIAL_METRICS_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print('✗ Could not import official metrics. Install with:')\n",
    "    print('  pip install git+https://github.com/cvzoya/saliency.git')\n",
    "    print(f'\\nError: {e}')\n",
    "    OFFICIAL_METRICS_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d28b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install official SALICON evaluation tools\n",
    "!pip install git+https://github.com/cvzoya/saliency.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c631c76b",
   "metadata": {},
   "source": [
    "## SALICON Official Evaluation\n",
    "\n",
    "Use the official MIT Saliency Benchmark evaluation code.\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install git+https://github.com/cvzoya/saliency.git\n",
    "```\n",
    "\n",
    "The official metrics include: AUC-Judd, AUC-Borji, shuffled AUC, NSS, CC, SIM, EMD, KL-divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e816e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SALICON Challenge Submission - OPTIMIZED VERSION\n",
    "# Run this cell when ready to generate all test predictions\n",
    "\n",
    "if len(test_dataset) > 0:\n",
    "    submission_output = generate_salicon_submission_optimized(\n",
    "        test_img_dir=cfg.test_img_dir,\n",
    "        output_dir=submission_dir,\n",
    "        guidance_scale=cfg.cfg_guidance_scale,\n",
    "        batch_size=16,  # Optimized for RTX A4500\n",
    "        num_workers=8,  # Optimized for your CPU\n",
    "        use_amp=True    # Mixed precision for 2x speedup\n",
    "    )\n",
    "    print(f'\\n✓ Submission ready at: {submission_output}')\n",
    "else:\n",
    "    print('No test images found. Please check cfg.test_img_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_salicon_submission_optimized(\n",
    "    test_img_dir: str,\n",
    "    output_dir: str,\n",
    "    guidance_scale: Optional[float] = None,\n",
    "    batch_size: int = 16,  # Increased default\n",
    "    num_workers: int = 8,  # Increased default\n",
    "    use_amp: bool = True   # Mixed precision for faster inference\n",
    "):\n",
    "    \"\"\"\n",
    "    OPTIMIZED: Generate SALICON challenge submission with mixed precision.\n",
    "    \n",
    "    Optimizations:\n",
    "    - Mixed precision (AMP) for ~2x speedup\n",
    "    - Larger batch size (16 vs 4)\n",
    "    - More workers (8 vs 4)\n",
    "    - Progress tracking\n",
    "    \n",
    "    Args:\n",
    "        test_img_dir: Directory containing test images\n",
    "        output_dir: Directory to save PNG saliency maps\n",
    "        guidance_scale: Classifier-free guidance scale\n",
    "        batch_size: Batch size for inference (default 16)\n",
    "        num_workers: Number of data loading workers (default 8)\n",
    "        use_amp: Use automatic mixed precision (default True)\n",
    "    \"\"\"\n",
    "    from torch.cuda.amp import autocast\n",
    "    import time\n",
    "    \n",
    "    ddpm.eval()\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create dataset and loader\n",
    "    challenge_dataset = SaliconChallengeDataset(test_img_dir, model_input_size=cfg.image_size)\n",
    "    challenge_loader = DataLoader(\n",
    "        challenge_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f'Generating SALICON submission for {len(challenge_dataset)} test images...')\n",
    "    print(f'Output directory: {output_dir}')\n",
    "    print(f'Guidance scale: {guidance_scale}')\n",
    "    print(f'Batch size: {batch_size}')\n",
    "    print(f'Mixed precision (AMP): {use_amp and torch.cuda.is_available()}')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    processed = 0\n",
    "    \n",
    "    for imgs, fnames, orig_sizes in challenge_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        B = imgs.size(0)\n",
    "        \n",
    "        # Generate saliency maps with optional mixed precision\n",
    "        if use_amp and torch.cuda.is_available():\n",
    "            with autocast():\n",
    "                preds = ddpm.sample(\n",
    "                    imgs,\n",
    "                    shape=(B, 1, cfg.image_size[1], cfg.image_size[0]),\n",
    "                    guidance_scale=guidance_scale if guidance_scale is not None else cfg.cfg_guidance_scale\n",
    "                ).clamp(0.0, 1.0)\n",
    "        else:\n",
    "            preds = ddpm.sample(\n",
    "                imgs,\n",
    "                shape=(B, 1, cfg.image_size[1], cfg.image_size[0]),\n",
    "                guidance_scale=guidance_scale if guidance_scale is not None else cfg.cfg_guidance_scale\n",
    "            ).clamp(0.0, 1.0)\n",
    "        \n",
    "        # Save each prediction at original resolution\n",
    "        for i in range(B):\n",
    "            fname = fnames[i]\n",
    "            stem = os.path.splitext(fname)[0]\n",
    "            orig_w, orig_h = orig_sizes[0][i].item(), orig_sizes[1][i].item()\n",
    "            \n",
    "            # Get prediction and resize to original dimensions\n",
    "            pred = preds[i, 0].cpu().numpy()\n",
    "            \n",
    "            # Resize to original image size\n",
    "            pred_pil = Image.fromarray((pred * 255).astype(np.uint8), mode='L')\n",
    "            pred_resized = pred_pil.resize((orig_w, orig_h), resample=Image.BILINEAR)\n",
    "            \n",
    "            # Save as PNG\n",
    "            output_path = os.path.join(output_dir, f'{stem}.png')\n",
    "            pred_resized.save(output_path)\n",
    "            \n",
    "            processed += 1\n",
    "            if processed % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = processed / elapsed\n",
    "                remaining = (len(challenge_dataset) - processed) / rate\n",
    "                print(f'Processed {processed}/{len(challenge_dataset)} images... '\n",
    "                      f'[{rate:.1f} img/s, ETA: {remaining/60:.1f}min]')\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f'\\n✓ Generated {processed} saliency maps in {total_time/60:.1f} minutes')\n",
    "    print(f'✓ Average speed: {processed/total_time:.1f} images/sec')\n",
    "    print(f'✓ Saved to: {output_dir}')\n",
    "    print(f'\\nNext steps:')\n",
    "    print(f'1. Verify output files: ls {output_dir} | wc -l')\n",
    "    print(f'2. Create submission ZIP: cd {os.path.dirname(output_dir)} && zip -r salicon_submission.zip {os.path.basename(output_dir)}/')\n",
    "    print(f'3. Submit to SALICON challenge leaderboard')\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "print('✓ Optimized submission generator ready')\n",
    "print('  - Mixed precision (AMP) for ~2x speedup')\n",
    "print('  - Larger batch size (16) and workers (8)')\n",
    "print('  - ETA tracking and performance metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a799ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliconChallengeDataset(Dataset):\n",
    "    \"\"\"Dataset for SALICON challenge: test images without ground truth.\"\"\"\n",
    "    def __init__(self, img_dir: str, model_input_size=(256, 192)):\n",
    "        self.img_dir = img_dir\n",
    "        self.model_input_size = model_input_size\n",
    "        self.files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.files.sort()\n",
    "        \n",
    "        # Transform for model input (resized)\n",
    "        self.model_transform = transforms.Compose([\n",
    "            transforms.Resize((model_input_size[1], model_input_size[0])),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        img_path = os.path.join(self.img_dir, fname)\n",
    "        \n",
    "        # Load original image\n",
    "        img_pil = Image.open(img_path).convert('RGB')\n",
    "        orig_size = img_pil.size  # (width, height)\n",
    "        \n",
    "        # Transform for model\n",
    "        img_tensor = self.model_transform(img_pil)\n",
    "        \n",
    "        return img_tensor, fname, orig_size\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_salicon_submission(\n",
    "    test_img_dir: str,\n",
    "    output_dir: str,\n",
    "    guidance_scale: Optional[float] = None,\n",
    "    batch_size: int = 4,\n",
    "    num_workers: int = 4\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate SALICON challenge submission files.\n",
    "    \n",
    "    Args:\n",
    "        test_img_dir: Directory containing test images\n",
    "        output_dir: Directory to save PNG saliency maps\n",
    "        guidance_scale: Classifier-free guidance scale (use None or cfg.cfg_guidance_scale)\n",
    "        batch_size: Batch size for inference\n",
    "        num_workers: Number of data loading workers\n",
    "    \"\"\"\n",
    "    ddpm.eval()\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create dataset and loader\n",
    "    challenge_dataset = SaliconChallengeDataset(test_img_dir, model_input_size=cfg.image_size)\n",
    "    challenge_loader = DataLoader(\n",
    "        challenge_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f'Generating SALICON submission for {len(challenge_dataset)} test images...')\n",
    "    print(f'Output directory: {output_dir}')\n",
    "    print(f'Guidance scale: {guidance_scale}')\n",
    "    \n",
    "    processed = 0\n",
    "    for imgs, fnames, orig_sizes in challenge_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        B = imgs.size(0)\n",
    "        \n",
    "        # Generate saliency maps at model resolution\n",
    "        preds = ddpm.sample(\n",
    "            imgs,\n",
    "            shape=(B, 1, cfg.image_size[1], cfg.image_size[0]),\n",
    "            guidance_scale=guidance_scale if guidance_scale is not None else cfg.cfg_guidance_scale\n",
    "        ).clamp(0.0, 1.0)\n",
    "        \n",
    "        # Save each prediction at original resolution\n",
    "        for i in range(B):\n",
    "            fname = fnames[i]\n",
    "            stem = os.path.splitext(fname)[0]\n",
    "            orig_w, orig_h = orig_sizes[0][i].item(), orig_sizes[1][i].item()\n",
    "            \n",
    "            # Get prediction and resize to original dimensions\n",
    "            pred = preds[i, 0].cpu().numpy()\n",
    "            \n",
    "            # Resize to original image size\n",
    "            pred_pil = Image.fromarray((pred * 255).astype(np.uint8), mode='L')\n",
    "            pred_resized = pred_pil.resize((orig_w, orig_h), resample=Image.BILINEAR)\n",
    "            \n",
    "            # Save as PNG\n",
    "            output_path = os.path.join(output_dir, f'{stem}.png')\n",
    "            pred_resized.save(output_path)\n",
    "            \n",
    "            processed += 1\n",
    "            if processed % 100 == 0:\n",
    "                print(f'Processed {processed}/{len(challenge_dataset)} images...')\n",
    "    \n",
    "    print(f'\\n✓ Generated {processed} saliency maps')\n",
    "    print(f'✓ Saved to: {output_dir}')\n",
    "    print(f'\\nNext steps:')\n",
    "    print(f'1. Verify output files: ls {output_dir} | wc -l')\n",
    "    print(f'2. Create submission ZIP: cd {os.path.dirname(output_dir)} && zip -r salicon_submission.zip {os.path.basename(output_dir)}/')\n",
    "    print(f'3. Submit to SALICON challenge leaderboard')\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "# Configuration for submission\n",
    "submission_dir = os.path.join(cfg.output_dir, 'salicon_challenge_submission')\n",
    "\n",
    "print('Ready to generate SALICON challenge submission!')\n",
    "print(f'Test images: {cfg.test_img_dir}')\n",
    "print(f'Output: {submission_dir}')\n",
    "print('\\nRun the cell below to generate submission files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facfb16a",
   "metadata": {},
   "source": [
    "# Option 1: Mixed Precision Training (Automatic Mixed Precision)\n",
    "# Provides ~2x speedup with negligible accuracy impact\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "USE_AMP = torch.cuda.is_available()  # Enable if GPU available\n",
    "USE_MULTI_GPU = torch.cuda.device_count() > 1  # Enable if multiple GPUs\n",
    "\n",
    "print(f'Mixed Precision (AMP): {\"ENABLED\" if USE_AMP else \"DISABLED\"}')\n",
    "print(f'Multi-GPU: {\"ENABLED\" if USE_MULTI_GPU else \"DISABLED\"}')\n",
    "\n",
    "# Wrap model for multi-GPU if available\n",
    "if USE_MULTI_GPU:\n",
    "    print(f'Using {torch.cuda.device_count()} GPUs with DataParallel')\n",
    "    ddpm = nn.DataParallel(ddpm)\n",
    "    # Note: Access underlying model with ddpm.module for saving/loading\n",
    "\n",
    "# Initialize gradient scaler for mixed precision\n",
    "scaler = GradScaler() if USE_AMP else None\n",
    "\n",
    "def train_epoch_optimized(epoch_idx: int):\n",
    "    \"\"\"Optimized training with mixed precision and optional multi-GPU.\"\"\"\n",
    "    if USE_MULTI_GPU:\n",
    "        ddpm.module.train()\n",
    "    else:\n",
    "        ddpm.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for imgs, sal_maps, _ in train_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        \n",
    "        # Maybe drop conditioning for classifier-free guidance\n",
    "        if USE_MULTI_GPU:\n",
    "            imgs = ddpm.module._maybe_drop_condition(imgs) if hasattr(ddpm.module, '_maybe_drop_condition') else imgs\n",
    "        else:\n",
    "            imgs = _maybe_drop_condition(imgs)\n",
    "        \n",
    "        # Normalize saliency\n",
    "        sal_maps = sal_maps / sal_maps.amax(dim=(-2, -1), keepdim=True).clamp(min=1e-6)\n",
    "        \n",
    "        b = imgs.size(0)\n",
    "        t = torch.randint(0, cfg.timesteps, (b,), device=cfg.device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        if USE_AMP:\n",
    "            with autocast():\n",
    "                if USE_MULTI_GPU:\n",
    "                    loss = ddpm.module.p_losses(imgs, sal_maps, t)\n",
    "                else:\n",
    "                    loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "            \n",
    "            # Scaled backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # Regular training\n",
    "            if USE_MULTI_GPU:\n",
    "                loss = ddpm.module.p_losses(imgs, sal_maps, t)\n",
    "            else:\n",
    "                loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    scheduler.step()\n",
    "    return total_loss / max(1, num_batches)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch_optimized(epoch_idx: int):\n",
    "    \"\"\"Optimized validation with mixed precision.\"\"\"\n",
    "    if USE_MULTI_GPU:\n",
    "        ddpm.module.eval()\n",
    "    else:\n",
    "        ddpm.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for imgs, sal_maps, _ in val_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        sal_maps = sal_maps / sal_maps.amax(dim=(-2, -1), keepdim=True).clamp(min=1e-6)\n",
    "        \n",
    "        b = imgs.size(0)\n",
    "        t = torch.randint(0, cfg.timesteps, (b,), device=cfg.device).long()\n",
    "        \n",
    "        if USE_AMP:\n",
    "            with autocast():\n",
    "                if USE_MULTI_GPU:\n",
    "                    loss = ddpm.module.p_losses(imgs, sal_maps, t)\n",
    "                else:\n",
    "                    loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "        else:\n",
    "            if USE_MULTI_GPU:\n",
    "                loss = ddpm.module.p_losses(imgs, sal_maps, t)\n",
    "            else:\n",
    "                loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / max(1, num_batches)\n",
    "\n",
    "print('\\n✓ Optimized training functions ready')\n",
    "print('  - Mixed precision (AMP) for 2x speedup')\n",
    "print('  - Multi-GPU support if available')\n",
    "print('\\nUse train_epoch_optimized() and validate_epoch_optimized() for faster training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747344bf",
   "metadata": {},
   "source": [
    "# MIT300 Benchmark Integration\n",
    "\n",
    "Using the official MIT Saliency Benchmark with pysaliency for standardized evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1843cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and setup MIT300 benchmark dependencies\n",
    "import sys\n",
    "sys.path.insert(0, '/home/oosulliv/projects/Computer-Vision/saliency-benchmarking')\n",
    "\n",
    "import pysaliency\n",
    "from saliency_benchmarking.datasets import get_mit300\n",
    "from saliency_benchmarking.evaluation import MIT300 as MIT300Benchmark\n",
    "\n",
    "print('✓ MIT300 benchmark tools loaded')\n",
    "print('  - pysaliency for dataset handling')\n",
    "print('  - Official MIT300 evaluation metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MIT300 dataset\n",
    "# This will download ~300 test images from the MIT Saliency Benchmark\n",
    "import numpy\n",
    "\n",
    "print('Downloading MIT300 dataset...')\n",
    "print(numpy.version.version)\n",
    "mit300_stimuli = get_mit300()\n",
    "print(f'\\n✓ MIT300 dataset loaded')\n",
    "print(f'  - Number of images: {len(mit300_stimuli)}')\n",
    "print(f'  - Image sizes: variable (will be resized to {cfg.image_size})')\n",
    "print(f'  - No ground truth fixations (test set only)')\n",
    "print(f'\\nMIT300 is the official test set for the MIT Saliency Benchmark')\n",
    "print('Predictions will be evaluated on the server at: https://saliency.mit.edu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIT300Dataset(Dataset):\n",
    "    \"\"\"Dataset wrapper for MIT300 benchmark images.\"\"\"\n",
    "    \n",
    "    def __init__(self, stimuli, model_input_size=(256, 192)):\n",
    "        self.stimuli = stimuli\n",
    "        self.model_input_size = model_input_size\n",
    "        \n",
    "        # Store original sizes for resizing predictions back (height, width only)\n",
    "        self.original_sizes = []\n",
    "        for i in range(len(stimuli)):\n",
    "            img = stimuli.stimuli[i]\n",
    "            self.original_sizes.append(tuple(img.shape[:2]))  # (H, W)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(model_input_size, antialias=True),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stimuli)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image from pysaliency stimuli\n",
    "        img_array = self.stimuli.stimuli[idx]  # RGB numpy array\n",
    "        \n",
    "        # Convert to tensor and normalize\n",
    "        img_tensor = self.transform(img_array)\n",
    "        \n",
    "        # Get image filename\n",
    "        filename = self.stimuli.stimulus_ids[idx]\n",
    "        \n",
    "        orig_h, orig_w = self.original_sizes[idx][:2]\n",
    "        return img_tensor, filename, (orig_h, orig_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e38be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_mit300_predictions(\n",
    "    model,\n",
    "    stimuli,\n",
    "    output_dir,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "    use_amp=True,\n",
    "    use_guidance=True,\n",
    "    guidance_scale=1.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate saliency predictions for MIT300 benchmark.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained diffusion model\n",
    "        stimuli: MIT300 stimuli from pysaliency\n",
    "        output_dir: Directory to save predictions\n",
    "        batch_size: Batch size for inference\n",
    "        use_amp: Use automatic mixed precision\n",
    "        use_guidance: Use classifier-free guidance\n",
    "        guidance_scale: Guidance strength (>1 = stronger)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping stimulus_id to saliency map (original size)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create dataset and loader\n",
    "    mit300_dataset = MIT300Dataset(stimuli, model_input_size=cfg.image_size)\n",
    "    mit300_loader = DataLoader(\n",
    "        mit300_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    print(f'Generating MIT300 predictions for {len(mit300_dataset)} images...')\n",
    "    print(f'  - Batch size: {batch_size}')\n",
    "    print(f'  - Mixed precision: {use_amp}')\n",
    "    print(f'  - Classifier-free guidance: {use_guidance} (scale={guidance_scale})')\n",
    "    print(f'  - Output: {output_dir}')\n",
    "    \n",
    "    for batch_imgs, filenames, orig_sizes in tqdm(mit300_loader, desc='MIT300 Inference'):\n",
    "        batch_imgs = batch_imgs.to(cfg.device)\n",
    "        \n",
    "        pred_shape = (batch_imgs.size(0), 1, batch_imgs.size(2), batch_imgs.size(3))\n",
    "\n",
    "\n",
    "        if use_amp:\n",
    "            with autocast():\n",
    "                saliency_pred = model.sample(\n",
    "                    batch_imgs,\n",
    "                    shape=pred_shape,\n",
    "                    guidance_scale=guidance_scale if use_guidance else None\n",
    "                )\n",
    "        else:\n",
    "            saliency_pred = model.sample(\n",
    "                batch_imgs,\n",
    "                shape=pred_shape,\n",
    "                guidance_scale=guidance_scale if use_guidance else None\n",
    "            )\n",
    "\n",
    "            if use_guidance:\n",
    "                saliency_pred = model.sample(\n",
    "                    batch_imgs,\n",
    "                    guidance_scale=guidance_scale\n",
    "                )\n",
    "            else:\n",
    "                saliency_pred = model.sample(batch_imgs)\n",
    "        \n",
    "        # Process each image in batch\n",
    "        # orig_sizes is a tuple of tensors: (heights_tensor, widths_tensor)\n",
    "        heights, widths = orig_sizes\n",
    "        for i, (pred, filename) in enumerate(zip(saliency_pred, filenames)):\n",
    "            # pred shape: (1, H, W)\n",
    "            pred = pred.squeeze(0).cpu().numpy()  # (H, W)\n",
    "            \n",
    "            # Resize back to original dimensions\n",
    "            orig_h = int(heights[i].item()) if hasattr(heights[i], 'item') else int(heights[i])\n",
    "            orig_w = int(widths[i].item()) if hasattr(widths[i], 'item') else int(widths[i])\n",
    "            pred_pil = Image.fromarray((pred * 255).astype(np.uint8))\n",
    "            pred_resized = pred_pil.resize((orig_w, orig_h), Image.BILINEAR)\n",
    "            pred_resized = np.array(pred_resized).astype(np.float32) / 255.0\n",
    "            \n",
    "            # Normalize to [0, 1]\n",
    "            pred_resized = (pred_resized - pred_resized.min()) / (pred_resized.max() - pred_resized.min() + 1e-8)\n",
    "            \n",
    "            # Save prediction\n",
    "            output_path = os.path.join(output_dir, f'{filename}.png')\n",
    "            pred_uint8 = (pred_resized * 255).astype(np.uint8)\n",
    "            Image.fromarray(pred_uint8).save(output_path)\n",
    "            \n",
    "            predictions[filename] = pred_resized\n",
    "    \n",
    "    print(f'\\n✓ Generated {len(predictions)} MIT300 predictions')\n",
    "    print(f'  - Saved to: {output_dir}')\n",
    "    print(f'\\nNext steps:')\n",
    "    print(f'  1. Create submission file: zip -r mit300_submission.zip {os.path.basename(output_dir)}/')\n",
    "    print(f'  2. Submit to: https://saliency.mit.edu/')\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print('✓ MIT300 prediction generator ready')\n",
    "print('  - Supports mixed precision inference')\n",
    "print('  - Handles variable image sizes')\n",
    "print('  - Resizes predictions to original dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb44bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MIT300 predictions (uncomment to run)\n",
    "from tqdm import tqdm \n",
    "\n",
    "mit300_output = os.path.join(cfg.output_dir, 'mit300_predictions')\n",
    "mit300_predictions = generate_mit300_predictions(\n",
    "    model=ddpm,\n",
    "    stimuli=mit300_stimuli,\n",
    "    output_dir=mit300_output,\n",
    "    batch_size=16,\n",
    "    use_amp=USE_AMP,\n",
    "    use_guidance=True,\n",
    "    guidance_scale=1.5\n",
    ")\n",
    "\n",
    "print('Ready to generate MIT300 predictions!')\n",
    "print('\\nUncomment the code above and run after training to:')\n",
    "print('  1. Generate predictions for all 300 MIT300 test images')\n",
    "print('  2. Save predictions as PNG files (original dimensions)')\n",
    "print('  3. Create submission ZIP for MIT Saliency Benchmark')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9853cc7d",
   "metadata": {},
   "source": [
    "## MIT1003 for Development & Evaluation\n",
    "\n",
    "MIT1003 contains 1003 images with ground truth fixations - use this for model development and evaluation before submitting to MIT300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc3c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MIT1003 dataset (for evaluation with ground truth)\n",
    "from saliency_benchmarking.datasets import get_mit1003\n",
    "from saliency_benchmarking.evaluation import MIT1003 as MIT1003Benchmark\n",
    "\n",
    "print('Downloading MIT1003 dataset...')\n",
    "mit1003_stimuli, mit1003_fixations = get_mit1003()\n",
    "\n",
    "print(f'\\n✓ MIT1003 dataset loaded')\n",
    "print(f'  - Number of images: {len(mit1003_stimuli)}')\n",
    "print(f'  - Ground truth fixations: ✓ Available')\n",
    "print(f'  - Can compute metrics: AUC, sAUC, NSS, IG, CC, KL-Div, SIM')\n",
    "print(f'\\nUse MIT1003 for:')\n",
    "print(f'  - Model development and hyperparameter tuning')\n",
    "print(f'  - Computing metrics before MIT300 submission')\n",
    "print(f'  - Comparing with baseline models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionSaliencyMapModel(pysaliency.SaliencyMapModel):\n",
    "    \"\"\"Wrapper to make diffusion model compatible with pysaliency evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, diffusion_model, model_input_size=(256, 192), device='cuda', \n",
    "                 use_amp=True, use_guidance=True, guidance_scale=1.5):\n",
    "        super().__init__()\n",
    "        self.diffusion_model = diffusion_model\n",
    "        self.model_input_size = model_input_size\n",
    "        self.device = device\n",
    "        self.use_amp = use_amp\n",
    "        self.use_guidance = use_guidance\n",
    "        self.guidance_scale = guidance_scale\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(model_input_size, antialias=True),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.diffusion_model.eval()\n",
    "    \n",
    "    def _predict(self, stimuli, stimulus_indices):\n",
    "        \"\"\"Generate saliency maps for given stimuli indices.\"\"\"\n",
    "        saliency_maps = []\n",
    "        \n",
    "        for idx in tqdm(stimulus_indices, desc='Generating predictions', leave=False):\n",
    "            # Get image\n",
    "            img_array = stimuli.stimuli[idx]\n",
    "            orig_h, orig_w = img_array.shape[:2]\n",
    "            \n",
    "            # Transform and add batch dimension\n",
    "            img_tensor = self.transform(img_array).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Generate saliency map\n",
    "            with torch.no_grad():\n",
    "                if self.use_amp:\n",
    "                    with autocast():\n",
    "                        if self.use_guidance:\n",
    "                            sal_pred = self.diffusion_model.sample(\n",
    "                                img_tensor, \n",
    "                                guidance_scale=self.guidance_scale\n",
    "                            )\n",
    "                        else:\n",
    "                            sal_pred = self.diffusion_model.sample(img_tensor)\n",
    "                else:\n",
    "                    if self.use_guidance:\n",
    "                        sal_pred = self.diffusion_model.sample(\n",
    "                            img_tensor,\n",
    "                            guidance_scale=self.guidance_scale\n",
    "                        )\n",
    "                    else:\n",
    "                        sal_pred = self.diffusion_model.sample(img_tensor)\n",
    "            \n",
    "            # Convert to numpy and resize to original dimensions\n",
    "            sal_map = sal_pred[0, 0].cpu().numpy()  # (H, W)\n",
    "            \n",
    "            # Resize to original size\n",
    "            sal_pil = Image.fromarray((sal_map * 255).astype(np.uint8))\n",
    "            sal_resized = sal_pil.resize((orig_w, orig_h), Image.BILINEAR)\n",
    "            sal_resized = np.array(sal_resized).astype(np.float32) / 255.0\n",
    "            \n",
    "            # Normalize\n",
    "            sal_resized = (sal_resized - sal_resized.min()) / (sal_resized.max() - sal_resized.min() + 1e-8)\n",
    "            \n",
    "            saliency_maps.append(sal_resized)\n",
    "        \n",
    "        return saliency_maps\n",
    "\n",
    "print('✓ DiffusionSaliencyMapModel wrapper defined')\n",
    "print('  - Compatible with pysaliency evaluation framework')\n",
    "print('  - Automatically handles resizing to original dimensions')\n",
    "print('  - Supports all MIT Saliency Benchmark metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_on_mit1003(model, stimuli, fixations, sample_size=None, use_amp=True, \n",
    "                        use_guidance=True, guidance_scale=1.5):\n",
    "    \"\"\"\n",
    "    Evaluate diffusion model on MIT1003 using official metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained diffusion model\n",
    "        stimuli: MIT1003 stimuli\n",
    "        fixations: MIT1003 fixations\n",
    "        sample_size: Number of images to evaluate (None = all)\n",
    "        use_amp: Use automatic mixed precision\n",
    "        use_guidance: Use classifier-free guidance\n",
    "        guidance_scale: Guidance strength\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metric scores\n",
    "    \"\"\"\n",
    "    # Wrap model for pysaliency\n",
    "    saliency_model = DiffusionSaliencyMapModel(\n",
    "        diffusion_model=model,\n",
    "        model_input_size=cfg.image_size,\n",
    "        device=cfg.device,\n",
    "        use_amp=use_amp,\n",
    "        use_guidance=use_guidance,\n",
    "        guidance_scale=guidance_scale\n",
    "    )\n",
    "    \n",
    "    # Create benchmark evaluator\n",
    "    benchmark = MIT1003Benchmark()\n",
    "    \n",
    "    # Optionally subsample for faster evaluation\n",
    "    if sample_size is not None and sample_size < len(stimuli):\n",
    "        print(f'Evaluating on random subset of {sample_size} images...')\n",
    "        indices = np.random.choice(len(stimuli), sample_size, replace=False)\n",
    "        stimuli = stimuli[indices]\n",
    "        fixations = fixations[fixations.n.isin(indices)]\n",
    "    \n",
    "    print(f'\\nEvaluating on MIT1003 ({len(stimuli)} images)...')\n",
    "    print(f'  - Metrics: AUC, sAUC, NSS, CC, KL-Div, SIM')\n",
    "    print(f'  - Guidance: {use_guidance} (scale={guidance_scale})')\n",
    "    print(f'  - Mixed precision: {use_amp}')\n",
    "    \n",
    "    # Evaluate all metrics\n",
    "    results = {}\n",
    "    \n",
    "    print('\\nComputing AUC (Area Under ROC Curve)...')\n",
    "    auc_scores = saliency_model.AUCs(stimuli, fixations, verbose=True)\n",
    "    results['AUC'] = np.mean(auc_scores)\n",
    "    results['AUC_std'] = np.std(auc_scores)\n",
    "    \n",
    "    print('\\nComputing sAUC (Shuffled AUC)...')\n",
    "    sauc_scores = saliency_model.AUCs(stimuli, fixations, nonfixations='shuffled', verbose=True)\n",
    "    results['sAUC'] = np.mean(sauc_scores)\n",
    "    results['sAUC_std'] = np.std(sauc_scores)\n",
    "    \n",
    "    print('\\nComputing NSS (Normalized Scanpath Saliency)...')\n",
    "    nss_scores = saliency_model.NSSs(stimuli, fixations, verbose=True)\n",
    "    results['NSS'] = np.mean(nss_scores)\n",
    "    results['NSS_std'] = np.std(nss_scores)\n",
    "    \n",
    "    print('\\nComputing CC (Correlation Coefficient)...')\n",
    "    # Need empirical maps for CC\n",
    "    empirical_maps = pysaliency.FixationMap(stimuli, fixations, kernel_size=23.99)\n",
    "    cc_scores = saliency_model.CCs(stimuli, empirical_maps, verbose=True)\n",
    "    results['CC'] = np.mean(cc_scores)\n",
    "    results['CC_std'] = np.std(cc_scores)\n",
    "    \n",
    "    print('\\nComputing KL-Div (KL Divergence)...')\n",
    "    kl_scores = saliency_model.image_based_kl_divergences(\n",
    "        stimuli, empirical_maps, verbose=True,\n",
    "        minimum_value=0,\n",
    "        log_regularization=2.2204e-16,\n",
    "        quotient_regularization=2.2204e-16\n",
    "    )\n",
    "    results['KL-Div'] = np.mean(kl_scores)\n",
    "    results['KL-Div_std'] = np.std(kl_scores)\n",
    "    \n",
    "    print('\\nComputing SIM (Similarity)...')\n",
    "    sim_scores = saliency_model.similarities(stimuli, empirical_maps, verbose=True)\n",
    "    results['SIM'] = np.mean(sim_scores)\n",
    "    results['SIM_std'] = np.std(sim_scores)\n",
    "    \n",
    "    # Print summary\n",
    "    print('\\n' + '='*60)\n",
    "    print('MIT1003 EVALUATION RESULTS')\n",
    "    print('='*60)\n",
    "    for metric in ['AUC', 'sAUC', 'NSS', 'CC', 'KL-Div', 'SIM']:\n",
    "        mean = results[metric]\n",
    "        std = results[metric + '_std']\n",
    "        print(f'{metric:8s}: {mean:.4f} ± {std:.4f}')\n",
    "    print('='*60)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print('✓ MIT1003 evaluation function ready')\n",
    "print('  - Computes all standard saliency metrics')\n",
    "print('  - Compatible with MIT Saliency Benchmark')\n",
    "print('  - Returns mean and standard deviation for each metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f3262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on MIT1003 (uncomment to run after training)\n",
    "# mit1003_results = evaluate_on_mit1003(\n",
    "#     model=ddpm,\n",
    "#     stimuli=mit1003_stimuli,\n",
    "#     fixations=mit1003_fixations,\n",
    "#     sample_size=None,  # Use all 1003 images (set to e.g. 100 for quick test)\n",
    "#     use_amp=USE_AMP,\n",
    "#     use_guidance=True,\n",
    "#     guidance_scale=1.5\n",
    "# )\n",
    "\n",
    "print('Ready to evaluate on MIT1003!')\n",
    "print('\\nUncomment the code above after training to:')\n",
    "print('  1. Evaluate on 1003 images with ground truth fixations')\n",
    "print('  2. Compute all standard metrics (AUC, sAUC, NSS, CC, KL-Div, SIM)')\n",
    "print('  3. Get mean ± std for each metric')\n",
    "print('  4. Compare with published baselines')\n",
    "print('\\nTip: Use sample_size=100 for quick testing during development')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82605490",
   "metadata": {},
   "source": [
    "## Training on MIT1003 (Optional)\n",
    "\n",
    "You can also train directly on MIT1003 instead of SALICON. MIT1003 has 1003 images with eye tracking fixations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9558711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIT1003SaliencyDataset(Dataset):\n",
    "    \"\"\"Dataset for training on MIT1003 with fixation annotations.\"\"\"\n",
    "    \n",
    "    def __init__(self, stimuli, fixations, model_size=(256, 192), sigma=8.0):\n",
    "        self.stimuli = stimuli\n",
    "        self.fixations = fixations\n",
    "        self.model_size = model_size\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # Group fixations by image\n",
    "        self.fixations_by_image = {}\n",
    "        for n in range(len(stimuli)):\n",
    "            img_fixations = fixations[fixations.n == n]\n",
    "            self.fixations_by_image[n] = img_fixations\n",
    "        \n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(model_size, antialias=True),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stimuli)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image\n",
    "        img_array = self.stimuli.stimuli[idx]  # RGB numpy array\n",
    "        orig_h, orig_w = img_array.shape[:2]\n",
    "        \n",
    "        # Transform image\n",
    "        img_tensor = self.img_transform(img_array)\n",
    "        \n",
    "        # Create saliency map from fixations\n",
    "        img_fixations = self.fixations_by_image[idx]\n",
    "        \n",
    "        # Initialize empty saliency map\n",
    "        sal_map = np.zeros((orig_h, orig_w), dtype=np.float32)\n",
    "        \n",
    "        # Add Gaussian at each fixation location\n",
    "        y_coords = img_fixations.y_int.values\n",
    "        x_coords = img_fixations.x_int.values\n",
    "        \n",
    "        for y, x in zip(y_coords, x_coords):\n",
    "            if 0 <= y < orig_h and 0 <= x < orig_w:\n",
    "                sal_map[y, x] += 1.0\n",
    "        \n",
    "        # Apply Gaussian blur\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "        sal_map = gaussian_filter(sal_map, sigma=self.sigma)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        if sal_map.max() > 0:\n",
    "            sal_map = sal_map / sal_map.max()\n",
    "        \n",
    "        # Resize to model size\n",
    "        sal_pil = Image.fromarray((sal_map * 255).astype(np.uint8))\n",
    "        sal_resized = sal_pil.resize(self.model_size[::-1], Image.BILINEAR)\n",
    "        sal_tensor = torch.from_numpy(np.array(sal_resized)).float() / 255.0\n",
    "        sal_tensor = sal_tensor.unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        return img_tensor, sal_tensor, idx\n",
    "\n",
    "print('✓ MIT1003SaliencyDataset defined')\n",
    "print('  - Converts fixations to dense saliency maps')\n",
    "print('  - Compatible with existing training loop')\n",
    "print('  - Use for training directly on MIT data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6597c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create train/val split for MIT1003\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split into train (80%) and val (20%)\n",
    "# all_indices = list(range(len(mit1003_stimuli)))\n",
    "# train_indices, val_indices = train_test_split(all_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create datasets\n",
    "# mit1003_train = MIT1003SaliencyDataset(\n",
    "#     stimuli=mit1003_stimuli[train_indices],\n",
    "#     fixations=mit1003_fixations[mit1003_fixations.n.isin(train_indices)],\n",
    "#     model_size=cfg.image_size,\n",
    "#     sigma=cfg.saliency_gaussian_sigma\n",
    "# )\n",
    "\n",
    "# mit1003_val = MIT1003SaliencyDataset(\n",
    "#     stimuli=mit1003_stimuli[val_indices],\n",
    "#     fixations=mit1003_fixations[mit1003_fixations.n.isin(val_indices)],\n",
    "#     model_size=cfg.image_size,\n",
    "#     sigma=cfg.saliency_gaussian_sigma\n",
    "# )\n",
    "\n",
    "# # Create data loaders\n",
    "# mit1003_train_loader = DataLoader(\n",
    "#     mit1003_train,\n",
    "#     batch_size=cfg.batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=cfg.num_workers,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# mit1003_val_loader = DataLoader(\n",
    "#     mit1003_val,\n",
    "#     batch_size=cfg.batch_size,\n",
    "#     shuffle=False,\n",
    "#     num_workers=cfg.num_workers,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# print(f'MIT1003 Training: {len(mit1003_train)} images')\n",
    "# print(f'MIT1003 Validation: {len(mit1003_val)} images')\n",
    "\n",
    "print('MIT1003 training setup ready!')\n",
    "print('\\nTo train on MIT1003 instead of SALICON:')\n",
    "print('  1. Uncomment the code above to create train/val split')\n",
    "print('  2. Replace train_loader with mit1003_train_loader')\n",
    "print('  3. Replace val_loader with mit1003_val_loader')\n",
    "print('  4. Run the training loop')\n",
    "print('\\nNote: MIT1003 is smaller (1003 images) vs SALICON (10k+ images)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4f401",
   "metadata": {},
   "source": [
    "## Summary: MIT Saliency Benchmark Integration\n",
    "\n",
    "### Datasets Available:\n",
    "\n",
    "1. **SALICON** (Original)\n",
    "   - 10,000 training images\n",
    "   - 5,000 validation images  \n",
    "   - 5,000 test images (no labels)\n",
    "   - Challenge is inactive\n",
    "\n",
    "2. **MIT1003** (With Ground Truth)\n",
    "   - 1,003 images with eye tracking fixations\n",
    "   - Use for: Development, evaluation, hyperparameter tuning\n",
    "   - Can compute all metrics locally\n",
    "\n",
    "3. **MIT300** (Official Test Set)\n",
    "   - 300 images (no labels)\n",
    "   - Submit predictions to: https://saliency.mit.edu/\n",
    "   - Active benchmark with leaderboard\n",
    "\n",
    "### Workflow:\n",
    "\n",
    "#### Option A: Train on SALICON, Evaluate on MIT\n",
    "1. Train on SALICON (10k images) - already set up\n",
    "2. Evaluate on MIT1003 to get metrics\n",
    "3. Submit to MIT300 for official benchmarking\n",
    "\n",
    "#### Option B: Train on MIT1003\n",
    "1. Split MIT1003 into train/val (80/20)\n",
    "2. Train on smaller dataset (~800 images)\n",
    "3. Evaluate on validation split\n",
    "4. Submit to MIT300\n",
    "\n",
    "### Key Functions:\n",
    "- `evaluate_on_mit1003()` - Compute all metrics with ground truth\n",
    "- `generate_mit300_predictions()` - Create submission for MIT300\n",
    "- `DiffusionSaliencyMapModel` - Wrapper for pysaliency compatibility\n",
    "\n",
    "### Metrics Computed:\n",
    "- **AUC**: Area Under ROC Curve\n",
    "- **sAUC**: Shuffled AUC  \n",
    "- **NSS**: Normalized Scanpath Saliency\n",
    "- **CC**: Correlation Coefficient\n",
    "- **KL-Div**: KL Divergence\n",
    "- **SIM**: Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal DDPM loader without datasets\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Initialize model + DDPM on device without any dataloaders\n",
    "device = cfg.device if 'cfg' in globals() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNetSaliency().to(device)\n",
    "ddpm = SaliencyDDPM(model, timesteps=cfg.timesteps, beta_start=cfg.beta_start, beta_end=cfg.beta_end).to(device)\n",
    "\n",
    "# Try loading from best checkpoint, then last.pt\n",
    "candidates = []\n",
    "if hasattr(cfg, 'checkpoint_path') and os.path.isfile(cfg.checkpoint_path):\n",
    "    candidates.append(cfg.checkpoint_path)\n",
    "last_path = os.path.join(cfg.output_dir, 'last.pt')\n",
    "if os.path.isfile(last_path):\n",
    "    candidates.append(last_path)\n",
    "\n",
    "loaded = False\n",
    "for p in candidates:\n",
    "    try:\n",
    "        ckpt = torch.load(p, map_location=device)\n",
    "        state_dict = ckpt.get('model_state', ckpt)\n",
    "        ddpm.load_state_dict(state_dict)\n",
    "        print(f'Loaded DDPM weights from: {p}')\n",
    "        loaded = True\n",
    "        break\n",
    "    }\n",
    "    except Exception as e:\n",
    "        print(f'Failed to load {p}: {e}')\n",
    "if not loaded:\n",
    "    print('No valid checkpoint found; using fresh DDPM weights.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
