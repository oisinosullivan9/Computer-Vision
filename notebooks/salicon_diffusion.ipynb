{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    train_img_dir = os.path.abspath(os.path.join('..', 'data', 'train'))\n",
    "    val_img_dir = os.path.abspath(os.path.join('..', 'data', 'val'))\n",
    "    test_img_dir = os.path.abspath(os.path.join('..', 'data', 'test'))\n",
    "\n",
    "    train_fixations_json = os.path.abspath(os.path.join('..', 'data', 'fixations_train2014.json'))\n",
    "    val_fixations_json = os.path.abspath(os.path.join('..', 'data', 'fixations_val2014.json'))\n",
    "\n",
    "    image_size = (256, 192)\n",
    "    saliency_size = (256, 192)\n",
    "    saliency_gaussian_sigma = 4.0\n",
    "\n",
    "    batch_size = 4\n",
    "    num_workers = 4\n",
    "    num_epochs = 2\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    timesteps = 100\n",
    "    beta_start = 1e-4\n",
    "    beta_end = 0.02\n",
    "\n",
    "    output_dir = os.path.abspath('./saliency_diffusion_outputs')\n",
    "    checkpoint_path = os.path.join(output_dir, 'saliency_diffusion_unet.pt')\n",
    "    sample_dir = os.path.join(output_dir, 'samples')\n",
    "    gt_saliency_dir = os.path.abspath('./saliency_ground_truth')\n",
    "    train_saliency_dir = os.path.join(gt_saliency_dir, 'train')\n",
    "    val_saliency_dir = os.path.join(gt_saliency_dir, 'val')\n",
    "\n",
    "    device = 'cuda' if os.environ.get('CUDA_VISIBLE_DEVICES') is not None else 'cuda' if __import__('torch').cuda.is_available() else 'cpu'\n",
    "\n",
    "cfg = Config()\n",
    "for path in [cfg.output_dir, cfg.sample_dir, cfg.gt_saliency_dir, cfg.train_saliency_dir, cfg.val_saliency_dir]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "cfg.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import lru_cache\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "print('Using device:', cfg.device)\n",
    "print('Train dir exists:', os.path.isdir(cfg.train_img_dir))\n",
    "print('Val dir exists:', os.path.isdir(cfg.val_img_dir))\n",
    "print('Test dir exists:', os.path.isdir(cfg.test_img_dir))\n",
    "print('Train fixations JSON exists:', os.path.isfile(cfg.train_fixations_json))\n",
    "print('Val fixations JSON exists:', os.path.isfile(cfg.val_fixations_json))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbbc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: str) -> Dict:\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "@lru_cache(maxsize=32)\n",
    "def _gaussian_kernel1d(radius: int, sigma: float) -> np.ndarray:\n",
    "    ax = np.arange(-radius, radius + 1, dtype=np.float32)\n",
    "    kernel = np.exp(-(ax ** 2) / (2.0 * sigma ** 2))\n",
    "    kernel_sum = kernel.sum()\n",
    "    if kernel_sum > 0:\n",
    "        kernel /= kernel_sum\n",
    "    return kernel.astype(np.float32)\n",
    "\n",
    "def apply_gaussian_blur(array: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    if sigma <= 0:\n",
    "        return array\n",
    "    radius = max(1, int(round(3.0 * sigma)))\n",
    "    kernel = _gaussian_kernel1d(radius, float(sigma))\n",
    "    array = array.astype(np.float32, copy=False)\n",
    "    blurred = np.apply_along_axis(lambda m: np.convolve(m, kernel, mode='same'), axis=0, arr=array)\n",
    "    blurred = np.apply_along_axis(lambda m: np.convolve(m, kernel, mode='same'), axis=1, arr=blurred)\n",
    "    return blurred.astype(np.float32)\n",
    "    \n",
    "def parse_fixations_json(fixations: Dict, split: str, cache_dir: Optional[str] = None) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Parse SALICON-style fixation annotations into dense saliency maps.\"\"\"\n",
    "    if not fixations:\n",
    "        return {}\n",
    "\n",
    "    if cache_dir:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    # Map image id to file metadata\n",
    "    img_meta: Dict[int, str] = {}\n",
    "    dims: Dict[str, Tuple[int, int]] = {}\n",
    "    for img in fixations.get('images', []):\n",
    "        img_id = img.get('id')\n",
    "        fname = img.get('file_name')\n",
    "        if img_id is None or not fname:\n",
    "            continue\n",
    "        img_meta[img_id] = fname\n",
    "        width = img.get('width')\n",
    "        height = img.get('height')\n",
    "        if width and height:\n",
    "            dims[fname] = (int(width), int(height))\n",
    "\n",
    "    # Aggregate fixation points per image filename\n",
    "    mapping: Dict[str, List[Tuple[float, float]]] = {}\n",
    "    for ann in fixations.get('annotations', []):\n",
    "        img_id = ann.get('image_id')\n",
    "        fname = img_meta.get(img_id)\n",
    "        if not fname:\n",
    "            continue\n",
    "        pts = ann.get('fixations') or ann.get('points') or []\n",
    "        if not pts:\n",
    "            continue\n",
    "        bucket = mapping.setdefault(fname, [])\n",
    "        for p in pts:\n",
    "            if not isinstance(p, (list, tuple)) or len(p) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                row = float(p[0])\n",
    "                col = float(p[1])\n",
    "            except (TypeError, ValueError):\n",
    "                continue\n",
    "            if np.isnan(row) or np.isnan(col):\n",
    "                continue\n",
    "            # SALICON fixations are 1-indexed (row, col) == (y, x)\n",
    "            y = row - 1.0\n",
    "            x = col - 1.0\n",
    "            bucket.append((y, x))\n",
    "\n",
    "    saliency_maps: Dict[str, np.ndarray] = {}\n",
    "    W, H = cfg.saliency_size\n",
    "\n",
    "    processed = 0\n",
    "    filenames = sorted(set(img_meta.values()))\n",
    "    for fname in filenames:\n",
    "        sal_map = np.zeros((H, W), dtype=np.float32)\n",
    "        pts = mapping.get(fname, [])\n",
    "        orig_w, orig_h = dims.get(fname, (None, None))\n",
    "\n",
    "        if pts:\n",
    "            for y, x in pts:\n",
    "                if orig_w and orig_h and orig_w > 1 and orig_h > 1:\n",
    "                    sy = (y / max(1.0, orig_h - 1)) * (H - 1)\n",
    "                    sx = (x / max(1.0, orig_w - 1)) * (W - 1)\n",
    "                else:\n",
    "                    sy, sx = y, x\n",
    "\n",
    "                iy = int(round(np.clip(sy, 0, H - 1)))\n",
    "                ix = int(round(np.clip(sx, 0, W - 1)))\n",
    "                sal_map[iy, ix] += 1.0\n",
    "\n",
    "            if cfg.saliency_gaussian_sigma and cfg.saliency_gaussian_sigma > 0:\n",
    "                sal_map = apply_gaussian_blur(sal_map, cfg.saliency_gaussian_sigma)\n",
    "\n",
    "            if sal_map.max() > 0:\n",
    "                sal_map /= sal_map.max()\n",
    "\n",
    "        saliency_maps[fname] = sal_map.astype(np.float32)\n",
    "        processed += 1\n",
    "\n",
    "        if cache_dir:\n",
    "            stem = os.path.splitext(fname)[0]\n",
    "            cache_path = os.path.join(cache_dir, f'{stem}.npy')\n",
    "            np.save(cache_path, sal_map)\n",
    "\n",
    "    msg_prefix = f\"{split.capitalize()} saliency maps\"\n",
    "    if cache_dir:\n",
    "        print(f\"{msg_prefix}: saved {processed} arrays to {cache_dir}\")\n",
    "    else:\n",
    "        print(f\"{msg_prefix}: generated {processed} arrays in memory\")\n",
    "\n",
    "    return saliency_maps\n",
    "    \n",
    "try:\n",
    "    train_fix_raw = load_json(cfg.train_fixations_json)\n",
    "    val_fix_raw = load_json(cfg.val_fixations_json)\n",
    "    print('Loaded fixation JSONs.')\n",
    "except Exception as e:\n",
    "    print('Error loading or parsing fixation JSONs. Please adapt parse_fixations_json to your format.')\n",
    "    print(e)\n",
    "    train_fix_raw, val_fix_raw = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12431126",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_saliency_maps = parse_fixations_json(train_fix_raw, split='train', cache_dir=cfg.train_saliency_dir) if train_fix_raw is not None else {}\n",
    "val_saliency_maps = parse_fixations_json(val_fix_raw, split='val', cache_dir=cfg.val_saliency_dir) if val_fix_raw is not None else {}\n",
    "\n",
    "print('Train saliency entries in memory:', len(train_saliency_maps))\n",
    "print('Val saliency entries in memory:', len(val_saliency_maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliconSaliencyDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, saliency_maps: Dict[str, np.ndarray], image_size=(256, 192), saliency_cache_dir: Optional[str] = None):\n",
    "        self.img_dir = img_dir\n",
    "        self.saliency_maps = saliency_maps or {}\n",
    "        self.saliency_cache_dir = saliency_cache_dir\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Collect only images that have saliency info (either in-memory or cached on disk)\n",
    "        self.image_files = [f for f in os.listdir(img_dir) if self._has_saliency(f)]\n",
    "        self.image_files.sort()\n",
    "\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size[1], image_size[0])),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        # Saliency maps are kept in [0,1] range, single-channel\n",
    "\n",
    "    def _has_saliency(self, fname: str) -> bool:\n",
    "        if fname in self.saliency_maps:\n",
    "            return True\n",
    "        if not self.saliency_cache_dir:\n",
    "            return False\n",
    "        cache_path = os.path.join(self.saliency_cache_dir, f\"{os.path.splitext(fname)[0]}.npy\")\n",
    "        return os.path.isfile(cache_path)\n",
    "\n",
    "    def _load_saliency(self, fname: str) -> np.ndarray:\n",
    "        sal_map = self.saliency_maps.get(fname)\n",
    "        if sal_map is None and self.saliency_cache_dir:\n",
    "            cache_path = os.path.join(self.saliency_cache_dir, f\"{os.path.splitext(fname)[0]}.npy\")\n",
    "            if os.path.isfile(cache_path):\n",
    "                sal_map = np.load(cache_path, allow_pickle=False)\n",
    "        if sal_map is None:\n",
    "            raise KeyError(f'No saliency map found for {fname}.')\n",
    "        return sal_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, fname)\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.img_transform(img)\n",
    "\n",
    "        sal_map = self._load_saliency(fname)\n",
    "        H, W = self.image_size[1], self.image_size[0]\n",
    "        if sal_map.shape != (H, W):\n",
    "            sal_img = Image.fromarray((sal_map * 255).astype(np.uint8))\n",
    "            sal_img = sal_img.resize((W, H), resample=Image.BILINEAR)\n",
    "            sal_tensor = torch.from_numpy(np.array(sal_img)).float() / 255.0\n",
    "        else:\n",
    "            sal_tensor = torch.from_numpy(sal_map).float()\n",
    "        sal_tensor = sal_tensor.unsqueeze(0)  # (1, H, W)\n",
    "\n",
    "        return img, sal_tensor, fname\n",
    "    \n",
    "train_dataset = SaliconSaliencyDataset(\n",
    "    cfg.train_img_dir,\n",
    "    train_saliency_maps,\n",
    "    image_size=cfg.image_size,\n",
    "    saliency_cache_dir=cfg.train_saliency_dir\n",
    ")\n",
    "val_dataset = SaliconSaliencyDataset(\n",
    "    cfg.val_img_dir,\n",
    "    val_saliency_maps,\n",
    "    image_size=cfg.image_size,\n",
    "    saliency_cache_dir=cfg.val_saliency_dir\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "print('Train dataset size:', len(train_dataset))\n",
    "print('Val dataset size:', len(val_dataset))\n",
    "\n",
    "batch = next(iter(train_loader)) if len(train_dataset) > 0 else None\n",
    "if batch is not None:\n",
    "    imgs, sal_maps, fnames = batch\n",
    "    print('Batch shapes:', imgs.shape, sal_maps.shape)\n",
    "    grid = vutils.make_grid(imgs, nrow=min(4, imgs.size(0)))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.title('Sample input images')\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    grid_sal = vutils.make_grid(sal_maps, nrow=min(4, sal_maps.size(0)))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.title('Sample ground truth saliency maps')\n",
    "    plt.imshow(grid_sal[0].cpu(), cmap='gray', vmin=0.0, vmax=1.0) #cmap = hot\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Warning: Train dataset is empty. Check your paths and JSON mapping.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc957419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def visualize_cached_saliency(split: str = 'train', num_samples: int = 3):\n",
    "    cache_dir = cfg.train_saliency_dir if split == 'train' else cfg.val_saliency_dir\n",
    "    img_dir = cfg.train_img_dir if split == 'train' else cfg.val_img_dir\n",
    "    if not os.path.isdir(cache_dir):\n",
    "        print(f'Cache directory not found: {cache_dir}')\n",
    "        return\n",
    "\n",
    "    saliency_paths = sorted(glob.glob(os.path.join(cache_dir, '*.npy')))\n",
    "    if not saliency_paths:\n",
    "        print(f'No cached saliency maps found in {cache_dir}')\n",
    "        return\n",
    "\n",
    "    for npy_path in saliency_paths[:max(0, num_samples)]:\n",
    "        sal_map = np.load(npy_path)\n",
    "        base = os.path.splitext(os.path.basename(npy_path))[0]\n",
    "        candidates = sorted(glob.glob(os.path.join(img_dir, base + '.*')))\n",
    "        img = None\n",
    "        if candidates:\n",
    "            try:\n",
    "                img = Image.open(candidates[0]).convert('RGB')\n",
    "                img = img.resize(cfg.image_size)\n",
    "            except Exception as exc:\n",
    "                print(f'Failed to load image for {base}: {exc}')\n",
    "                img = None\n",
    "\n",
    "        if img is not None:\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
    "            axs[0].imshow(img)\n",
    "            axs[0].set_title('Image')\n",
    "            axs[0].axis('off')\n",
    "            axs[1].imshow(sal_map, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            axs[1].set_title(f'Saliency {base}')\n",
    "            axs[1].axis('off')\n",
    "        else:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "            ax.imshow(sal_map, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            ax.set_title(f'Saliency {base}')\n",
    "            ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualize_cached_saliency('train', num_samples=3)\n",
    "visualize_cached_saliency('val', num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.time_mlp = None\n",
    "        if time_emb_dim is not None:\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                nn.Linear(time_emb_dim, out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, t_emb=None):\n",
    "        h = self.conv1(x)\n",
    "        if self.time_mlp is not None and t_emb is not None:\n",
    "            # Add time embedding (broadcast over spatial dims)\n",
    "            temb = self.time_mlp(t_emb)[:, :, None, None]\n",
    "            h = h + temb\n",
    "        h = self.act(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.act(h)\n",
    "        return h\n",
    "\n",
    "# Sinusoidal positional time embeddings\n",
    "def sinusoidal_time_embedding(t: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    # t: (B,) in [0, timesteps)\n",
    "    device = t.device\n",
    "    half_dim = dim // 2\n",
    "    # log space frequencies\n",
    "    emb_factor = math.log(10000) / max(1, half_dim - 1)\n",
    "    # shape (half_dim,)\n",
    "    freqs = torch.exp(torch.arange(0, half_dim, device=device) * (-emb_factor))\n",
    "    # normalize t to [0,1]\n",
    "    t_norm = t.float() / max(1, cfg.timesteps - 1)\n",
    "    # outer product (B, half_dim)\n",
    "    angles = t_norm[:, None] * freqs[None, :]\n",
    "    emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "    if dim % 2 == 1:\n",
    "        # pad one channel if odd dim\n",
    "        emb = torch.nn.functional.pad(emb, (0, 1), mode='constant', value=0.0)\n",
    "    return emb\n",
    "\n",
    "class UNetSaliency(nn.Module):\n",
    "    def __init__(self, img_channels=3, saliency_channels=1, base_ch=64, time_emb_dim=128, pos_dim=64):\n",
    "        super().__init__()\n",
    "        in_ch = img_channels + saliency_channels\n",
    "\n",
    "        # Time embedding: sinusoidal -> linear projection -> MLP\n",
    "        self.pos_dim = pos_dim\n",
    "        self.time_proj = nn.Linear(pos_dim, time_emb_dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Encoder (deeper: 3 downsamples + bottleneck)\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch, time_emb_dim)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch * 2, time_emb_dim)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ConvBlock(base_ch * 2, base_ch * 4, time_emb_dim)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(base_ch * 4, base_ch * 8, time_emb_dim)\n",
    "        \n",
    "        # Decoder (mirror)\n",
    "        self.up3 = nn.ConvTranspose2d(base_ch * 8, base_ch * 4, 2, stride=2)\n",
    "        self.dec3 = ConvBlock(base_ch * 8, base_ch * 4, time_emb_dim)\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch * 4, base_ch * 2, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(base_ch * 4, base_ch * 2, time_emb_dim)\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch * 2, base_ch, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(base_ch * 2, base_ch, time_emb_dim)\n",
    "        \n",
    "        self.out_conv = nn.Conv2d(base_ch, saliency_channels, 1)\n",
    "\n",
    "    def forward(self, x_img, x_sal_noisy, t):\n",
    "        # x_img: (B, 3, H, W), x_sal_noisy: (B, 1, H, W), t: (B,) timestep index\n",
    "        # Build sinusoidal positional embedding\n",
    "        pos = sinusoidal_time_embedding(t, self.pos_dim)\n",
    "        t_emb = self.time_proj(pos)\n",
    "        t_emb = self.time_mlp(t_emb)\n",
    "\n",
    "        x = torch.cat([x_img, x_sal_noisy], dim=1)\n",
    "        \n",
    "        e1 = self.enc1(x, t_emb)\n",
    "        p1 = self.pool1(e1)\n",
    "        e2 = self.enc2(p1, t_emb)\n",
    "        p2 = self.pool2(e2)\n",
    "        e3 = self.enc3(p2, t_emb)\n",
    "        p3 = self.pool3(e3)\n",
    "\n",
    "        b = self.bottleneck(p3, t_emb)\n",
    "\n",
    "        u3 = self.up3(b)\n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1), t_emb)\n",
    "        u2 = self.up2(d3)\n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1), t_emb)\n",
    "        u1 = self.up1(d2)\n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1), t_emb)\n",
    "\n",
    "        out = self.out_conv(d1)\n",
    "        return out\n",
    "\n",
    "model = UNetSaliency().to(cfg.device)\n",
    "print('Model params:', sum(p.numel() for p in model.parameters()) / 1e6, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce253e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyDDPM(nn.Module):\n",
    "    def __init__(self, model: nn.Module, timesteps: int = 1000, beta_start: float = 1e-4, beta_end: float = 0.02):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "        betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "       \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - alphas_cumprod))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, x_img, x_sal, t):\n",
    "        noise = torch.randn_like(x_sal)\n",
    "        x_noisy = self.q_sample(x_sal, t, noise)\n",
    "        noise_pred = self.model(x_img, x_noisy, t)\n",
    "        return nn.functional.mse_loss(noise_pred, noise)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x_img, t, x):\n",
    "        betas_t = self.betas[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_recip_alphas_t = torch.sqrt(1.0 / self.alphas[t]).view(-1, 1, 1, 1)\n",
    "       \n",
    "        # Predict noise\n",
    "        noise_pred = self.model(x_img, x, t)\n",
    "        model_mean = sqrt_recip_alphas_t * (x - betas_t / sqrt_one_minus_alphas_cumprod_t * noise_pred)\n",
    "       \n",
    "        if t[0] == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma_t = torch.sqrt(betas_t)\n",
    "            return model_mean + sigma_t * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, x_img, shape):\n",
    "        # x_img: condition, shape: (B, 1, H, W) saliency shape\n",
    "        x = torch.randn(shape, device=x_img.device)\n",
    "        B = x.shape[0]\n",
    "        for i in reversed(range(self.timesteps)):\n",
    "            t = torch.full((B,), i, device=x_img.device, dtype=torch.long)\n",
    "            x = self.p_sample(x_img, t, x)\n",
    "        return x\n",
    "\n",
    "ddpm = SaliencyDDPM(model, timesteps=cfg.timesteps, beta_start=cfg.beta_start, beta_end=cfg.beta_end).to(cfg.device)\n",
    "ddpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(ddpm.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "\n",
    "def train_epoch(epoch_idx: int):\n",
    "    ddpm.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for imgs, sal_maps, _ in train_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        \n",
    "        b = imgs.size(0)\n",
    "        t = torch.randint(0, cfg.timesteps, (b,), device=cfg.device).long()\n",
    "\n",
    "        loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    return total_loss / max(1, num_batches)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(epoch_idx: int):\n",
    "    ddpm.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for imgs, sal_maps, _ in val_loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        b = imgs.size(0)\n",
    "        t = torch.randint(0, cfg.timesteps, (b,), device=cfg.device).long()\n",
    "        loss = ddpm.p_losses(imgs, sal_maps, t)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    return total_loss / max(1, num_batches)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    train_loss = train_epoch(epoch)\n",
    "    val_loss = validate_epoch(epoch)\n",
    "    print(f\"Epoch {epoch+1}/{cfg.num_epochs} | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(ddpm.state_dict(), cfg.checkpoint_path)\n",
    "        print('  Saved new best model to', cfg.checkpoint_path)\n",
    "print('Training complete. Best val loss:', best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f86db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(cfg.checkpoint_path):\n",
    "    ddpm.load_state_dict(torch.load(cfg.checkpoint_path, map_location=cfg.device))\n",
    "    print('Loaded best checkpoint.')\n",
    "else:\n",
    "    print('Checkpoint not found; using current model weights.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d6f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_samples_from_loader(loader, num_batches: int = 1, tag: str = 'val'):\n",
    "    ddpm.eval()\n",
    "    batch_count = 0\n",
    "    for imgs, sal_maps, fnames in loader:\n",
    "        imgs = imgs.to(cfg.device)\n",
    "        sal_maps = sal_maps.to(cfg.device)\n",
    "        B = imgs.size(0)\n",
    "\n",
    "        # Sample saliency maps via reverse diffusion\n",
    "        samples = ddpm.sample(imgs, shape=sal_maps.shape)\n",
    "        samples = samples.clamp(0.0, 1.0)\n",
    "        \n",
    "        for i in range(B):\n",
    "            img = imgs[i].cpu()\n",
    "            gt = sal_maps[i, 0].cpu()\n",
    "            pred = samples[i, 0].cpu()\n",
    "\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "            axs[0].imshow(img.permute(1, 2, 0))\n",
    "            axs[0].set_title('Image')\n",
    "            axs[0].axis('off')\n",
    "\n",
    "            axs[1].imshow(gt, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            axs[1].set_title('GT saliency')\n",
    "            axs[1].axis('off')\n",
    "\n",
    "            axs[2].imshow(pred, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            axs[2].set_title('Pred saliency (diffusion)')\n",
    "            axs[2].axis('off')\n",
    "            plt.tight_layout()\n",
    "            save_path = os.path.join(cfg.sample_dir, f'{tag}_{fnames[i]}')\n",
    "            fig.savefig(save_path, dpi=150)\n",
    "            plt.close(fig)\n",
    "\n",
    "        batch_count += 1\n",
    "        if batch_count >= num_batches:\n",
    "            break\n",
    "\n",
    "    print(f'Saved sample visualizations to {cfg.sample_dir} (tag={tag}).')\n",
    "# Generate a few validation samples\n",
    "if len(val_dataset) > 0:\n",
    "    generate_samples_from_loader(val_loader, num_batches=1, tag='val')\n",
    "else:\n",
    "    print('No validation data; skipping sample generation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bf8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, image_size=(256, 192)):\n",
    "        self.img_dir = img_dir\n",
    "        self.files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.files.sort()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size[1], image_size[0])),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        img_path = os.path.join(self.img_dir, fname)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img, fname\n",
    "\n",
    "test_dataset = TestImageDataset(cfg.test_img_dir, image_size=cfg.image_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "print('Test dataset size:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cached_saliency(split: str = 'train', num_samples: int = 3):\n",
    "    cache_dir = cfg.train_saliency_dir if split == 'train' else cfg.val_saliency_dir\n",
    "    img_dir = cfg.train_img_dir if split == 'train' else cfg.val_img_dir\n",
    "    if not os.path.isdir(cache_dir):\n",
    "        print(f'Cache directory not found: {cache_dir}')\n",
    "        return\n",
    "\n",
    "    saliency_paths = sorted(glob.glob(os.path.join(cache_dir, '*.npy')))\n",
    "    if not saliency_paths:\n",
    "        print(f'No cached saliency maps found in {cache_dir}')\n",
    "        return\n",
    "\n",
    "    for npy_path in saliency_paths[:max(0, num_samples)]:\n",
    "        sal_map = np.load(npy_path)\n",
    "        base = os.path.splitext(os.path.basename(npy_path))[0]\n",
    "        candidates = sorted(glob.glob(os.path.join(img_dir, base + '.*')))\n",
    "        img = None\n",
    "        if candidates:\n",
    "            try:\n",
    "                img = Image.open(candidates[0]).convert('RGB')\n",
    "                img = img.resize(cfg.image_size)\n",
    "            except Exception as exc:\n",
    "                print(f'Failed to load image for {base}: {exc}')\n",
    "                img = None\n",
    "\n",
    "        if img is not None:\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
    "            axs[0].imshow(img)\n",
    "            axs[0].set_title('Image')\n",
    "            axs[0].axis('off')\n",
    "            axs[1].imshow(sal_map, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            axs[1].set_title(f'Saliency {base}')\n",
    "            axs[1].axis('off')\n",
    "        else:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "            ax.imshow(sal_map, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            ax.set_title(f'Saliency {base}')\n",
    "            ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualize_cached_saliency('train', num_samples=3)\n",
    "visualize_cached_saliency('val', num_samples=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
